{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "\n",
    "from utils import data_preprocessing_util as dpu\n",
    "from utils import classification_util as cu\n",
    "\n",
    "from MLP import MLP\n",
    "from nn_framework import NNFramework\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fertility Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'data/fertility_diagnosis.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "df = dpu.preprocess_fertility_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NNFramework()\n",
    "nn.fit_encoder(df=df, cols_to_encode=df.columns.difference(['age', 'hours_sitting']))\n",
    "df_encoded = nn.encode_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_encoded.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1038\n",
    "scaling = True\n",
    "oversampling = True\n",
    "\n",
    "scaler = preprocessing.StandardScaler() if scaling else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_encoded['diagnosis']\n",
    "X = df_encoded[df_encoded.columns.difference(['diagnosis'])]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = random_seed, shuffle=True, stratify=y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Params to check out:\n",
    "- 2 activation functions\n",
    "- 3 lrs: 0.001, 0.01, 0.1\n",
    "- number of nodes per layer\n",
    "- 1 layer:\n",
    "    - 5\n",
    "    - 32\n",
    "- 2 layer:\n",
    "    - 16 16\n",
    "- 3 layers\n",
    "    - 10 5 5\n",
    "    - 16 8 8\n",
    "    - 64 32 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = ['relu', 'sigmoid']\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 0.5]\n",
    "hidden_layer_sizes = [(5,), (32,), (16, 16), (10, 5, 5), (16, 8, 8), (64, 32, 32),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = []\n",
    "\n",
    "for af in activation_functions:\n",
    "    for lr in learning_rates:\n",
    "        for hls in hidden_layer_sizes:\n",
    "            methods.append((f'MLP-{af}-{lr}-{hls}', MLP(n_iter=5000, activation_function=af, learning_rate=lr, hidden_layer_sizes=hls)))\n",
    "    \n",
    "pipelines = cu.define_pipelines(methods, scaler=scaler, oversampling=oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP-relu-0.001-(5,)\n",
      "f1 scores: [0.39393939 0.56709957 0.60784314 0.49820789 0.44444444]\n",
      "f1 mean: 0.502\n",
      "f1 std: 0.078\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-relu-0.001-(32,)\n",
      "f1 scores: [0.49820789 0.56709957 0.72222222 0.42857143 0.60784314]\n",
      "f1 mean: 0.565\n",
      "f1 std: 0.100\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.001-(16, 16)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.41176471 0.39393939]\n",
      "f1 mean: 0.216\n",
      "f1 std: 0.153\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.001-(10, 5, 5)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.45945946 0.45945946]\n",
      "f1 mean: 0.238\n",
      "f1 std: 0.181\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.001-(16, 8, 8)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.13043478 0.13043478]\n",
      "f1 mean: 0.336\n",
      "f1 std: 0.168\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.001-(64, 32, 32)\n",
      "f1 scores: [0.45945946 0.45945946 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.462\n",
      "f1 std: 0.006\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(5,)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.53125    0.44444444]\n",
      "f1 mean: 0.250\n",
      "f1 std: 0.196\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(32,)\n",
      "f1 scores: [0.09090909 0.2        0.04761905 0.42857143 0.60784314]\n",
      "f1 mean: 0.275\n",
      "f1 std: 0.212\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(16, 16)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.45945946 0.45945946]\n",
      "f1 mean: 0.238\n",
      "f1 std: 0.181\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(10, 5, 5)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.45945946 0.45945946]\n",
      "f1 mean: 0.238\n",
      "f1 std: 0.181\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(16, 8, 8)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.13043478 0.13043478]\n",
      "f1 mean: 0.336\n",
      "f1 std: 0.168\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(64, 32, 32)\n",
      "f1 scores: [0.45945946 0.45945946 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.462\n",
      "f1 std: 0.006\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(5,)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.34837093 0.41333333]\n",
      "f1 mean: 0.207\n",
      "f1 std: 0.144\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(32,)\n",
      "f1 scores: [0.09090909 0.2        0.04761905 0.44444444 0.45945946]\n",
      "f1 mean: 0.248\n",
      "f1 std: 0.173\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(16, 16)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.45945946 0.45945946]\n",
      "f1 mean: 0.238\n",
      "f1 std: 0.181\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(10, 5, 5)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.45945946 0.45945946]\n",
      "f1 mean: 0.238\n",
      "f1 std: 0.181\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(16, 8, 8)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.13043478 0.13043478]\n",
      "f1 mean: 0.336\n",
      "f1 std: 0.168\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(64, 32, 32)\n",
      "f1 scores: [0.45945946 0.45945946 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.462\n",
      "f1 std: 0.006\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.001-(5,)\n",
      "f1 scores: [0.28571429 0.65714286 0.60784314 0.375      0.41176471]\n",
      "f1 mean: 0.467\n",
      "f1 std: 0.142\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.001-(32,)\n",
      "f1 scores: [0.37321937 0.74025974 0.41176471 0.42857143 0.39393939]\n",
      "f1 mean: 0.470\n",
      "f1 std: 0.137\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.001-(16, 16)\n",
      "f1 scores: [0.45054945 0.35483871 0.37321937 0.45945946 0.2481203 ]\n",
      "f1 mean: 0.377\n",
      "f1 std: 0.077\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.001-(10, 5, 5)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.13043478 0.13043478]\n",
      "f1 mean: 0.336\n",
      "f1 std: 0.168\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.001-(16, 8, 8)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.468\n",
      "f1 std: 0.007\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.001-(64, 32, 32)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.468\n",
      "f1 std: 0.007\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.01-(5,)\n",
      "f1 scores: [0.4047619  0.81981982 0.44444444 0.375      0.41176471]\n",
      "f1 mean: 0.491\n",
      "f1 std: 0.166\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.01-(32,)\n",
      "f1 scores: [0.37321937 0.74025974 0.42857143 0.42857143 0.42857143]\n",
      "f1 mean: 0.480\n",
      "f1 std: 0.132\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.01-(16, 16)\n",
      "f1 scores: [0.49820789 0.65714286 0.375      0.39393939 0.42857143]\n",
      "f1 mean: 0.471\n",
      "f1 std: 0.102\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.01-(10, 5, 5)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.45945946 0.37321937]\n",
      "f1 mean: 0.451\n",
      "f1 std: 0.039\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.01-(16, 8, 8)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.468\n",
      "f1 std: 0.007\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.01-(64, 32, 32)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.468\n",
      "f1 std: 0.007\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.1-(5,)\n",
      "f1 scores: [0.49820789 0.72222222 0.60784314 0.39393939 0.56709957]\n",
      "f1 mean: 0.558\n",
      "f1 std: 0.110\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.1-(32,)\n",
      "f1 scores: [0.30666667 0.53125    0.41176471 0.44444444 0.56709957]\n",
      "f1 mean: 0.452\n",
      "f1 std: 0.092\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.1-(16, 16)\n",
      "f1 scores: [0.34065934 0.60784314 0.375      0.35483871 0.42857143]\n",
      "f1 mean: 0.421\n",
      "f1 std: 0.098\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.1-(10, 5, 5)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.39393939 0.60784314]\n",
      "f1 mean: 0.485\n",
      "f1 std: 0.069\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.1-(16, 8, 8)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.468\n",
      "f1 std: 0.007\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.1-(64, 32, 32)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.468\n",
      "f1 std: 0.007\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cv_num = 5\n",
    "model_params = {}\n",
    "models = {}\n",
    "\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    cv_results = cross_validate(pipeline, X, y, cv=cv_num, scoring='f1_macro', return_estimator=True)\n",
    "\n",
    "    models[model_name] = cv_results['estimator']\n",
    "    model_params[model_name] = {}\n",
    "\n",
    "    num_cols = ['test_score', 'fit_time', 'score_time']\n",
    "\n",
    "    for num_col in num_cols:\n",
    "        model_params[model_name][num_col] = cv_results[num_col]\n",
    "        model_params[model_name][f'{num_col}_mean'] = cv_results[num_col].mean()\n",
    "        model_params[model_name][f'{num_col}_std'] = cv_results[num_col].std()\n",
    "    \n",
    "    model_params[model_name]['parameter_num'] = cv_results['estimator'][0][model_name].number_of_params_\n",
    "    model_params[model_name]['hidden_layer_sizes'] = cv_results['estimator'][0][model_name].hidden_layer_sizes\n",
    "    model_params[model_name]['activation_function'] = cv_results['estimator'][0][model_name].activation_function\n",
    "    model_params[model_name]['learning_rate'] = cv_results['estimator'][0][model_name].learning_rate\n",
    "    model_params[model_name]['converged'] = [e[model_name].converged_ for e in cv_results['estimator']]\n",
    "    model_params[model_name]['validation_losses'] = [e[model_name].validation_losses_ for e in cv_results['estimator']]\n",
    "    model_params[model_name]['training_losses'] = [e[model_name].training_losses_ for e in cv_results['estimator']]\n",
    "    model_params[model_name]['num_iter'] = np.array(list([len(e[model_name].training_losses_) for e in cv_results['estimator']])).mean()\n",
    "\n",
    "    print(model_name)\n",
    "    print(\n",
    "        f\"f1 scores: {model_params[model_name]['test_score']}\\n\" +\n",
    "        f\"f1 mean: {model_params[model_name]['test_score_mean']:.3f}\\n\" +\n",
    "        f\"f1 std: {model_params[model_name]['test_score_std']:.3f}\\n\"\n",
    "    )\n",
    "    print('----------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_mean</th>\n",
       "      <th>test_score_std</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>fit_time_mean</th>\n",
       "      <th>fit_time_std</th>\n",
       "      <th>score_time</th>\n",
       "      <th>score_time_mean</th>\n",
       "      <th>score_time_std</th>\n",
       "      <th>parameter_num</th>\n",
       "      <th>hidden_layer_sizes</th>\n",
       "      <th>activation_function</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>converged</th>\n",
       "      <th>validation_losses</th>\n",
       "      <th>training_losses</th>\n",
       "      <th>num_iter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP-relu-0.001-(5,)</td>\n",
       "      <td>[0.39393939393939387, 0.5670995670995671, 0.60...</td>\n",
       "      <td>0.502307</td>\n",
       "      <td>0.078031</td>\n",
       "      <td>[5.800848960876465, 5.833294153213501, 5.65875...</td>\n",
       "      <td>5.771315</td>\n",
       "      <td>0.060739</td>\n",
       "      <td>[0.0020003318786621094, 0.0029938220977783203,...</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>117</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[[1.2334464780565, 0.9929382153109731, 0.83348...</td>\n",
       "      <td>[[0.9495062401575112, 0.8133720555444218, 0.73...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP-relu-0.001-(32,)</td>\n",
       "      <td>[0.4982078853046595, 0.5670995670995671, 0.722...</td>\n",
       "      <td>0.564789</td>\n",
       "      <td>0.099632</td>\n",
       "      <td>[6.034198999404907, 6.041671991348267, 5.99051...</td>\n",
       "      <td>6.031088</td>\n",
       "      <td>0.032892</td>\n",
       "      <td>[0.003000497817993164, 0.0020008087158203125, ...</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>738</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[[0.571559500828169, 0.46260801340678004, 0.40...</td>\n",
       "      <td>[[0.6713400062412406, 0.5928890608372823, 0.54...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP-relu-0.001-(16, 16)</td>\n",
       "      <td>[0.09090909090909091, 0.09090909090909091, 0.0...</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.152924</td>\n",
       "      <td>[2.7233526706695557, 4.19177770614624, 2.14267...</td>\n",
       "      <td>5.075912</td>\n",
       "      <td>2.605945</td>\n",
       "      <td>[0.002000093460083008, 0.002000570297241211, 0...</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>642</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[False, False, False, True, True]</td>\n",
       "      <td>[[0.7202748805536567, 0.5960108189752704, 0.54...</td>\n",
       "      <td>[[0.856512505048089, 0.747793020727742, 0.6845...</td>\n",
       "      <td>621.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP-relu-0.001-(10, 5, 5)</td>\n",
       "      <td>[0.09090909090909091, 0.09090909090909091, 0.0...</td>\n",
       "      <td>0.238329</td>\n",
       "      <td>0.180552</td>\n",
       "      <td>[0.283064603805542, 0.24588608741760254, 0.348...</td>\n",
       "      <td>0.27296</td>\n",
       "      <td>0.048594</td>\n",
       "      <td>[0.002000570297241211, 0.003001689910888672, 0...</td>\n",
       "      <td>0.002201</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>307</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>[[0.8999277271121642, 0.9042320696322854, 0.91...</td>\n",
       "      <td>[[0.9860105884175189, 0.9891275796274209, 0.99...</td>\n",
       "      <td>25.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP-relu-0.001-(16, 8, 8)</td>\n",
       "      <td>[0.4736842105263158, 0.4736842105263158, 0.473...</td>\n",
       "      <td>0.336384</td>\n",
       "      <td>0.168157</td>\n",
       "      <td>[0.031006574630737305, 0.054012298583984375, 0...</td>\n",
       "      <td>0.040212</td>\n",
       "      <td>0.012123</td>\n",
       "      <td>[0.0020003318786621094, 0.002000570297241211, ...</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>562</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>[[4.134041693741337, 10.812845872129204], [1.8...</td>\n",
       "      <td>[[4.537235663629631, nan], [1.7955302805772584...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP-relu-0.001-(64, 32, 32)</td>\n",
       "      <td>[0.4594594594594595, 0.4594594594594595, 0.473...</td>\n",
       "      <td>0.462304</td>\n",
       "      <td>0.00569</td>\n",
       "      <td>[0.004259824752807617, 0.00500178337097168, 0....</td>\n",
       "      <td>0.004854</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>[0.002999544143676758, 0.0018682479858398438, ...</td>\n",
       "      <td>0.002374</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>4546</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLP-relu-0.01-(5,)</td>\n",
       "      <td>[0.09090909090909091, 0.09090909090909091, 0.0...</td>\n",
       "      <td>0.249684</td>\n",
       "      <td>0.196387</td>\n",
       "      <td>[3.579591989517212, 3.7283995151519775, 3.0883...</td>\n",
       "      <td>4.37326</td>\n",
       "      <td>1.132105</td>\n",
       "      <td>[0.002000570297241211, 0.002000570297241211, 0...</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>117</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[False, False, False, True, True]</td>\n",
       "      <td>[[0.5288148363661361, 0.41737921156972974, 0.3...</td>\n",
       "      <td>[[0.5893710807460183, 0.512832762659213, 0.452...</td>\n",
       "      <td>762.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLP-relu-0.01-(32,)</td>\n",
       "      <td>[0.09090909090909091, 0.19999999999999998, 0.0...</td>\n",
       "      <td>0.274989</td>\n",
       "      <td>0.212408</td>\n",
       "      <td>[2.231398582458496, 2.6350533962249756, 1.8674...</td>\n",
       "      <td>3.826858</td>\n",
       "      <td>1.952976</td>\n",
       "      <td>[0.0020003318786621094, 0.0020008087158203125,...</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>738</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[False, False, False, True, True]</td>\n",
       "      <td>[[0.24396183892380904, 0.1713048913483235, 0.1...</td>\n",
       "      <td>[[0.4532890628303428, 0.35828773587267687, 0.2...</td>\n",
       "      <td>625.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MLP-relu-0.01-(16, 16)</td>\n",
       "      <td>[0.09090909090909091, 0.09090909090909091, 0.0...</td>\n",
       "      <td>0.238329</td>\n",
       "      <td>0.180552</td>\n",
       "      <td>[0.013003110885620117, 0.016004085540771484, 0...</td>\n",
       "      <td>0.013203</td>\n",
       "      <td>0.002926</td>\n",
       "      <td>[0.0020008087158203125, 0.002000093460083008, ...</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>642</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>[[84.07095085701769], [nan], [4.82252291446771...</td>\n",
       "      <td>[[nan], [nan], [7.647845614030195], [], [nan]]</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLP-relu-0.01-(10, 5, 5)</td>\n",
       "      <td>[0.09090909090909091, 0.09090909090909091, 0.0...</td>\n",
       "      <td>0.238329</td>\n",
       "      <td>0.180552</td>\n",
       "      <td>[0.009002685546875, 0.009002208709716797, 0.00...</td>\n",
       "      <td>0.007202</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>[0.002000570297241211, 0.0019998550415039062, ...</td>\n",
       "      <td>0.002201</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>307</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MLP-relu-0.01-(16, 8, 8)</td>\n",
       "      <td>[0.4736842105263158, 0.4736842105263158, 0.473...</td>\n",
       "      <td>0.336384</td>\n",
       "      <td>0.168157</td>\n",
       "      <td>[0.005000114440917969, 0.005001068115234375, 0...</td>\n",
       "      <td>0.004401</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>[0.002000570297241211, 0.003000497817993164, 0...</td>\n",
       "      <td>0.002201</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>562</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MLP-relu-0.01-(64, 32, 32)</td>\n",
       "      <td>[0.4594594594594595, 0.4594594594594595, 0.473...</td>\n",
       "      <td>0.462304</td>\n",
       "      <td>0.00569</td>\n",
       "      <td>[0.004001140594482422, 0.004000663757324219, 0...</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>[0.0020008087158203125, 0.0019996166229248047,...</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4546</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MLP-relu-0.1-(5,)</td>\n",
       "      <td>[0.09090909090909091, 0.09090909090909091, 0.0...</td>\n",
       "      <td>0.206886</td>\n",
       "      <td>0.14352</td>\n",
       "      <td>[0.019003629684448242, 0.01700425148010254, 0....</td>\n",
       "      <td>0.017804</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>[0.0020003318786621094, 0.0030002593994140625,...</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>117</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>[[0.39948955907877065, 9.5580019783381], [0.96...</td>\n",
       "      <td>[[1.3577757175150287, 21.34987967827427], [1.2...</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MLP-relu-0.1-(32,)</td>\n",
       "      <td>[0.09090909090909091, 0.19999999999999998, 0.0...</td>\n",
       "      <td>0.248486</td>\n",
       "      <td>0.173458</td>\n",
       "      <td>[0.014003515243530273, 0.011002779006958008, 0...</td>\n",
       "      <td>0.012203</td>\n",
       "      <td>0.00194</td>\n",
       "      <td>[0.0020012855529785156, 0.0020008087158203125,...</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>738</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>[[3.298125657220015], [24.646350477774764], [5...</td>\n",
       "      <td>[[9.308968684573369], [36.298818204242615], [5...</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MLP-relu-0.1-(16, 16)</td>\n",
       "      <td>[0.09090909090909091, 0.09090909090909091, 0.0...</td>\n",
       "      <td>0.238329</td>\n",
       "      <td>0.180552</td>\n",
       "      <td>[0.0050013065338134766, 0.005001544952392578, ...</td>\n",
       "      <td>0.004201</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>[0.0019996166229248047, 0.002000570297241211, ...</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>642</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MLP-relu-0.1-(10, 5, 5)</td>\n",
       "      <td>[0.09090909090909091, 0.09090909090909091, 0.0...</td>\n",
       "      <td>0.238329</td>\n",
       "      <td>0.180552</td>\n",
       "      <td>[0.00400090217590332, 0.003999948501586914, 0....</td>\n",
       "      <td>0.004401</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>[0.0010001659393310547, 0.002001047134399414, ...</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>307</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MLP-relu-0.1-(16, 8, 8)</td>\n",
       "      <td>[0.4736842105263158, 0.4736842105263158, 0.473...</td>\n",
       "      <td>0.336384</td>\n",
       "      <td>0.168157</td>\n",
       "      <td>[0.0050008296966552734, 0.003999471664428711, ...</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>[0.003001689910888672, 0.002000570297241211, 0...</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>562</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MLP-relu-0.1-(64, 32, 32)</td>\n",
       "      <td>[0.4594594594594595, 0.4594594594594595, 0.473...</td>\n",
       "      <td>0.462304</td>\n",
       "      <td>0.00569</td>\n",
       "      <td>[0.004000425338745117, 0.004001140594482422, 0...</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0019998550415039062, 0.0020003318786621094,...</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>4546</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MLP-sigmoid-0.001-(5,)</td>\n",
       "      <td>[0.2857142857142857, 0.6571428571428571, 0.607...</td>\n",
       "      <td>0.467493</td>\n",
       "      <td>0.141682</td>\n",
       "      <td>[6.781397104263306, 6.75782585144043, 6.674428...</td>\n",
       "      <td>6.752202</td>\n",
       "      <td>0.042053</td>\n",
       "      <td>[0.0020008087158203125, 0.0020003318786621094,...</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>117</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[[0.8086612543683216, 0.7838240974520597, 0.76...</td>\n",
       "      <td>[[0.7467873950391224, 0.7288419216370972, 0.71...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MLP-sigmoid-0.001-(32,)</td>\n",
       "      <td>[0.3732193732193732, 0.7402597402597403, 0.411...</td>\n",
       "      <td>0.469551</td>\n",
       "      <td>0.136601</td>\n",
       "      <td>[7.091639995574951, 7.033848285675049, 7.10409...</td>\n",
       "      <td>7.120622</td>\n",
       "      <td>0.059029</td>\n",
       "      <td>[0.002000570297241211, 0.0020003318786621094, ...</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>738</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[[0.5943959431219367, 0.5844835223993747, 0.57...</td>\n",
       "      <td>[[0.5969220354366359, 0.591602178775377, 0.587...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MLP-sigmoid-0.001-(16, 16)</td>\n",
       "      <td>[0.4505494505494505, 0.3548387096774194, 0.373...</td>\n",
       "      <td>0.377237</td>\n",
       "      <td>0.076582</td>\n",
       "      <td>[9.857231378555298, 10.036181211471558, 10.139...</td>\n",
       "      <td>10.026182</td>\n",
       "      <td>0.101905</td>\n",
       "      <td>[0.0030007362365722656, 0.002001047134399414, ...</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>642</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[[0.8749123928004863, 0.7226266656044958, 0.69...</td>\n",
       "      <td>[[0.8735145331034436, 0.7221014017756288, 0.69...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MLP-sigmoid-0.001-(10, 5, 5)</td>\n",
       "      <td>[0.4736842105263158, 0.4736842105263158, 0.473...</td>\n",
       "      <td>0.336384</td>\n",
       "      <td>0.168157</td>\n",
       "      <td>[12.789811849594116, 12.536858320236206, 12.53...</td>\n",
       "      <td>12.661065</td>\n",
       "      <td>0.106483</td>\n",
       "      <td>[0.0029997825622558594, 0.0029997825622558594,...</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>307</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[[0.8496312219532959, 0.7844169820815906, 0.74...</td>\n",
       "      <td>[[0.8492973924452568, 0.7842383909602058, 0.74...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>MLP-sigmoid-0.001-(16, 8, 8)</td>\n",
       "      <td>[0.4736842105263158, 0.4736842105263158, 0.473...</td>\n",
       "      <td>0.467994</td>\n",
       "      <td>0.006969</td>\n",
       "      <td>[12.69232964515686, 12.672608852386475, 12.610...</td>\n",
       "      <td>12.822598</td>\n",
       "      <td>0.222928</td>\n",
       "      <td>[0.0020003318786621094, 0.0020003318786621094,...</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>562</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[[0.6954807923123058, 0.6942318054724493, 0.69...</td>\n",
       "      <td>[[0.6954226335329524, 0.6941779350064206, 0.69...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>MLP-sigmoid-0.001-(64, 32, 32)</td>\n",
       "      <td>[0.4736842105263158, 0.4736842105263158, 0.473...</td>\n",
       "      <td>0.467994</td>\n",
       "      <td>0.006969</td>\n",
       "      <td>[14.420699119567871, 14.237870216369629, 14.47...</td>\n",
       "      <td>14.395254</td>\n",
       "      <td>0.095017</td>\n",
       "      <td>[0.002000093460083008, 0.0019996166229248047, ...</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>4546</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[[0.7104309314205262, 0.6931875770444306, 0.69...</td>\n",
       "      <td>[[0.7104309312236585, 0.6931875768918799, 0.69...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MLP-sigmoid-0.01-(5,)</td>\n",
       "      <td>[0.40476190476190477, 0.8198198198198199, 0.44...</td>\n",
       "      <td>0.491158</td>\n",
       "      <td>0.165808</td>\n",
       "      <td>[6.81985068321228, 6.795758962631226, 6.744296...</td>\n",
       "      <td>6.790822</td>\n",
       "      <td>0.025214</td>\n",
       "      <td>[0.0030007362365722656, 0.003000497817993164, ...</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>117</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[[0.7033377843649671, 0.6795045519523014, 0.66...</td>\n",
       "      <td>[[0.6755749296933777, 0.6605835560044867, 0.64...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>MLP-sigmoid-0.01-(32,)</td>\n",
       "      <td>[0.3732193732193732, 0.7402597402597403, 0.428...</td>\n",
       "      <td>0.479839</td>\n",
       "      <td>0.131963</td>\n",
       "      <td>[7.318730354309082, 7.272669553756714, 7.15867...</td>\n",
       "      <td>7.222022</td>\n",
       "      <td>0.062387</td>\n",
       "      <td>[0.002001047134399414, 0.002000570297241211, 0...</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>738</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[[0.543545200730683, 0.5080594193199424, 0.477...</td>\n",
       "      <td>[[0.5655714521570097, 0.538158470787907, 0.514...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>MLP-sigmoid-0.01-(16, 16)</td>\n",
       "      <td>[0.4982078853046595, 0.6571428571428571, 0.375...</td>\n",
       "      <td>0.470572</td>\n",
       "      <td>0.102316</td>\n",
       "      <td>[11.073191404342651, 10.910325288772583, 10.96...</td>\n",
       "      <td>10.633125</td>\n",
       "      <td>0.43395</td>\n",
       "      <td>[0.0030012130737304688, 0.002000093460083008, ...</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>642</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[[0.6928435440016354, 0.6927932368359943, 0.69...</td>\n",
       "      <td>[[0.6926219003283868, 0.6925565931906984, 0.69...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>MLP-sigmoid-0.01-(10, 5, 5)</td>\n",
       "      <td>[0.4736842105263158, 0.4736842105263158, 0.473...</td>\n",
       "      <td>0.450746</td>\n",
       "      <td>0.039153</td>\n",
       "      <td>[12.562489986419678, 13.025301933288574, 13.25...</td>\n",
       "      <td>12.902332</td>\n",
       "      <td>0.232776</td>\n",
       "      <td>[0.002000570297241211, 0.0030002593994140625, ...</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>307</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[[0.6959589122834879, 0.6940169610722303, 0.69...</td>\n",
       "      <td>[[0.6959565907996245, 0.69401162592777, 0.6939...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>MLP-sigmoid-0.01-(16, 8, 8)</td>\n",
       "      <td>[0.4736842105263158, 0.4736842105263158, 0.473...</td>\n",
       "      <td>0.467994</td>\n",
       "      <td>0.006969</td>\n",
       "      <td>[14.146836519241333, 12.924090147018433, 12.91...</td>\n",
       "      <td>13.199023</td>\n",
       "      <td>0.478668</td>\n",
       "      <td>[0.0020003318786621094, 0.002000570297241211, ...</td>\n",
       "      <td>0.001999</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>562</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[[0.6949066873603444, 0.6948879662916979, 0.69...</td>\n",
       "      <td>[[0.6948503493522139, 0.6948317421856988, 0.69...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>MLP-sigmoid-0.01-(64, 32, 32)</td>\n",
       "      <td>[0.4736842105263158, 0.4736842105263158, 0.473...</td>\n",
       "      <td>0.467994</td>\n",
       "      <td>0.006969</td>\n",
       "      <td>[14.469161987304688, 14.738301277160645, 14.44...</td>\n",
       "      <td>14.493885</td>\n",
       "      <td>0.141457</td>\n",
       "      <td>[0.003000497817993164, 0.0020012855529785156, ...</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>4546</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[[0.6988841782745242, 0.6988841790035082, 0.69...</td>\n",
       "      <td>[[0.698884178148291, 0.6988841788772754, 0.698...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>MLP-sigmoid-0.1-(5,)</td>\n",
       "      <td>[0.4982078853046595, 0.7222222222222222, 0.607...</td>\n",
       "      <td>0.557862</td>\n",
       "      <td>0.109569</td>\n",
       "      <td>[6.7383058071136475, 7.390515089035034, 6.9148...</td>\n",
       "      <td>6.965293</td>\n",
       "      <td>0.222492</td>\n",
       "      <td>[0.0019998550415039062, 0.0030007362365722656,...</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>117</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[[0.5437626794533614, 0.4116011647524065, 0.33...</td>\n",
       "      <td>[[0.5618290438082242, 0.4788511573023306, 0.43...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>MLP-sigmoid-0.1-(32,)</td>\n",
       "      <td>[0.30666666666666664, 0.53125, 0.4117647058823...</td>\n",
       "      <td>0.452245</td>\n",
       "      <td>0.092006</td>\n",
       "      <td>[7.269628047943115, 7.161156415939331, 7.22617...</td>\n",
       "      <td>7.301642</td>\n",
       "      <td>0.106958</td>\n",
       "      <td>[0.0019998550415039062, 0.0020008087158203125,...</td>\n",
       "      <td>0.002201</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>738</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[[0.31061256311806934, 0.22881258392297874, 0....</td>\n",
       "      <td>[[0.4049655480359375, 0.3157888862231593, 0.25...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>MLP-sigmoid-0.1-(16, 16)</td>\n",
       "      <td>[0.34065934065934067, 0.6078431372549019, 0.37...</td>\n",
       "      <td>0.421383</td>\n",
       "      <td>0.097893</td>\n",
       "      <td>[10.096295833587646, 10.164588928222656, 10.11...</td>\n",
       "      <td>10.078667</td>\n",
       "      <td>0.06036</td>\n",
       "      <td>[0.002000570297241211, 0.002000570297241211, 0...</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>642</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[[1.0710278633165473, 1.0712480978043373, 1.07...</td>\n",
       "      <td>[[1.0693896209832887, 1.0697069113280733, 1.06...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>MLP-sigmoid-0.1-(10, 5, 5)</td>\n",
       "      <td>[0.4736842105263158, 0.4736842105263158, 0.473...</td>\n",
       "      <td>0.484567</td>\n",
       "      <td>0.068943</td>\n",
       "      <td>[12.567689180374146, 12.505593299865723, 12.59...</td>\n",
       "      <td>12.593354</td>\n",
       "      <td>0.053732</td>\n",
       "      <td>[0.002000093460083008, 0.003000974655151367, 0...</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>307</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[[0.7173608602881799, 0.7179525203160825, 0.71...</td>\n",
       "      <td>[[0.7173331382464797, 0.7179264906972361, 0.71...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>MLP-sigmoid-0.1-(16, 8, 8)</td>\n",
       "      <td>[0.4736842105263158, 0.4736842105263158, 0.473...</td>\n",
       "      <td>0.467994</td>\n",
       "      <td>0.006969</td>\n",
       "      <td>[12.767987251281738, 12.862648487091064, 12.78...</td>\n",
       "      <td>12.820404</td>\n",
       "      <td>0.037808</td>\n",
       "      <td>[0.002000093460083008, 0.002000093460083008, 0...</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>562</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[[0.7913048812275317, 0.7917157139767808, 0.79...</td>\n",
       "      <td>[[0.7911605492687277, 0.7915792627145308, 0.79...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>MLP-sigmoid-0.1-(64, 32, 32)</td>\n",
       "      <td>[0.4736842105263158, 0.4736842105263158, 0.473...</td>\n",
       "      <td>0.467994</td>\n",
       "      <td>0.006969</td>\n",
       "      <td>[14.272512674331665, 14.328404664993286, 14.31...</td>\n",
       "      <td>14.394763</td>\n",
       "      <td>0.111758</td>\n",
       "      <td>[0.0030002593994140625, 0.003000974655151367, ...</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>4546</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[[1.655088758263443, 1.6550887582695835, 1.655...</td>\n",
       "      <td>[[1.65508875826243, 1.6550887582685694, 1.6550...</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             model  \\\n",
       "0              MLP-relu-0.001-(5,)   \n",
       "1             MLP-relu-0.001-(32,)   \n",
       "2          MLP-relu-0.001-(16, 16)   \n",
       "3        MLP-relu-0.001-(10, 5, 5)   \n",
       "4        MLP-relu-0.001-(16, 8, 8)   \n",
       "5      MLP-relu-0.001-(64, 32, 32)   \n",
       "6               MLP-relu-0.01-(5,)   \n",
       "7              MLP-relu-0.01-(32,)   \n",
       "8           MLP-relu-0.01-(16, 16)   \n",
       "9         MLP-relu-0.01-(10, 5, 5)   \n",
       "10        MLP-relu-0.01-(16, 8, 8)   \n",
       "11      MLP-relu-0.01-(64, 32, 32)   \n",
       "12               MLP-relu-0.1-(5,)   \n",
       "13              MLP-relu-0.1-(32,)   \n",
       "14           MLP-relu-0.1-(16, 16)   \n",
       "15         MLP-relu-0.1-(10, 5, 5)   \n",
       "16         MLP-relu-0.1-(16, 8, 8)   \n",
       "17       MLP-relu-0.1-(64, 32, 32)   \n",
       "18          MLP-sigmoid-0.001-(5,)   \n",
       "19         MLP-sigmoid-0.001-(32,)   \n",
       "20      MLP-sigmoid-0.001-(16, 16)   \n",
       "21    MLP-sigmoid-0.001-(10, 5, 5)   \n",
       "22    MLP-sigmoid-0.001-(16, 8, 8)   \n",
       "23  MLP-sigmoid-0.001-(64, 32, 32)   \n",
       "24           MLP-sigmoid-0.01-(5,)   \n",
       "25          MLP-sigmoid-0.01-(32,)   \n",
       "26       MLP-sigmoid-0.01-(16, 16)   \n",
       "27     MLP-sigmoid-0.01-(10, 5, 5)   \n",
       "28     MLP-sigmoid-0.01-(16, 8, 8)   \n",
       "29   MLP-sigmoid-0.01-(64, 32, 32)   \n",
       "30            MLP-sigmoid-0.1-(5,)   \n",
       "31           MLP-sigmoid-0.1-(32,)   \n",
       "32        MLP-sigmoid-0.1-(16, 16)   \n",
       "33      MLP-sigmoid-0.1-(10, 5, 5)   \n",
       "34      MLP-sigmoid-0.1-(16, 8, 8)   \n",
       "35    MLP-sigmoid-0.1-(64, 32, 32)   \n",
       "\n",
       "                                           test_score test_score_mean  \\\n",
       "0   [0.39393939393939387, 0.5670995670995671, 0.60...        0.502307   \n",
       "1   [0.4982078853046595, 0.5670995670995671, 0.722...        0.564789   \n",
       "2   [0.09090909090909091, 0.09090909090909091, 0.0...        0.215686   \n",
       "3   [0.09090909090909091, 0.09090909090909091, 0.0...        0.238329   \n",
       "4   [0.4736842105263158, 0.4736842105263158, 0.473...        0.336384   \n",
       "5   [0.4594594594594595, 0.4594594594594595, 0.473...        0.462304   \n",
       "6   [0.09090909090909091, 0.09090909090909091, 0.0...        0.249684   \n",
       "7   [0.09090909090909091, 0.19999999999999998, 0.0...        0.274989   \n",
       "8   [0.09090909090909091, 0.09090909090909091, 0.0...        0.238329   \n",
       "9   [0.09090909090909091, 0.09090909090909091, 0.0...        0.238329   \n",
       "10  [0.4736842105263158, 0.4736842105263158, 0.473...        0.336384   \n",
       "11  [0.4594594594594595, 0.4594594594594595, 0.473...        0.462304   \n",
       "12  [0.09090909090909091, 0.09090909090909091, 0.0...        0.206886   \n",
       "13  [0.09090909090909091, 0.19999999999999998, 0.0...        0.248486   \n",
       "14  [0.09090909090909091, 0.09090909090909091, 0.0...        0.238329   \n",
       "15  [0.09090909090909091, 0.09090909090909091, 0.0...        0.238329   \n",
       "16  [0.4736842105263158, 0.4736842105263158, 0.473...        0.336384   \n",
       "17  [0.4594594594594595, 0.4594594594594595, 0.473...        0.462304   \n",
       "18  [0.2857142857142857, 0.6571428571428571, 0.607...        0.467493   \n",
       "19  [0.3732193732193732, 0.7402597402597403, 0.411...        0.469551   \n",
       "20  [0.4505494505494505, 0.3548387096774194, 0.373...        0.377237   \n",
       "21  [0.4736842105263158, 0.4736842105263158, 0.473...        0.336384   \n",
       "22  [0.4736842105263158, 0.4736842105263158, 0.473...        0.467994   \n",
       "23  [0.4736842105263158, 0.4736842105263158, 0.473...        0.467994   \n",
       "24  [0.40476190476190477, 0.8198198198198199, 0.44...        0.491158   \n",
       "25  [0.3732193732193732, 0.7402597402597403, 0.428...        0.479839   \n",
       "26  [0.4982078853046595, 0.6571428571428571, 0.375...        0.470572   \n",
       "27  [0.4736842105263158, 0.4736842105263158, 0.473...        0.450746   \n",
       "28  [0.4736842105263158, 0.4736842105263158, 0.473...        0.467994   \n",
       "29  [0.4736842105263158, 0.4736842105263158, 0.473...        0.467994   \n",
       "30  [0.4982078853046595, 0.7222222222222222, 0.607...        0.557862   \n",
       "31  [0.30666666666666664, 0.53125, 0.4117647058823...        0.452245   \n",
       "32  [0.34065934065934067, 0.6078431372549019, 0.37...        0.421383   \n",
       "33  [0.4736842105263158, 0.4736842105263158, 0.473...        0.484567   \n",
       "34  [0.4736842105263158, 0.4736842105263158, 0.473...        0.467994   \n",
       "35  [0.4736842105263158, 0.4736842105263158, 0.473...        0.467994   \n",
       "\n",
       "   test_score_std                                           fit_time  \\\n",
       "0        0.078031  [5.800848960876465, 5.833294153213501, 5.65875...   \n",
       "1        0.099632  [6.034198999404907, 6.041671991348267, 5.99051...   \n",
       "2        0.152924  [2.7233526706695557, 4.19177770614624, 2.14267...   \n",
       "3        0.180552  [0.283064603805542, 0.24588608741760254, 0.348...   \n",
       "4        0.168157  [0.031006574630737305, 0.054012298583984375, 0...   \n",
       "5         0.00569  [0.004259824752807617, 0.00500178337097168, 0....   \n",
       "6        0.196387  [3.579591989517212, 3.7283995151519775, 3.0883...   \n",
       "7        0.212408  [2.231398582458496, 2.6350533962249756, 1.8674...   \n",
       "8        0.180552  [0.013003110885620117, 0.016004085540771484, 0...   \n",
       "9        0.180552  [0.009002685546875, 0.009002208709716797, 0.00...   \n",
       "10       0.168157  [0.005000114440917969, 0.005001068115234375, 0...   \n",
       "11        0.00569  [0.004001140594482422, 0.004000663757324219, 0...   \n",
       "12        0.14352  [0.019003629684448242, 0.01700425148010254, 0....   \n",
       "13       0.173458  [0.014003515243530273, 0.011002779006958008, 0...   \n",
       "14       0.180552  [0.0050013065338134766, 0.005001544952392578, ...   \n",
       "15       0.180552  [0.00400090217590332, 0.003999948501586914, 0....   \n",
       "16       0.168157  [0.0050008296966552734, 0.003999471664428711, ...   \n",
       "17        0.00569  [0.004000425338745117, 0.004001140594482422, 0...   \n",
       "18       0.141682  [6.781397104263306, 6.75782585144043, 6.674428...   \n",
       "19       0.136601  [7.091639995574951, 7.033848285675049, 7.10409...   \n",
       "20       0.076582  [9.857231378555298, 10.036181211471558, 10.139...   \n",
       "21       0.168157  [12.789811849594116, 12.536858320236206, 12.53...   \n",
       "22       0.006969  [12.69232964515686, 12.672608852386475, 12.610...   \n",
       "23       0.006969  [14.420699119567871, 14.237870216369629, 14.47...   \n",
       "24       0.165808  [6.81985068321228, 6.795758962631226, 6.744296...   \n",
       "25       0.131963  [7.318730354309082, 7.272669553756714, 7.15867...   \n",
       "26       0.102316  [11.073191404342651, 10.910325288772583, 10.96...   \n",
       "27       0.039153  [12.562489986419678, 13.025301933288574, 13.25...   \n",
       "28       0.006969  [14.146836519241333, 12.924090147018433, 12.91...   \n",
       "29       0.006969  [14.469161987304688, 14.738301277160645, 14.44...   \n",
       "30       0.109569  [6.7383058071136475, 7.390515089035034, 6.9148...   \n",
       "31       0.092006  [7.269628047943115, 7.161156415939331, 7.22617...   \n",
       "32       0.097893  [10.096295833587646, 10.164588928222656, 10.11...   \n",
       "33       0.068943  [12.567689180374146, 12.505593299865723, 12.59...   \n",
       "34       0.006969  [12.767987251281738, 12.862648487091064, 12.78...   \n",
       "35       0.006969  [14.272512674331665, 14.328404664993286, 14.31...   \n",
       "\n",
       "   fit_time_mean fit_time_std  \\\n",
       "0       5.771315     0.060739   \n",
       "1       6.031088     0.032892   \n",
       "2       5.075912     2.605945   \n",
       "3        0.27296     0.048594   \n",
       "4       0.040212     0.012123   \n",
       "5       0.004854     0.000297   \n",
       "6        4.37326     1.132105   \n",
       "7       3.826858     1.952976   \n",
       "8       0.013203     0.002926   \n",
       "9       0.007202     0.001601   \n",
       "10      0.004401      0.00049   \n",
       "11      0.004001     0.000633   \n",
       "12      0.017804     0.001721   \n",
       "13      0.012203      0.00194   \n",
       "14      0.004201     0.000749   \n",
       "15      0.004401     0.000491   \n",
       "16      0.004001     0.000895   \n",
       "17      0.004001          0.0   \n",
       "18      6.752202     0.042053   \n",
       "19      7.120622     0.059029   \n",
       "20     10.026182     0.101905   \n",
       "21     12.661065     0.106483   \n",
       "22     12.822598     0.222928   \n",
       "23     14.395254     0.095017   \n",
       "24      6.790822     0.025214   \n",
       "25      7.222022     0.062387   \n",
       "26     10.633125      0.43395   \n",
       "27     12.902332     0.232776   \n",
       "28     13.199023     0.478668   \n",
       "29     14.493885     0.141457   \n",
       "30      6.965293     0.222492   \n",
       "31      7.301642     0.106958   \n",
       "32     10.078667      0.06036   \n",
       "33     12.593354     0.053732   \n",
       "34     12.820404     0.037808   \n",
       "35     14.394763     0.111758   \n",
       "\n",
       "                                           score_time score_time_mean  \\\n",
       "0   [0.0020003318786621094, 0.0029938220977783203,...        0.002199   \n",
       "1   [0.003000497817993164, 0.0020008087158203125, ...        0.002601   \n",
       "2   [0.002000093460083008, 0.002000570297241211, 0...          0.0024   \n",
       "3   [0.002000570297241211, 0.003001689910888672, 0...        0.002201   \n",
       "4   [0.0020003318786621094, 0.002000570297241211, ...        0.002401   \n",
       "5   [0.002999544143676758, 0.0018682479858398438, ...        0.002374   \n",
       "6   [0.002000570297241211, 0.002000570297241211, 0...        0.002401   \n",
       "7   [0.0020003318786621094, 0.0020008087158203125,...           0.002   \n",
       "8   [0.0020008087158203125, 0.002000093460083008, ...        0.002001   \n",
       "9   [0.002000570297241211, 0.0019998550415039062, ...        0.002201   \n",
       "10  [0.002000570297241211, 0.003000497817993164, 0...        0.002201   \n",
       "11  [0.0020008087158203125, 0.0019996166229248047,...           0.002   \n",
       "12  [0.0020003318786621094, 0.0030002593994140625,...          0.0022   \n",
       "13  [0.0020012855529785156, 0.0020008087158203125,...        0.002001   \n",
       "14  [0.0019996166229248047, 0.002000570297241211, ...           0.002   \n",
       "15  [0.0010001659393310547, 0.002001047134399414, ...        0.001801   \n",
       "16  [0.003001689910888672, 0.002000570297241211, 0...        0.002001   \n",
       "17  [0.0019998550415039062, 0.0020003318786621094,...          0.0018   \n",
       "18  [0.0020008087158203125, 0.0020003318786621094,...        0.002401   \n",
       "19  [0.002000570297241211, 0.0020003318786621094, ...        0.002401   \n",
       "20  [0.0030007362365722656, 0.002001047134399414, ...          0.0022   \n",
       "21  [0.0029997825622558594, 0.0029997825622558594,...          0.0026   \n",
       "22  [0.0020003318786621094, 0.0020003318786621094,...        0.002401   \n",
       "23  [0.002000093460083008, 0.0019996166229248047, ...          0.0022   \n",
       "24  [0.0030007362365722656, 0.003000497817993164, ...        0.002601   \n",
       "25  [0.002001047134399414, 0.002000570297241211, 0...        0.002001   \n",
       "26  [0.0030012130737304688, 0.002000093460083008, ...          0.0022   \n",
       "27  [0.002000570297241211, 0.0030002593994140625, ...          0.0024   \n",
       "28  [0.0020003318786621094, 0.002000570297241211, ...        0.001999   \n",
       "29  [0.003000497817993164, 0.0020012855529785156, ...          0.0026   \n",
       "30  [0.0019998550415039062, 0.0030007362365722656,...        0.002601   \n",
       "31  [0.0019998550415039062, 0.0020008087158203125,...        0.002201   \n",
       "32  [0.002000570297241211, 0.002000570297241211, 0...           0.002   \n",
       "33  [0.002000093460083008, 0.003000974655151367, 0...        0.002601   \n",
       "34  [0.002000093460083008, 0.002000093460083008, 0...          0.0022   \n",
       "35  [0.0030002593994140625, 0.003000974655151367, ...          0.0026   \n",
       "\n",
       "   score_time_std parameter_num hidden_layer_sizes activation_function  \\\n",
       "0        0.000397           117               (5,)                relu   \n",
       "1         0.00049           738              (32,)                relu   \n",
       "2         0.00049           642           (16, 16)                relu   \n",
       "3        0.000401           307         (10, 5, 5)                relu   \n",
       "4         0.00049           562         (16, 8, 8)                relu   \n",
       "5        0.000513          4546       (64, 32, 32)                relu   \n",
       "6         0.00049           117               (5,)                relu   \n",
       "7        0.000001           738              (32,)                relu   \n",
       "8             0.0           642           (16, 16)                relu   \n",
       "9          0.0004           307         (10, 5, 5)                relu   \n",
       "10         0.0004           562         (16, 8, 8)                relu   \n",
       "11            0.0          4546       (64, 32, 32)                relu   \n",
       "12         0.0004           117               (5,)                relu   \n",
       "13            0.0           738              (32,)                relu   \n",
       "14       0.000001           642           (16, 16)                relu   \n",
       "15         0.0004           307         (10, 5, 5)                relu   \n",
       "16       0.000633           562         (16, 8, 8)                relu   \n",
       "17         0.0004          4546       (64, 32, 32)                relu   \n",
       "18       0.000491           117               (5,)             sigmoid   \n",
       "19        0.00049           738              (32,)             sigmoid   \n",
       "20         0.0004           642           (16, 16)             sigmoid   \n",
       "21        0.00049           307         (10, 5, 5)             sigmoid   \n",
       "22        0.00049           562         (16, 8, 8)             sigmoid   \n",
       "23         0.0004          4546       (64, 32, 32)             sigmoid   \n",
       "24        0.00049           117               (5,)             sigmoid   \n",
       "25       0.000001           738              (32,)             sigmoid   \n",
       "26       0.000401           642           (16, 16)             sigmoid   \n",
       "27        0.00049           307         (10, 5, 5)             sigmoid   \n",
       "28       0.000004           562         (16, 8, 8)             sigmoid   \n",
       "29        0.00049          4546       (64, 32, 32)             sigmoid   \n",
       "30        0.00049           117               (5,)             sigmoid   \n",
       "31         0.0004           738              (32,)             sigmoid   \n",
       "32            0.0           642           (16, 16)             sigmoid   \n",
       "33        0.00049           307         (10, 5, 5)             sigmoid   \n",
       "34         0.0004           562         (16, 8, 8)             sigmoid   \n",
       "35        0.00049          4546       (64, 32, 32)             sigmoid   \n",
       "\n",
       "   learning_rate                            converged  \\\n",
       "0          0.001       [True, True, True, True, True]   \n",
       "1          0.001       [True, True, True, True, True]   \n",
       "2          0.001    [False, False, False, True, True]   \n",
       "3          0.001  [False, False, False, False, False]   \n",
       "4          0.001  [False, False, False, False, False]   \n",
       "5          0.001  [False, False, False, False, False]   \n",
       "6           0.01    [False, False, False, True, True]   \n",
       "7           0.01    [False, False, False, True, True]   \n",
       "8           0.01  [False, False, False, False, False]   \n",
       "9           0.01  [False, False, False, False, False]   \n",
       "10          0.01  [False, False, False, False, False]   \n",
       "11          0.01  [False, False, False, False, False]   \n",
       "12           0.1  [False, False, False, False, False]   \n",
       "13           0.1  [False, False, False, False, False]   \n",
       "14           0.1  [False, False, False, False, False]   \n",
       "15           0.1  [False, False, False, False, False]   \n",
       "16           0.1  [False, False, False, False, False]   \n",
       "17           0.1  [False, False, False, False, False]   \n",
       "18         0.001       [True, True, True, True, True]   \n",
       "19         0.001       [True, True, True, True, True]   \n",
       "20         0.001       [True, True, True, True, True]   \n",
       "21         0.001       [True, True, True, True, True]   \n",
       "22         0.001       [True, True, True, True, True]   \n",
       "23         0.001       [True, True, True, True, True]   \n",
       "24          0.01       [True, True, True, True, True]   \n",
       "25          0.01       [True, True, True, True, True]   \n",
       "26          0.01       [True, True, True, True, True]   \n",
       "27          0.01       [True, True, True, True, True]   \n",
       "28          0.01       [True, True, True, True, True]   \n",
       "29          0.01       [True, True, True, True, True]   \n",
       "30           0.1       [True, True, True, True, True]   \n",
       "31           0.1       [True, True, True, True, True]   \n",
       "32           0.1       [True, True, True, True, True]   \n",
       "33           0.1       [True, True, True, True, True]   \n",
       "34           0.1       [True, True, True, True, True]   \n",
       "35           0.1       [True, True, True, True, True]   \n",
       "\n",
       "                                    validation_losses  \\\n",
       "0   [[1.2334464780565, 0.9929382153109731, 0.83348...   \n",
       "1   [[0.571559500828169, 0.46260801340678004, 0.40...   \n",
       "2   [[0.7202748805536567, 0.5960108189752704, 0.54...   \n",
       "3   [[0.8999277271121642, 0.9042320696322854, 0.91...   \n",
       "4   [[4.134041693741337, 10.812845872129204], [1.8...   \n",
       "5                                [[], [], [], [], []]   \n",
       "6   [[0.5288148363661361, 0.41737921156972974, 0.3...   \n",
       "7   [[0.24396183892380904, 0.1713048913483235, 0.1...   \n",
       "8   [[84.07095085701769], [nan], [4.82252291446771...   \n",
       "9                                [[], [], [], [], []]   \n",
       "10                               [[], [], [], [], []]   \n",
       "11                               [[], [], [], [], []]   \n",
       "12  [[0.39948955907877065, 9.5580019783381], [0.96...   \n",
       "13  [[3.298125657220015], [24.646350477774764], [5...   \n",
       "14                               [[], [], [], [], []]   \n",
       "15                               [[], [], [], [], []]   \n",
       "16                               [[], [], [], [], []]   \n",
       "17                               [[], [], [], [], []]   \n",
       "18  [[0.8086612543683216, 0.7838240974520597, 0.76...   \n",
       "19  [[0.5943959431219367, 0.5844835223993747, 0.57...   \n",
       "20  [[0.8749123928004863, 0.7226266656044958, 0.69...   \n",
       "21  [[0.8496312219532959, 0.7844169820815906, 0.74...   \n",
       "22  [[0.6954807923123058, 0.6942318054724493, 0.69...   \n",
       "23  [[0.7104309314205262, 0.6931875770444306, 0.69...   \n",
       "24  [[0.7033377843649671, 0.6795045519523014, 0.66...   \n",
       "25  [[0.543545200730683, 0.5080594193199424, 0.477...   \n",
       "26  [[0.6928435440016354, 0.6927932368359943, 0.69...   \n",
       "27  [[0.6959589122834879, 0.6940169610722303, 0.69...   \n",
       "28  [[0.6949066873603444, 0.6948879662916979, 0.69...   \n",
       "29  [[0.6988841782745242, 0.6988841790035082, 0.69...   \n",
       "30  [[0.5437626794533614, 0.4116011647524065, 0.33...   \n",
       "31  [[0.31061256311806934, 0.22881258392297874, 0....   \n",
       "32  [[1.0710278633165473, 1.0712480978043373, 1.07...   \n",
       "33  [[0.7173608602881799, 0.7179525203160825, 0.71...   \n",
       "34  [[0.7913048812275317, 0.7917157139767808, 0.79...   \n",
       "35  [[1.655088758263443, 1.6550887582695835, 1.655...   \n",
       "\n",
       "                                      training_losses num_iter  \n",
       "0   [[0.9495062401575112, 0.8133720555444218, 0.73...   1000.0  \n",
       "1   [[0.6713400062412406, 0.5928890608372823, 0.54...   1000.0  \n",
       "2   [[0.856512505048089, 0.747793020727742, 0.6845...    621.2  \n",
       "3   [[0.9860105884175189, 0.9891275796274209, 0.99...     25.4  \n",
       "4   [[4.537235663629631, nan], [1.7955302805772584...      3.0  \n",
       "5                                [[], [], [], [], []]      0.0  \n",
       "6   [[0.5893710807460183, 0.512832762659213, 0.452...    762.2  \n",
       "7   [[0.4532890628303428, 0.35828773587267687, 0.2...    625.6  \n",
       "8      [[nan], [nan], [7.647845614030195], [], [nan]]      0.8  \n",
       "9                                [[], [], [], [], []]      0.0  \n",
       "10                               [[], [], [], [], []]      0.0  \n",
       "11                               [[], [], [], [], []]      0.0  \n",
       "12  [[1.3577757175150287, 21.34987967827427], [1.2...      2.2  \n",
       "13  [[9.308968684573369], [36.298818204242615], [5...      1.2  \n",
       "14                               [[], [], [], [], []]      0.0  \n",
       "15                               [[], [], [], [], []]      0.0  \n",
       "16                               [[], [], [], [], []]      0.0  \n",
       "17                               [[], [], [], [], []]      0.0  \n",
       "18  [[0.7467873950391224, 0.7288419216370972, 0.71...   1000.0  \n",
       "19  [[0.5969220354366359, 0.591602178775377, 0.587...   1000.0  \n",
       "20  [[0.8735145331034436, 0.7221014017756288, 0.69...   1000.0  \n",
       "21  [[0.8492973924452568, 0.7842383909602058, 0.74...   1000.0  \n",
       "22  [[0.6954226335329524, 0.6941779350064206, 0.69...   1000.0  \n",
       "23  [[0.7104309312236585, 0.6931875768918799, 0.69...   1000.0  \n",
       "24  [[0.6755749296933777, 0.6605835560044867, 0.64...   1000.0  \n",
       "25  [[0.5655714521570097, 0.538158470787907, 0.514...   1000.0  \n",
       "26  [[0.6926219003283868, 0.6925565931906984, 0.69...   1000.0  \n",
       "27  [[0.6959565907996245, 0.69401162592777, 0.6939...   1000.0  \n",
       "28  [[0.6948503493522139, 0.6948317421856988, 0.69...   1000.0  \n",
       "29  [[0.698884178148291, 0.6988841788772754, 0.698...   1000.0  \n",
       "30  [[0.5618290438082242, 0.4788511573023306, 0.43...   1000.0  \n",
       "31  [[0.4049655480359375, 0.3157888862231593, 0.25...   1000.0  \n",
       "32  [[1.0693896209832887, 1.0697069113280733, 1.06...   1000.0  \n",
       "33  [[0.7173331382464797, 0.7179264906972361, 0.71...   1000.0  \n",
       "34  [[0.7911605492687277, 0.7915792627145308, 0.79...   1000.0  \n",
       "35  [[1.65508875826243, 1.6550887582685694, 1.6550...   1000.0  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_param = pd.DataFrame(model_params).transpose()\n",
    "df_param = df_param.reset_index(drop=False)\n",
    "df_param = df_param.rename(columns={'index': 'model'})\n",
    "\n",
    "df_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_param.to_csv(r'results/fertility_params.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation_function</th>\n",
       "      <th>converged</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>hidden_layer_sizes</th>\n",
       "      <th>num_iter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relu</td>\n",
       "      <td>[False, False, False, True, True]</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>621.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relu</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>25.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>relu</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>relu</td>\n",
       "      <td>[False, False, False, True, True]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>762.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>relu</td>\n",
       "      <td>[False, False, False, True, True]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>625.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>relu</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>relu</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>relu</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>relu</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>relu</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>relu</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>relu</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>relu</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>relu</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>relu</td>\n",
       "      <td>[False, False, False, False, False]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activation_function                            converged learning_rate  \\\n",
       "0                 relu       [True, True, True, True, True]         0.001   \n",
       "1                 relu       [True, True, True, True, True]         0.001   \n",
       "2                 relu    [False, False, False, True, True]         0.001   \n",
       "3                 relu  [False, False, False, False, False]         0.001   \n",
       "4                 relu  [False, False, False, False, False]         0.001   \n",
       "5                 relu  [False, False, False, False, False]         0.001   \n",
       "6                 relu    [False, False, False, True, True]          0.01   \n",
       "7                 relu    [False, False, False, True, True]          0.01   \n",
       "8                 relu  [False, False, False, False, False]          0.01   \n",
       "9                 relu  [False, False, False, False, False]          0.01   \n",
       "10                relu  [False, False, False, False, False]          0.01   \n",
       "11                relu  [False, False, False, False, False]          0.01   \n",
       "12                relu  [False, False, False, False, False]           0.1   \n",
       "13                relu  [False, False, False, False, False]           0.1   \n",
       "14                relu  [False, False, False, False, False]           0.1   \n",
       "15                relu  [False, False, False, False, False]           0.1   \n",
       "16                relu  [False, False, False, False, False]           0.1   \n",
       "17                relu  [False, False, False, False, False]           0.1   \n",
       "18             sigmoid       [True, True, True, True, True]         0.001   \n",
       "19             sigmoid       [True, True, True, True, True]         0.001   \n",
       "20             sigmoid       [True, True, True, True, True]         0.001   \n",
       "21             sigmoid       [True, True, True, True, True]         0.001   \n",
       "22             sigmoid       [True, True, True, True, True]         0.001   \n",
       "23             sigmoid       [True, True, True, True, True]         0.001   \n",
       "24             sigmoid       [True, True, True, True, True]          0.01   \n",
       "25             sigmoid       [True, True, True, True, True]          0.01   \n",
       "26             sigmoid       [True, True, True, True, True]          0.01   \n",
       "27             sigmoid       [True, True, True, True, True]          0.01   \n",
       "28             sigmoid       [True, True, True, True, True]          0.01   \n",
       "29             sigmoid       [True, True, True, True, True]          0.01   \n",
       "30             sigmoid       [True, True, True, True, True]           0.1   \n",
       "31             sigmoid       [True, True, True, True, True]           0.1   \n",
       "32             sigmoid       [True, True, True, True, True]           0.1   \n",
       "33             sigmoid       [True, True, True, True, True]           0.1   \n",
       "34             sigmoid       [True, True, True, True, True]           0.1   \n",
       "35             sigmoid       [True, True, True, True, True]           0.1   \n",
       "\n",
       "   hidden_layer_sizes num_iter  \n",
       "0                (5,)   1000.0  \n",
       "1               (32,)   1000.0  \n",
       "2            (16, 16)    621.2  \n",
       "3          (10, 5, 5)     25.4  \n",
       "4          (16, 8, 8)      3.0  \n",
       "5        (64, 32, 32)      0.0  \n",
       "6                (5,)    762.2  \n",
       "7               (32,)    625.6  \n",
       "8            (16, 16)      0.8  \n",
       "9          (10, 5, 5)      0.0  \n",
       "10         (16, 8, 8)      0.0  \n",
       "11       (64, 32, 32)      0.0  \n",
       "12               (5,)      2.2  \n",
       "13              (32,)      1.2  \n",
       "14           (16, 16)      0.0  \n",
       "15         (10, 5, 5)      0.0  \n",
       "16         (16, 8, 8)      0.0  \n",
       "17       (64, 32, 32)      0.0  \n",
       "18               (5,)   1000.0  \n",
       "19              (32,)   1000.0  \n",
       "20           (16, 16)   1000.0  \n",
       "21         (10, 5, 5)   1000.0  \n",
       "22         (16, 8, 8)   1000.0  \n",
       "23       (64, 32, 32)   1000.0  \n",
       "24               (5,)   1000.0  \n",
       "25              (32,)   1000.0  \n",
       "26           (16, 16)   1000.0  \n",
       "27         (10, 5, 5)   1000.0  \n",
       "28         (16, 8, 8)   1000.0  \n",
       "29       (64, 32, 32)   1000.0  \n",
       "30               (5,)   1000.0  \n",
       "31              (32,)   1000.0  \n",
       "32           (16, 16)   1000.0  \n",
       "33         (10, 5, 5)   1000.0  \n",
       "34         (16, 8, 8)   1000.0  \n",
       "35       (64, 32, 32)   1000.0  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_param.loc[:,['activation_function', 'converged', 'learning_rate', 'hidden_layer_sizes', 'num_iter']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
