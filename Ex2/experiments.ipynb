{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "\n",
    "from utils import data_preprocessing_util as dpu\n",
    "from utils import classification_util as cu\n",
    "\n",
    "from MLP import MLP\n",
    "from nn_framework import NNFramework\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fertility Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'data/fertility_diagnosis.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "df = dpu.preprocess_fertility_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NNFramework()\n",
    "nn.fit_encoder(df=df, cols_to_encode=df.columns.difference(['age', 'hours_sitting']))\n",
    "df_encoded = nn.encode_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_encoded.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1038\n",
    "scaling = True\n",
    "oversampling = True\n",
    "\n",
    "scaler = preprocessing.StandardScaler() if scaling else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_encoded['diagnosis']\n",
    "X = df_encoded[df_encoded.columns.difference(['diagnosis'])]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = random_seed, shuffle=True, stratify=y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Params to check out:\n",
    "- 2 activation functions\n",
    "- 3 lrs: 0.001, 0.01, 0.1\n",
    "- number of nodes per layer\n",
    "- 1 layer:\n",
    "    - 5\n",
    "    - 32\n",
    "- 2 layer:\n",
    "    - 16 16\n",
    "- 3 layers\n",
    "    - 10 5 5\n",
    "    - 16 8 8\n",
    "    - 64 32 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = ['relu', 'sigmoid']\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "hidden_layer_sizes = [(5,), (32,), (16, 16), (10, 5, 5), (16, 8, 8), (64, 32, 32),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = []\n",
    "\n",
    "for af in activation_functions:\n",
    "    for lr in learning_rates:\n",
    "        for hls in hidden_layer_sizes:\n",
    "            methods.append((f'MLP-{af}-{lr}-{hls}', MLP(n_iter=1000, activation_function=af, learning_rate=lr, hidden_layer_sizes=hls)))\n",
    "    \n",
    "pipelines = cu.define_pipelines(methods, scaler=scaler, oversampling=oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP-relu-0.001-(5,)\n",
      "f1 scores: [0.39393939 0.56709957 0.60784314 0.49820789 0.44444444]\n",
      "f1 mean: 0.502\n",
      "f1 std: 0.078\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-relu-0.001-(32,)\n",
      "f1 scores: [0.49820789 0.56709957 0.72222222 0.42857143 0.60784314]\n",
      "f1 mean: 0.565\n",
      "f1 std: 0.100\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.001-(16, 16)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.41176471 0.39393939]\n",
      "f1 mean: 0.216\n",
      "f1 std: 0.153\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.001-(10, 5, 5)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.45945946 0.45945946]\n",
      "f1 mean: 0.238\n",
      "f1 std: 0.181\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.001-(16, 8, 8)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.13043478 0.13043478]\n",
      "f1 mean: 0.336\n",
      "f1 std: 0.168\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.001-(64, 32, 32)\n",
      "f1 scores: [0.45945946 0.45945946 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.462\n",
      "f1 std: 0.006\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(5,)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.53125    0.44444444]\n",
      "f1 mean: 0.250\n",
      "f1 std: 0.196\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(32,)\n",
      "f1 scores: [0.09090909 0.2        0.04761905 0.42857143 0.60784314]\n",
      "f1 mean: 0.275\n",
      "f1 std: 0.212\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(16, 16)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.45945946 0.45945946]\n",
      "f1 mean: 0.238\n",
      "f1 std: 0.181\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(10, 5, 5)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.45945946 0.45945946]\n",
      "f1 mean: 0.238\n",
      "f1 std: 0.181\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(16, 8, 8)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.13043478 0.13043478]\n",
      "f1 mean: 0.336\n",
      "f1 std: 0.168\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(64, 32, 32)\n",
      "f1 scores: [0.45945946 0.45945946 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.462\n",
      "f1 std: 0.006\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(5,)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.34837093 0.41333333]\n",
      "f1 mean: 0.207\n",
      "f1 std: 0.144\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(32,)\n",
      "f1 scores: [0.09090909 0.2        0.04761905 0.44444444 0.45945946]\n",
      "f1 mean: 0.248\n",
      "f1 std: 0.173\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(16, 16)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.45945946 0.45945946]\n",
      "f1 mean: 0.238\n",
      "f1 std: 0.181\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(10, 5, 5)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.45945946 0.45945946]\n",
      "f1 mean: 0.238\n",
      "f1 std: 0.181\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(16, 8, 8)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.13043478 0.13043478]\n",
      "f1 mean: 0.336\n",
      "f1 std: 0.168\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(64, 32, 32)\n",
      "f1 scores: [0.45945946 0.45945946 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.462\n",
      "f1 std: 0.006\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.001-(5,)\n",
      "f1 scores: [0.28571429 0.65714286 0.60784314 0.375      0.41176471]\n",
      "f1 mean: 0.467\n",
      "f1 std: 0.142\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.001-(32,)\n",
      "f1 scores: [0.37321937 0.74025974 0.41176471 0.42857143 0.39393939]\n",
      "f1 mean: 0.470\n",
      "f1 std: 0.137\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.001-(16, 16)\n",
      "f1 scores: [0.45054945 0.35483871 0.37321937 0.45945946 0.2481203 ]\n",
      "f1 mean: 0.377\n",
      "f1 std: 0.077\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.001-(10, 5, 5)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.13043478 0.13043478]\n",
      "f1 mean: 0.336\n",
      "f1 std: 0.168\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.001-(16, 8, 8)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.468\n",
      "f1 std: 0.007\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.001-(64, 32, 32)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.468\n",
      "f1 std: 0.007\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.01-(5,)\n",
      "f1 scores: [0.4047619  0.81981982 0.44444444 0.375      0.41176471]\n",
      "f1 mean: 0.491\n",
      "f1 std: 0.166\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.01-(32,)\n",
      "f1 scores: [0.37321937 0.74025974 0.42857143 0.42857143 0.42857143]\n",
      "f1 mean: 0.480\n",
      "f1 std: 0.132\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.01-(16, 16)\n",
      "f1 scores: [0.49820789 0.65714286 0.375      0.39393939 0.42857143]\n",
      "f1 mean: 0.471\n",
      "f1 std: 0.102\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cv_num = 5\n",
    "model_params = {}\n",
    "models = {}\n",
    "\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    cv_results = cross_validate(pipeline, X, y, cv=cv_num, scoring='f1_macro', return_estimator=True)\n",
    "\n",
    "    models[model_name] = cv_results['estimator']\n",
    "    model_params[model_name] = {}\n",
    "\n",
    "    num_cols = ['test_score', 'fit_time', 'score_time']\n",
    "\n",
    "    for num_col in num_cols:\n",
    "        model_params[model_name][num_col] = cv_results[num_col]\n",
    "        model_params[model_name][f'{num_col}_mean'] = cv_results[num_col].mean()\n",
    "        model_params[model_name][f'{num_col}_std'] = cv_results[num_col].std()\n",
    "    \n",
    "    model_params[model_name]['parameter_num'] = cv_results['estimator'][0][model_name].number_of_params_\n",
    "    model_params[model_name]['hidden_layer_sizes'] = cv_results['estimator'][0][model_name].hidden_layer_sizes\n",
    "    model_params[model_name]['activation_function'] = cv_results['estimator'][0][model_name].activation_function\n",
    "    model_params[model_name]['learning_rate'] = cv_results['estimator'][0][model_name].learning_rate\n",
    "    model_params[model_name]['converged'] = [e[model_name].converged_ for e in cv_results['estimator']]\n",
    "\n",
    "    print(model_name)\n",
    "    print(\n",
    "        f\"f1 scores: {model_params[model_name]['test_score']}\\n\" +\n",
    "        f\"f1 mean: {model_params[model_name]['test_score_mean']:.3f}\\n\" +\n",
    "        f\"f1 std: {model_params[model_name]['test_score_std']:.3f}\\n\"\n",
    "    )\n",
    "    print('----------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MLP' object has no attribute 'converged'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cv_results[\u001b[39m'\u001b[39;49m\u001b[39mestimator\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m][model_name]\u001b[39m.\u001b[39;49mconverged\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MLP' object has no attribute 'converged'"
     ]
    }
   ],
   "source": [
    "cv_results['estimator'][0][model_name].converged_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_mean</th>\n",
       "      <th>test_score_std</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>fit_time_mean</th>\n",
       "      <th>fit_time_std</th>\n",
       "      <th>score_time</th>\n",
       "      <th>score_time_mean</th>\n",
       "      <th>score_time_std</th>\n",
       "      <th>parameter_num</th>\n",
       "      <th>hidden_layer_sizes</th>\n",
       "      <th>activation_function</th>\n",
       "      <th>layer_num</th>\n",
       "      <th>layer_sizes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP-relu-1-5</td>\n",
       "      <td>[0.31034482758620685, 0.5670995670995671, 0.49...</td>\n",
       "      <td>0.447992</td>\n",
       "      <td>0.085067</td>\n",
       "      <td>[0.588165283203125, 0.5871419906616211, 0.5853...</td>\n",
       "      <td>0.582532</td>\n",
       "      <td>0.006307</td>\n",
       "      <td>[0.0030002593994140625, 0.0020008087158203125,...</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>117</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP-sigmoid-1-5</td>\n",
       "      <td>[0.34065934065934067, 0.6571428571428571, 0.60...</td>\n",
       "      <td>0.478482</td>\n",
       "      <td>0.128693</td>\n",
       "      <td>[0.6839420795440674, 0.6629798412322998, 0.674...</td>\n",
       "      <td>0.676983</td>\n",
       "      <td>0.013858</td>\n",
       "      <td>[0.0019998550415039062, 0.002000093460083008, ...</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>117</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model                                         test_score  \\\n",
       "0     MLP-relu-1-5  [0.31034482758620685, 0.5670995670995671, 0.49...   \n",
       "1  MLP-sigmoid-1-5  [0.34065934065934067, 0.6571428571428571, 0.60...   \n",
       "\n",
       "  test_score_mean test_score_std  \\\n",
       "0        0.447992       0.085067   \n",
       "1        0.478482       0.128693   \n",
       "\n",
       "                                            fit_time fit_time_mean  \\\n",
       "0  [0.588165283203125, 0.5871419906616211, 0.5853...      0.582532   \n",
       "1  [0.6839420795440674, 0.6629798412322998, 0.674...      0.676983   \n",
       "\n",
       "  fit_time_std                                         score_time  \\\n",
       "0     0.006307  [0.0030002593994140625, 0.0020008087158203125,...   \n",
       "1     0.013858  [0.0019998550415039062, 0.002000093460083008, ...   \n",
       "\n",
       "  score_time_mean score_time_std parameter_num hidden_layer_sizes  \\\n",
       "0        0.002401        0.00049           117               (5,)   \n",
       "1          0.0022         0.0004           117               (5,)   \n",
       "\n",
       "  activation_function  layer_num  layer_sizes  \n",
       "0                relu          1            5  \n",
       "1             sigmoid          1            5  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_param = pd.DataFrame(model_params).transpose()\n",
    "df_param = df_param.reset_index(drop=False)\n",
    "df_param = df_param.rename(columns={'index': 'model'})\n",
    "\n",
    "# df_param['activation_function'] = df_param.model.str.extract(r'MLP-(\\w+)-.*')\n",
    "# df_param['layer_num'] = df_param.model.str.extract(r'MLP-\\w+-(\\d+).*').astype(int)\n",
    "# df_param['layer_sizes'] = df_param.model.str.extract(r'MLP-\\w+-\\d-(\\d+)').astype(int)\n",
    "\n",
    "df_param"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
