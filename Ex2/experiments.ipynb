{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "\n",
    "from utils import data_preprocessing_util as dpu\n",
    "from utils import classification_util as cu\n",
    "\n",
    "from MLP import MLP\n",
    "from nn_framework import NNFramework\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fertility Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'data/fertility_diagnosis.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "df = dpu.preprocess_fertility_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NNFramework()\n",
    "nn.fit_encoder(df=df, cols_to_encode=df.columns.difference(['age', 'hours_sitting']))\n",
    "df_encoded = nn.encode_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_encoded.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1038\n",
    "scaling = True\n",
    "oversampling = True\n",
    "\n",
    "scaler = preprocessing.StandardScaler() if scaling else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_encoded['diagnosis']\n",
    "X = df_encoded[df_encoded.columns.difference(['diagnosis'])]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = random_seed, shuffle=True, stratify=y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Params to check out:\n",
    "- 2 activation functions\n",
    "- 3 lrs: 0.001, 0.01, 0.1\n",
    "- number of nodes per layer\n",
    "- 1 layer:\n",
    "    - 5\n",
    "    - 32\n",
    "- 2 layer:\n",
    "    - 16 16\n",
    "- 3 layers\n",
    "    - 10 5 5\n",
    "    - 16 8 8\n",
    "    - 64 32 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = ['relu', 'sigmoid']\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "hidden_layer_sizes = [(5,), (32,), (16, 16), (10, 5, 5), (16, 8, 8), (64, 32, 32),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = []\n",
    "\n",
    "for af in activation_functions:\n",
    "    for lr in learning_rates:\n",
    "        for hls in hidden_layer_sizes:\n",
    "            methods.append((f'MLP-{af}-{lr}-{hls}', MLP(n_iter=1000, activation_function=af, learning_rate=lr, hidden_layer_sizes=hls)))\n",
    "    \n",
    "pipelines = cu.define_pipelines(methods, scaler=scaler, oversampling=oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP-relu-0.001-(5,)\n",
      "f1 scores: [0.39393939 0.56709957 0.60784314 0.49820789 0.44444444]\n",
      "f1 mean: 0.502\n",
      "f1 std: 0.078\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-relu-0.001-(32,)\n",
      "f1 scores: [0.49820789 0.56709957 0.72222222 0.42857143 0.60784314]\n",
      "f1 mean: 0.565\n",
      "f1 std: 0.100\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.001-(16, 16)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.41176471 0.39393939]\n",
      "f1 mean: 0.216\n",
      "f1 std: 0.153\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.001-(10, 5, 5)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.45945946 0.45945946]\n",
      "f1 mean: 0.238\n",
      "f1 std: 0.181\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.001-(16, 8, 8)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.13043478 0.13043478]\n",
      "f1 mean: 0.336\n",
      "f1 std: 0.168\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.001-(64, 32, 32)\n",
      "f1 scores: [0.45945946 0.45945946 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.462\n",
      "f1 std: 0.006\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(5,)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.53125    0.44444444]\n",
      "f1 mean: 0.250\n",
      "f1 std: 0.196\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(32,)\n",
      "f1 scores: [0.09090909 0.2        0.04761905 0.42857143 0.60784314]\n",
      "f1 mean: 0.275\n",
      "f1 std: 0.212\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(16, 16)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.45945946 0.45945946]\n",
      "f1 mean: 0.238\n",
      "f1 std: 0.181\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(10, 5, 5)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.45945946 0.45945946]\n",
      "f1 mean: 0.238\n",
      "f1 std: 0.181\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(16, 8, 8)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.13043478 0.13043478]\n",
      "f1 mean: 0.336\n",
      "f1 std: 0.168\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(64, 32, 32)\n",
      "f1 scores: [0.45945946 0.45945946 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.462\n",
      "f1 std: 0.006\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(5,)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.34837093 0.41333333]\n",
      "f1 mean: 0.207\n",
      "f1 std: 0.144\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(32,)\n",
      "f1 scores: [0.09090909 0.2        0.04761905 0.44444444 0.45945946]\n",
      "f1 mean: 0.248\n",
      "f1 std: 0.173\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(16, 16)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.45945946 0.45945946]\n",
      "f1 mean: 0.238\n",
      "f1 std: 0.181\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(10, 5, 5)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.45945946 0.45945946]\n",
      "f1 mean: 0.238\n",
      "f1 std: 0.181\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(16, 8, 8)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.13043478 0.13043478]\n",
      "f1 mean: 0.336\n",
      "f1 std: 0.168\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(64, 32, 32)\n",
      "f1 scores: [0.45945946 0.45945946 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.462\n",
      "f1 std: 0.006\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.001-(5,)\n",
      "f1 scores: [0.28571429 0.65714286 0.60784314 0.375      0.41176471]\n",
      "f1 mean: 0.467\n",
      "f1 std: 0.142\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.001-(32,)\n",
      "f1 scores: [0.37321937 0.74025974 0.41176471 0.42857143 0.39393939]\n",
      "f1 mean: 0.470\n",
      "f1 std: 0.137\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.001-(16, 16)\n",
      "f1 scores: [0.45054945 0.35483871 0.37321937 0.45945946 0.2481203 ]\n",
      "f1 mean: 0.377\n",
      "f1 std: 0.077\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.001-(10, 5, 5)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.13043478 0.13043478]\n",
      "f1 mean: 0.336\n",
      "f1 std: 0.168\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.001-(16, 8, 8)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.468\n",
      "f1 std: 0.007\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.001-(64, 32, 32)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.468\n",
      "f1 std: 0.007\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.01-(5,)\n",
      "f1 scores: [0.4047619  0.81981982 0.44444444 0.375      0.41176471]\n",
      "f1 mean: 0.491\n",
      "f1 std: 0.166\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.01-(32,)\n",
      "f1 scores: [0.37321937 0.74025974 0.42857143 0.42857143 0.42857143]\n",
      "f1 mean: 0.480\n",
      "f1 std: 0.132\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.01-(16, 16)\n",
      "f1 scores: [0.49820789 0.65714286 0.375      0.39393939 0.42857143]\n",
      "f1 mean: 0.471\n",
      "f1 std: 0.102\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.01-(10, 5, 5)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.45945946 0.37321937]\n",
      "f1 mean: 0.451\n",
      "f1 std: 0.039\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.01-(16, 8, 8)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.468\n",
      "f1 std: 0.007\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.01-(64, 32, 32)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.468\n",
      "f1 std: 0.007\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.1-(5,)\n",
      "f1 scores: [0.49820789 0.72222222 0.60784314 0.39393939 0.56709957]\n",
      "f1 mean: 0.558\n",
      "f1 std: 0.110\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.1-(32,)\n",
      "f1 scores: [0.30666667 0.53125    0.41176471 0.44444444 0.56709957]\n",
      "f1 mean: 0.452\n",
      "f1 std: 0.092\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.1-(16, 16)\n",
      "f1 scores: [0.34065934 0.60784314 0.375      0.35483871 0.42857143]\n",
      "f1 mean: 0.421\n",
      "f1 std: 0.098\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.1-(10, 5, 5)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.39393939 0.60784314]\n",
      "f1 mean: 0.485\n",
      "f1 std: 0.069\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.1-(16, 8, 8)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.468\n",
      "f1 std: 0.007\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MLP-sigmoid-0.1-(64, 32, 32)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.468\n",
      "f1 std: 0.007\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cv_num = 5\n",
    "model_params = {}\n",
    "models = {}\n",
    "\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    cv_results = cross_validate(pipeline, X, y, cv=cv_num, scoring='f1_macro', return_estimator=True)\n",
    "\n",
    "    models[model_name] = cv_results['estimator']\n",
    "    model_params[model_name] = {}\n",
    "\n",
    "    num_cols = ['test_score', 'fit_time', 'score_time']\n",
    "\n",
    "    for num_col in num_cols:\n",
    "        model_params[model_name][num_col] = cv_results[num_col]\n",
    "        model_params[model_name][f'{num_col}_mean'] = cv_results[num_col].mean()\n",
    "        model_params[model_name][f'{num_col}_std'] = cv_results[num_col].std()\n",
    "    \n",
    "    model_params[model_name]['parameter_num'] = cv_results['estimator'][0][model_name].number_of_params_\n",
    "    model_params[model_name]['hidden_layer_sizes'] = cv_results['estimator'][0][model_name].hidden_layer_sizes\n",
    "    model_params[model_name]['activation_function'] = cv_results['estimator'][0][model_name].activation_function\n",
    "    model_params[model_name]['learning_rate'] = cv_results['estimator'][0][model_name].learning_rate\n",
    "    model_params[model_name]['converged'] = [e[model_name].converged_ for e in cv_results['estimator']]\n",
    "    model_params[model_name]['validation_losses'] = [e[model_name].validation_losses_ for e in cv_results['estimator']]\n",
    "    model_params[model_name]['training_losses'] = [e[model_name].training_losses_ for e in cv_results['estimator']]\n",
    "\n",
    "    print(model_name)\n",
    "    print(\n",
    "        f\"f1 scores: {model_params[model_name]['test_score']}\\n\" +\n",
    "        f\"f1 mean: {model_params[model_name]['test_score_mean']:.3f}\\n\" +\n",
    "        f\"f1 std: {model_params[model_name]['test_score_std']:.3f}\\n\"\n",
    "    )\n",
    "    print('----------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_param = pd.DataFrame(model_params).transpose()\n",
    "df_param = df_param.reset_index(drop=False)\n",
    "df_param = df_param.rename(columns={'index': 'model'})\n",
    "\n",
    "# df_param['activation_function'] = df_param.model.str.extract(r'MLP-(\\w+)-.*')\n",
    "# df_param['layer_num'] = df_param.model.str.extract(r'MLP-\\w+-(\\d+).*').astype(int)\n",
    "# df_param['layer_sizes'] = df_param.model.str.extract(r'MLP-\\w+-\\d-(\\d+)').astype(int)\n",
    "\n",
    "# df_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_param.to_csv(r'results/fertility_params.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_param\u001b[39m.\u001b[39;49mgroupby([\u001b[39m'\u001b[39;49m\u001b[39mactivation_function\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mconverged\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mcount()\n",
      "File \u001b[1;32mc:\\Users\\Alina\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:2070\u001b[0m, in \u001b[0;36mGroupBy.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2061\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2062\u001b[0m \u001b[39mCompute count of group, excluding missing values.\u001b[39;00m\n\u001b[0;32m   2063\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2067\u001b[0m \u001b[39m    Count of values within each group.\u001b[39;00m\n\u001b[0;32m   2068\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2069\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_data_to_aggregate()\n\u001b[1;32m-> 2070\u001b[0m ids, _, ngroups \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrouper\u001b[39m.\u001b[39;49mgroup_info\n\u001b[0;32m   2071\u001b[0m mask \u001b[39m=\u001b[39m ids \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m   2073\u001b[0m is_series \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Alina\\anaconda3\\lib\\site-packages\\pandas\\_libs\\properties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Alina\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:946\u001b[0m, in \u001b[0;36mBaseGrouper.group_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[39m@cache_readonly\u001b[39m\n\u001b[0;32m    945\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgroup_info\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39mintp], npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39mintp], \u001b[39mint\u001b[39m]:\n\u001b[1;32m--> 946\u001b[0m     comp_ids, obs_group_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_compressed_codes()\n\u001b[0;32m    948\u001b[0m     ngroups \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(obs_group_ids)\n\u001b[0;32m    949\u001b[0m     comp_ids \u001b[39m=\u001b[39m ensure_platform_int(comp_ids)\n",
      "File \u001b[1;32mc:\\Users\\Alina\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:972\u001b[0m, in \u001b[0;36mBaseGrouper._get_compressed_codes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m    967\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_compressed_codes\u001b[39m(\n\u001b[0;32m    968\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    969\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39msignedinteger], npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39mintp]]:\n\u001b[0;32m    970\u001b[0m     \u001b[39m# The first returned ndarray may have any signed integer dtype\u001b[39;00m\n\u001b[0;32m    971\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroupings) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 972\u001b[0m         group_index \u001b[39m=\u001b[39m get_group_index(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcodes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape, sort\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, xnull\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    973\u001b[0m         \u001b[39mreturn\u001b[39;00m compress_group_index(group_index, sort\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sort)\n\u001b[0;32m    974\u001b[0m         \u001b[39m# FIXME: compress_group_index's second return value is int64, not intp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Alina\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:897\u001b[0m, in \u001b[0;36mBaseGrouper.codes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m    895\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    896\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcodes\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39msignedinteger]]:\n\u001b[1;32m--> 897\u001b[0m     \u001b[39mreturn\u001b[39;00m [ping\u001b[39m.\u001b[39mcodes \u001b[39mfor\u001b[39;00m ping \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroupings]\n",
      "File \u001b[1;32mc:\\Users\\Alina\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:897\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m    895\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    896\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcodes\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39msignedinteger]]:\n\u001b[1;32m--> 897\u001b[0m     \u001b[39mreturn\u001b[39;00m [ping\u001b[39m.\u001b[39;49mcodes \u001b[39mfor\u001b[39;00m ping \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroupings]\n",
      "File \u001b[1;32mc:\\Users\\Alina\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py:621\u001b[0m, in \u001b[0;36mGrouping.codes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_codes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    618\u001b[0m     \u001b[39m# _codes is set in __init__ for MultiIndex cases\u001b[39;00m\n\u001b[0;32m    619\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_codes\n\u001b[1;32m--> 621\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_codes_and_uniques[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Alina\\anaconda3\\lib\\site-packages\\pandas\\_libs\\properties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Alina\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py:692\u001b[0m, in \u001b[0;36mGrouping._codes_and_uniques\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    685\u001b[0m     uniques \u001b[39m=\u001b[39m (\n\u001b[0;32m    686\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrouping_vector\u001b[39m.\u001b[39mresult_index\u001b[39m.\u001b[39m_values  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    687\u001b[0m     )\n\u001b[0;32m    688\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# GH35667, replace dropna=False with use_na_sentinel=False\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     \u001b[39m# error: Incompatible types in assignment (expression has type \"Union[\u001b[39;00m\n\u001b[0;32m    691\u001b[0m     \u001b[39m# ndarray[Any, Any], Index]\", variable has type \"Categorical\")\u001b[39;00m\n\u001b[1;32m--> 692\u001b[0m     codes, uniques \u001b[39m=\u001b[39m algorithms\u001b[39m.\u001b[39;49mfactorize(  \u001b[39m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[0;32m    693\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrouping_vector, sort\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sort, use_na_sentinel\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dropna\n\u001b[0;32m    694\u001b[0m     )\n\u001b[0;32m    695\u001b[0m \u001b[39mreturn\u001b[39;00m codes, uniques\n",
      "File \u001b[1;32mc:\\Users\\Alina\\anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py:822\u001b[0m, in \u001b[0;36mfactorize\u001b[1;34m(values, sort, na_sentinel, use_na_sentinel, size_hint)\u001b[0m\n\u001b[0;32m    819\u001b[0m             \u001b[39m# Don't modify (potentially user-provided) array\u001b[39;00m\n\u001b[0;32m    820\u001b[0m             values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(null_mask, na_value, values)\n\u001b[1;32m--> 822\u001b[0m     codes, uniques \u001b[39m=\u001b[39m factorize_array(\n\u001b[0;32m    823\u001b[0m         values,\n\u001b[0;32m    824\u001b[0m         na_sentinel\u001b[39m=\u001b[39;49mna_sentinel_arg,\n\u001b[0;32m    825\u001b[0m         size_hint\u001b[39m=\u001b[39;49msize_hint,\n\u001b[0;32m    826\u001b[0m     )\n\u001b[0;32m    828\u001b[0m \u001b[39mif\u001b[39;00m sort \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    829\u001b[0m     \u001b[39mif\u001b[39;00m na_sentinel \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    830\u001b[0m         \u001b[39m# TODO: Can remove when na_sentinel=na_sentinel as in TODO above\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Alina\\anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py:578\u001b[0m, in \u001b[0;36mfactorize_array\u001b[1;34m(values, na_sentinel, size_hint, na_value, mask)\u001b[0m\n\u001b[0;32m    575\u001b[0m hash_klass, values \u001b[39m=\u001b[39m _get_hashtable_algo(values)\n\u001b[0;32m    577\u001b[0m table \u001b[39m=\u001b[39m hash_klass(size_hint \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(values))\n\u001b[1;32m--> 578\u001b[0m uniques, codes \u001b[39m=\u001b[39m table\u001b[39m.\u001b[39;49mfactorize(\n\u001b[0;32m    579\u001b[0m     values,\n\u001b[0;32m    580\u001b[0m     na_sentinel\u001b[39m=\u001b[39;49mna_sentinel,\n\u001b[0;32m    581\u001b[0m     na_value\u001b[39m=\u001b[39;49mna_value,\n\u001b[0;32m    582\u001b[0m     mask\u001b[39m=\u001b[39;49mmask,\n\u001b[0;32m    583\u001b[0m     ignore_na\u001b[39m=\u001b[39;49mignore_na,\n\u001b[0;32m    584\u001b[0m )\n\u001b[0;32m    586\u001b[0m \u001b[39m# re-cast e.g. i8->dt64/td64, uint8->bool\u001b[39;00m\n\u001b[0;32m    587\u001b[0m uniques \u001b[39m=\u001b[39m _reconstruct_data(uniques, original\u001b[39m.\u001b[39mdtype, original)\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5943\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.factorize\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5857\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "df_param.groupby(['activation_function', 'converged']).model.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['activation_function', 'converged'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[39m.\u001b[39;49mloc[:,[\u001b[39m'\u001b[39;49m\u001b[39mactivation_function\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mconverged\u001b[39;49m\u001b[39m'\u001b[39;49m]]\n",
      "File \u001b[1;32mc:\\Users\\Alina\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1067\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1065\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1066\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_value(\u001b[39m*\u001b[39mkey, takeable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_takeable)\n\u001b[1;32m-> 1067\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple(key)\n\u001b[0;32m   1068\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1069\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Alina\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1256\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multi_take_opportunity(tup):\n\u001b[0;32m   1254\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multi_take(tup)\n\u001b[1;32m-> 1256\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple_same_dim(tup)\n",
      "File \u001b[1;32mc:\\Users\\Alina\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:924\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_tuple_same_dim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mis_null_slice(key):\n\u001b[0;32m    922\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m--> 924\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(retval, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\u001b[39m.\u001b[39;49m_getitem_axis(key, axis\u001b[39m=\u001b[39;49mi)\n\u001b[0;32m    925\u001b[0m \u001b[39m# We should never have retval.ndim < self.ndim, as that should\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[39m#  be handled by the _getitem_lowerdim call above.\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39massert\u001b[39;00m retval\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim\n",
      "File \u001b[1;32mc:\\Users\\Alina\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1301\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1298\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(key, \u001b[39m\"\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1299\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot index with multidimensional key\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1301\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_iterable(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   1303\u001b[0m \u001b[39m# nested tuple slicing\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m \u001b[39mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[1;32mc:\\Users\\Alina\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1239\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1236\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m   1238\u001b[0m \u001b[39m# A collection of keys\u001b[39;00m\n\u001b[1;32m-> 1239\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_listlike_indexer(key, axis)\n\u001b[0;32m   1240\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   1241\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_dups\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1242\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Alina\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1432\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1429\u001b[0m ax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1430\u001b[0m axis_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis_name(axis)\n\u001b[1;32m-> 1432\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39;49m_get_indexer_strict(key, axis_name)\n\u001b[0;32m   1434\u001b[0m \u001b[39mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32mc:\\Users\\Alina\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6067\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6070\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6072\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   6073\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6074\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Alina\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6130\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6128\u001b[0m     \u001b[39mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   6129\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 6130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6132\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[0;32m   6133\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['activation_function', 'converged'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "df.loc[:,['activation_function', 'converged']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
