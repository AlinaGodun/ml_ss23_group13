{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "\n",
    "from utils import data_preprocessing_util as dpu\n",
    "from utils import classification_util as cu\n",
    "\n",
    "from MLP import MLP\n",
    "from nn_framework import NNFramework\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'../Ex1/data/Breast_Cancer/breast-cancer-diagnostic.shuf.lrn.csv'\n",
    "df_original = pd.read_csv(data_path)\n",
    "\n",
    "random_seed = 32\n",
    "log_transform = True\n",
    "outlier_removal = True\n",
    "scaling = True\n",
    "\n",
    "scaler = preprocessing.StandardScaler() if scaling else None\n",
    "\n",
    "df = dpu.preprocess_breast_cancer_data(df_original, log_transform=log_transform, outlier_removal=outlier_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['class']\n",
    "X = df[df.columns.difference(['ID', 'class'])]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = random_seed, shuffle=True, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = ['relu', 'sigmoid']\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 0.5]\n",
    "hidden_layer_sizes = [(5,), (32,), (16, 16), (10, 5, 5), (16, 8, 8), (64, 32, 32),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = []\n",
    "\n",
    "for af in activation_functions:\n",
    "    for lr in learning_rates:\n",
    "        for hls in hidden_layer_sizes:\n",
    "            methods.append((f'MLP-{af}-{lr}-{hls}', MLP(n_iter=5000, activation_function=af, learning_rate=lr, hidden_layer_sizes=hls)))\n",
    "    \n",
    "pipelines = cu.define_pipelines(methods, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 294...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 278...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 264...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 321...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 298...\n",
      "MLP-relu-0.0001-(5,)\n",
      "f1 scores: [0.93215739 0.97802198 0.95659722 0.97854998 0.89010989]\n",
      "f1 mean: 0.947\n",
      "f1 std: 0.033\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 227...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 237...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 246...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 228...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 252...\n",
      "MLP-relu-0.0001-(32,)\n",
      "f1 scores: [0.93215739 0.97802198 0.95659722 0.97854998 0.93215739]\n",
      "f1 mean: 0.955\n",
      "f1 std: 0.021\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 190...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 198...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 199...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 210...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 247...\n",
      "MLP-relu-0.0001-(16, 16)\n",
      "f1 scores: [0.80952381 0.95659722 0.91319444 0.97854998 0.84984985]\n",
      "f1 mean: 0.902\n",
      "f1 std: 0.064\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 222...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 189...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 212...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 199...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 267...\n",
      "MLP-relu-0.0001-(10, 5, 5)\n",
      "f1 scores: [0.83637214 0.93406593 0.91319444 0.97854998 0.84984985]\n",
      "f1 mean: 0.902\n",
      "f1 std: 0.053\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 178...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 192...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 185...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 202...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 236...\n",
      "MLP-relu-0.0001-(16, 8, 8)\n",
      "f1 scores: [0.80952381 0.95659722 0.91319444 0.97854998 0.84984985]\n",
      "f1 mean: 0.902\n",
      "f1 std: 0.064\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 12...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 84...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 97...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.0001-(64, 32, 32)\n",
      "f1 scores: [0.83022071 0.875      0.875      0.87684729 0.47552448]\n",
      "f1 mean: 0.787\n",
      "f1 std: 0.156\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 81...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 80...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 84...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 85...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 121...\n",
      "MLP-relu-0.001-(5,)\n",
      "f1 scores: [0.95404412 1.         1.         0.97854998 0.93406593]\n",
      "f1 mean: 0.973\n",
      "f1 std: 0.026\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 81...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 81...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 82...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 82...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 129...\n",
      "MLP-relu-0.001-(32,)\n",
      "f1 scores: [0.95404412 1.         1.         0.95659722 0.95659722]\n",
      "f1 mean: 0.973\n",
      "f1 std: 0.022\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 77...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 83...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 91...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 130...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 93...\n",
      "MLP-relu-0.001-(16, 16)\n",
      "f1 scores: [0.95404412 1.         0.97854998 0.97854998 0.93406593]\n",
      "f1 mean: 0.969\n",
      "f1 std: 0.023\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 86...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 90...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 92...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 102...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 145...\n",
      "MLP-relu-0.001-(10, 5, 5)\n",
      "f1 scores: [0.95404412 0.97802198 0.97854998 0.97854998 0.93406593]\n",
      "f1 mean: 0.965\n",
      "f1 std: 0.018\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 85...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 77...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 101...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 100...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 131...\n",
      "MLP-relu-0.001-(16, 8, 8)\n",
      "f1 scores: [0.95404412 1.         0.97854998 0.97854998 0.95659722]\n",
      "f1 mean: 0.974\n",
      "f1 std: 0.017\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.001-(64, 32, 32)\n",
      "f1 scores: [0.45799458 0.3902439  0.3902439  0.3902439  0.47552448]\n",
      "f1 mean: 0.421\n",
      "f1 std: 0.038\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 18...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 23...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 19...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 47...\n",
      "MLP-relu-0.01-(5,)\n",
      "f1 scores: [0.95404412 1.         1.         0.97854998 0.95659722]\n",
      "f1 mean: 0.978\n",
      "f1 std: 0.020\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 18...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 18...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 19...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 49...\n",
      "MLP-relu-0.01-(32,)\n",
      "f1 scores: [0.95404412 1.         1.         0.95659722 0.95659722]\n",
      "f1 mean: 0.973\n",
      "f1 std: 0.022\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "MLP-relu-0.01-(16, 16)\n",
      "f1 scores: [0.93215739 0.3902439  0.38271605 0.3902439  0.44769331]\n",
      "f1 mean: 0.509\n",
      "f1 std: 0.213\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(10, 5, 5)\n",
      "f1 scores: [0.83022071 0.89665151 0.79707792 0.77920514 0.73906062]\n",
      "f1 mean: 0.808\n",
      "f1 std: 0.053\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:189: RuntimeWarning: divide by zero encountered in log\n",
      "  loss -= (idx == y)*np.log(y_prob)\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:189: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss -= (idx == y)*np.log(y_prob)\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP-relu-0.01-(16, 8, 8)\n",
      "f1 scores: [0.39759036 0.3902439  0.3902439  0.3902439  0.3902439 ]\n",
      "f1 mean: 0.392\n",
      "f1 std: 0.003\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(64, 32, 32)\n",
      "f1 scores: [0.45799458 0.3902439  0.3902439  0.3902439  0.47552448]\n",
      "f1 mean: 0.421\n",
      "f1 std: 0.038\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 12...\n",
      "MLP-relu-0.1-(5,)\n",
      "f1 scores: [0.95404412 0.875      0.84984985 0.97854998 0.75      ]\n",
      "f1 mean: 0.881\n",
      "f1 std: 0.081\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:189: RuntimeWarning: divide by zero encountered in log\n",
      "  loss -= (idx == y)*np.log(y_prob)\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:189: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss -= (idx == y)*np.log(y_prob)\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:189: RuntimeWarning: divide by zero encountered in log\n",
      "  loss -= (idx == y)*np.log(y_prob)\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:189: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss -= (idx == y)*np.log(y_prob)\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(32,)\n",
      "f1 scores: [0.36708861 0.35897436 0.35897436 0.32432432 0.34434191]\n",
      "f1 mean: 0.351\n",
      "f1 std: 0.015\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.WARNING: divergence detected, please set smaller learning rate.\n",
      "\n",
      "As a punishment we return a model with randomly initialized weights.As a punishment we return a model with randomly initialized weights.\n",
      "\n",
      "MLP-relu-0.1-(16, 16)\n",
      "f1 scores: [0.39759036 0.3902439  0.38271605 0.3902439  0.44769331]\n",
      "f1 mean: 0.402\n",
      "f1 std: 0.023\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(10, 5, 5)\n",
      "f1 scores: [0.83022071 0.89665151 0.79707792 0.77920514 0.73906062]\n",
      "f1 mean: 0.808\n",
      "f1 std: 0.053\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(16, 8, 8)\n",
      "f1 scores: [0.39759036 0.3902439  0.3902439  0.3902439  0.3902439 ]\n",
      "f1 mean: 0.392\n",
      "f1 std: 0.003\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(64, 32, 32)\n",
      "f1 scores: [0.45799458 0.3902439  0.3902439  0.3902439  0.47552448]\n",
      "f1 mean: 0.421\n",
      "f1 std: 0.038\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.5-(5,)\n",
      "f1 scores: [0.86213235 0.875      0.84984985 0.875      0.75      ]\n",
      "f1 mean: 0.842\n",
      "f1 std: 0.047\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.5-(32,)\n",
      "f1 scores: [0.36708861 0.35897436 0.35897436 0.32432432 0.34434191]\n",
      "f1 mean: 0.351\n",
      "f1 std: 0.015\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.5-(16, 16)\n",
      "f1 scores: [0.39759036 0.3902439  0.38271605 0.3902439  0.44769331]\n",
      "f1 mean: 0.402\n",
      "f1 std: 0.023\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.5-(10, 5, 5)\n",
      "f1 scores: [0.83022071 0.89665151 0.79707792 0.77920514 0.73906062]\n",
      "f1 mean: 0.808\n",
      "f1 std: 0.053\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.5-(16, 8, 8)\n",
      "f1 scores: [0.39759036 0.3902439  0.3902439  0.3902439  0.3902439 ]\n",
      "f1 mean: 0.392\n",
      "f1 std: 0.003\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.5-(64, 32, 32)\n",
      "f1 scores: [0.45799458 0.3902439  0.3902439  0.3902439  0.47552448]\n",
      "f1 mean: 0.421\n",
      "f1 std: 0.038\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 207...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 187...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 234...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 248...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 285...\n",
      "MLP-sigmoid-0.0001-(5,)\n",
      "f1 scores: [0.83637214 0.93406593 0.93406593 1.         0.89010989]\n",
      "f1 mean: 0.919\n",
      "f1 std: 0.054\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 156...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 178...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 184...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 188...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 220...\n",
      "MLP-sigmoid-0.0001-(32,)\n",
      "f1 scores: [0.95404412 1.         0.95659722 0.97854998 0.97802198]\n",
      "f1 mean: 0.973\n",
      "f1 std: 0.017\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 65...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 74...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 93...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 393...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 589...\n",
      "MLP-sigmoid-0.0001-(16, 16)\n",
      "f1 scores: [0.76       0.3902439  0.3902439  0.3902439  0.82174688]\n",
      "f1 mean: 0.550\n",
      "f1 std: 0.197\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "MLP-sigmoid-0.0001-(10, 5, 5)\n",
      "f1 scores: [0.39759036 0.3902439  0.3902439  0.3902439  0.3902439 ]\n",
      "f1 mean: 0.392\n",
      "f1 std: 0.003\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 46...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 46...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 46...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 46...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 46...\n",
      "MLP-sigmoid-0.0001-(16, 8, 8)\n",
      "f1 scores: [0.39759036 0.3902439  0.3902439  0.3902439  0.3902439 ]\n",
      "f1 mean: 0.392\n",
      "f1 std: 0.003\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 24...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 34...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 28...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 34...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 34...\n",
      "MLP-sigmoid-0.0001-(64, 32, 32)\n",
      "f1 scores: [0.39759036 0.3902439  0.3902439  0.3902439  0.3902439 ]\n",
      "f1 mean: 0.392\n",
      "f1 std: 0.003\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 86...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 85...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 95...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 85...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 150...\n",
      "MLP-sigmoid-0.001-(5,)\n",
      "f1 scores: [0.9773858  1.         0.95755518 0.97854998 0.95659722]\n",
      "f1 mean: 0.974\n",
      "f1 std: 0.016\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 47...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 55...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 60...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 59...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 100...\n",
      "MLP-sigmoid-0.001-(32,)\n",
      "f1 scores: [0.95404412 1.         1.         0.97854998 0.97802198]\n",
      "f1 mean: 0.982\n",
      "f1 std: 0.017\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 144...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 146...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 148...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 147...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 201...\n",
      "MLP-sigmoid-0.001-(16, 16)\n",
      "f1 scores: [0.95404412 1.         0.97854998 0.95755518 0.95659722]\n",
      "f1 mean: 0.969\n",
      "f1 std: 0.018\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "MLP-sigmoid-0.001-(10, 5, 5)\n",
      "f1 scores: [0.39759036 0.3902439  0.3902439  0.3902439  0.3902439 ]\n",
      "f1 mean: 0.392\n",
      "f1 std: 0.003\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 19...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 19...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 28...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 29...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 29...\n",
      "MLP-sigmoid-0.001-(16, 8, 8)\n",
      "f1 scores: [0.39759036 0.3902439  0.3902439  0.3902439  0.3902439 ]\n",
      "f1 mean: 0.392\n",
      "f1 std: 0.003\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 192...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 186...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 198...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 197...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 255...\n",
      "MLP-sigmoid-0.001-(64, 32, 32)\n",
      "f1 scores: [0.90808824 1.         0.95659722 0.97854998 0.95659722]\n",
      "f1 mean: 0.960\n",
      "f1 std: 0.031\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 18...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 23...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 28...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 22...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 51...\n",
      "MLP-sigmoid-0.01-(5,)\n",
      "f1 scores: [0.9773858  1.         0.97854998 0.97854998 0.95659722]\n",
      "f1 mean: 0.978\n",
      "f1 std: 0.014\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 13...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 14...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 40...\n",
      "MLP-sigmoid-0.01-(32,)\n",
      "f1 scores: [0.95404412 1.         0.97854998 0.95755518 0.97802198]\n",
      "f1 mean: 0.974\n",
      "f1 std: 0.017\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 25...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 24...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 25...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 25...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 57...\n",
      "MLP-sigmoid-0.01-(16, 16)\n",
      "f1 scores: [0.95404412 1.         0.95755518 0.95755518 0.95659722]\n",
      "f1 mean: 0.965\n",
      "f1 std: 0.017\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 58...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 60...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 60...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 66...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 91...\n",
      "MLP-sigmoid-0.01-(10, 5, 5)\n",
      "f1 scores: [0.9773858  1.         0.95755518 0.97854998 0.95659722]\n",
      "f1 mean: 0.974\n",
      "f1 std: 0.016\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 41...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 40...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 41...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 44...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 81...\n",
      "MLP-sigmoid-0.01-(16, 8, 8)\n",
      "f1 scores: [0.95404412 1.         0.95755518 0.95755518 0.95659722]\n",
      "f1 mean: 0.965\n",
      "f1 std: 0.017\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 31...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 30...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 29...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 33...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 60...\n",
      "MLP-sigmoid-0.01-(64, 32, 32)\n",
      "f1 scores: [0.92987377 1.         1.         0.97854998 0.95659722]\n",
      "f1 mean: 0.973\n",
      "f1 std: 0.027\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 11...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 11...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 20...\n",
      "MLP-sigmoid-0.1-(5,)\n",
      "f1 scores: [0.95404412 1.         0.91666667 0.97854998 0.93564994]\n",
      "f1 mean: 0.957\n",
      "f1 std: 0.030\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 28...\n",
      "MLP-sigmoid-0.1-(32,)\n",
      "f1 scores: [0.95404412 1.         0.95755518 0.95659722 0.95659722]\n",
      "f1 mean: 0.965\n",
      "f1 std: 0.018\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 11...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 11...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 11...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 11...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 20...\n",
      "MLP-sigmoid-0.1-(16, 16)\n",
      "f1 scores: [0.95404412 1.         0.95755518 0.95755518 0.95659722]\n",
      "f1 mean: 0.965\n",
      "f1 std: 0.017\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 14...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 15...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 18...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 15...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 23...\n",
      "MLP-sigmoid-0.1-(10, 5, 5)\n",
      "f1 scores: [0.95404412 1.         0.91666667 0.97854998 0.93564994]\n",
      "f1 mean: 0.957\n",
      "f1 std: 0.030\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 12...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 12...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 13...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 19...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 22...\n",
      "MLP-sigmoid-0.1-(16, 8, 8)\n",
      "f1 scores: [0.95404412 1.         0.9369483  0.95755518 0.95659722]\n",
      "f1 mean: 0.961\n",
      "f1 std: 0.021\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 12...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 12...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 13...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 12...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 22...\n",
      "MLP-sigmoid-0.1-(64, 32, 32)\n",
      "f1 scores: [0.95404412 1.         0.97854998 0.95659722 0.95659722]\n",
      "f1 mean: 0.969\n",
      "f1 std: 0.018\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 13...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 11...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 13...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "MLP-sigmoid-0.5-(5,)\n",
      "f1 scores: [0.95404412 1.         0.9369483  0.95755518 0.95659722]\n",
      "f1 mean: 0.961\n",
      "f1 std: 0.021\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 11...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 11...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 15...\n",
      "MLP-sigmoid-0.5-(32,)\n",
      "f1 scores: [0.95404412 1.         0.87684729 0.9369483  0.85714286]\n",
      "f1 mean: 0.925\n",
      "f1 std: 0.052\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 11...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 12...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 12...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 11...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 12...\n",
      "MLP-sigmoid-0.5-(16, 16)\n",
      "f1 scores: [0.95404412 1.         0.95755518 0.97854998 0.95659722]\n",
      "f1 mean: 0.969\n",
      "f1 std: 0.018\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "MLP-sigmoid-0.5-(10, 5, 5)\n",
      "f1 scores: [0.39759036 0.3902439  0.3902439  0.3902439  0.3902439 ]\n",
      "f1 mean: 0.392\n",
      "f1 std: 0.003\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 13...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 18...\n",
      "MLP-sigmoid-0.5-(16, 8, 8)\n",
      "f1 scores: [0.9773858  1.         1.         0.97854998 0.3902439 ]\n",
      "f1 mean: 0.869\n",
      "f1 std: 0.240\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 28...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 31...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 25...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 28...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 34...\n",
      "MLP-sigmoid-0.5-(64, 32, 32)\n",
      "f1 scores: [0.39759036 0.3902439  0.3902439  0.3902439  0.3902439 ]\n",
      "f1 mean: 0.392\n",
      "f1 std: 0.003\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cv_num = 5\n",
    "model_params = {}\n",
    "models = {}\n",
    "models_lists = {}\n",
    "\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    cv_results = cross_validate(pipeline, X, y, cv=cv_num, scoring='f1_macro', return_estimator=True, n_jobs=10)\n",
    "\n",
    "    models[model_name] = cv_results['estimator']\n",
    "    model_params[model_name] = {}\n",
    "    models_lists[model_name] = {}\n",
    "\n",
    "    num_cols = ['test_score', 'fit_time', 'score_time']\n",
    "\n",
    "    for num_col in num_cols:\n",
    "        models_lists[model_name][num_col] = cv_results[num_col]\n",
    "        model_params[model_name][f'{num_col}_mean'] = cv_results[num_col].mean()\n",
    "        model_params[model_name][f'{num_col}_std'] = cv_results[num_col].std()\n",
    "    \n",
    "    model_params[model_name]['parameter_num'] = cv_results['estimator'][0][model_name].number_of_params_\n",
    "    model_params[model_name]['hidden_layer_sizes'] = cv_results['estimator'][0][model_name].hidden_layer_sizes\n",
    "    model_params[model_name]['activation_function'] = cv_results['estimator'][0][model_name].activation_function\n",
    "    model_params[model_name]['learning_rate'] = cv_results['estimator'][0][model_name].learning_rate\n",
    "    models_lists[model_name]['converged'] = [e[model_name].converged_ for e in cv_results['estimator']]\n",
    "    models_lists[model_name]['validation_losses'] = [e[model_name].validation_losses_ for e in cv_results['estimator']]\n",
    "    models_lists[model_name]['training_losses'] = [e[model_name].training_losses_ for e in cv_results['estimator']]\n",
    "    model_params[model_name]['num_iter'] = np.array(list([len(e[model_name].training_losses_) for e in cv_results['estimator']])).mean()\n",
    "\n",
    "    print(model_name)\n",
    "    print(\n",
    "        f\"f1 scores: {models_lists[model_name]['test_score']}\\n\" +\n",
    "        f\"f1 mean: {model_params[model_name]['test_score_mean']:.3f}\\n\" +\n",
    "        f\"f1 std: {model_params[model_name]['test_score_std']:.3f}\\n\"\n",
    "    )\n",
    "    print('----------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_param = pd.DataFrame(model_params).transpose()\n",
    "df_param = df_param.reset_index(drop=False)\n",
    "df_param = df_param.rename(columns={'index': 'model'})\n",
    "\n",
    "# df_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_param.to_csv(r'results/breast_cancer_params_sigmoid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>test_score_mean</th>\n",
       "      <th>test_score_std</th>\n",
       "      <th>fit_time_mean</th>\n",
       "      <th>fit_time_std</th>\n",
       "      <th>score_time_mean</th>\n",
       "      <th>score_time_std</th>\n",
       "      <th>parameter_num</th>\n",
       "      <th>hidden_layer_sizes</th>\n",
       "      <th>activation_function</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>num_iter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>MLP-sigmoid-0.001-(32,)</td>\n",
       "      <td>0.982123</td>\n",
       "      <td>0.017072</td>\n",
       "      <td>2.59008</td>\n",
       "      <td>0.479271</td>\n",
       "      <td>0.009105</td>\n",
       "      <td>0.007221</td>\n",
       "      <td>1058</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "      <td>65.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>MLP-sigmoid-0.01-(5,)</td>\n",
       "      <td>0.978217</td>\n",
       "      <td>0.013732</td>\n",
       "      <td>0.961008</td>\n",
       "      <td>0.350908</td>\n",
       "      <td>0.008265</td>\n",
       "      <td>0.005729</td>\n",
       "      <td>167</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "      <td>29.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MLP-relu-0.01-(5,)</td>\n",
       "      <td>0.977838</td>\n",
       "      <td>0.020001</td>\n",
       "      <td>0.853623</td>\n",
       "      <td>0.326674</td>\n",
       "      <td>0.009437</td>\n",
       "      <td>0.005169</td>\n",
       "      <td>167</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>MLP-sigmoid-0.001-(5,)</td>\n",
       "      <td>0.974018</td>\n",
       "      <td>0.016009</td>\n",
       "      <td>3.3319</td>\n",
       "      <td>0.84472</td>\n",
       "      <td>0.007277</td>\n",
       "      <td>0.004158</td>\n",
       "      <td>167</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "      <td>101.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>MLP-sigmoid-0.01-(10, 5, 5)</td>\n",
       "      <td>0.974018</td>\n",
       "      <td>0.016009</td>\n",
       "      <td>4.262008</td>\n",
       "      <td>0.437246</td>\n",
       "      <td>0.009032</td>\n",
       "      <td>0.004592</td>\n",
       "      <td>407</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model test_score_mean test_score_std fit_time_mean  \\\n",
       "37      MLP-sigmoid-0.001-(32,)        0.982123       0.017072       2.59008   \n",
       "42        MLP-sigmoid-0.01-(5,)        0.978217       0.013732      0.961008   \n",
       "12           MLP-relu-0.01-(5,)        0.977838       0.020001      0.853623   \n",
       "36       MLP-sigmoid-0.001-(5,)        0.974018       0.016009        3.3319   \n",
       "45  MLP-sigmoid-0.01-(10, 5, 5)        0.974018       0.016009      4.262008   \n",
       "\n",
       "   fit_time_std score_time_mean score_time_std parameter_num  \\\n",
       "37     0.479271        0.009105       0.007221          1058   \n",
       "42     0.350908        0.008265       0.005729           167   \n",
       "12     0.326674        0.009437       0.005169           167   \n",
       "36      0.84472        0.007277       0.004158           167   \n",
       "45     0.437246        0.009032       0.004592           407   \n",
       "\n",
       "   hidden_layer_sizes activation_function learning_rate num_iter  \n",
       "37              (32,)             sigmoid         0.001     65.2  \n",
       "42               (5,)             sigmoid          0.01     29.4  \n",
       "12               (5,)                relu          0.01     25.6  \n",
       "36               (5,)             sigmoid         0.001    101.2  \n",
       "45         (10, 5, 5)             sigmoid          0.01     68.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_param.sort_values(['test_score_mean'], ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 30)\n",
      "MLP-sigmoid-0.01-(5,)\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 26...\n",
      "Evaluation metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9375    , 0.0625    ],\n",
       "       [0.05555556, 0.94444444]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.94\n",
      "balanced_acc: 0.9409722222222222\n",
      "\n",
      "Macro-averaged precision: 0.9312393887945671\n",
      "Macro-averaged recall: 0.9409722222222222\n",
      "Macro-averaged f-score: 0.9356499356499357\n",
      "Macro-averaged support: None\n",
      "MLP-relu-0.01-(5,)\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 33...\n",
      "Evaluation metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        ],\n",
       "       [0.05555556, 0.94444444]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.98\n",
      "balanced_acc: 0.9722222222222222\n",
      "\n",
      "Macro-averaged precision: 0.9848484848484849\n",
      "Macro-averaged recall: 0.9722222222222222\n",
      "Macro-averaged f-score: 0.9780219780219781\n",
      "Macro-averaged support: None\n",
      "MLP-sigmoid-0.001-(5,)\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 111...\n",
      "Evaluation metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9375    , 0.0625    ],\n",
       "       [0.11111111, 0.88888889]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.92\n",
      "balanced_acc: 0.9131944444444444\n",
      "\n",
      "Macro-averaged precision: 0.9131944444444444\n",
      "Macro-averaged recall: 0.9131944444444444\n",
      "Macro-averaged f-score: 0.9131944444444444\n",
      "Macro-averaged support: None\n",
      "MLP-sigmoid-0.01-(10, 5, 5)\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 80...\n",
      "Evaluation metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.96875   , 0.03125   ],\n",
       "       [0.05555556, 0.94444444]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.96\n",
      "balanced_acc: 0.9565972222222222\n",
      "\n",
      "Macro-averaged precision: 0.9565972222222222\n",
      "Macro-averaged recall: 0.9565972222222222\n",
      "Macro-averaged f-score: 0.9565972222222222\n",
      "Macro-averaged support: None\n"
     ]
    }
   ],
   "source": [
    "modelsss = df_param.sort_values(['test_score_mean'], ascending=False).head(5)['model']\n",
    "print(X_train.shape)\n",
    "results_df = pd.DataFrame()\n",
    "for mod in modelsss[1:]:\n",
    "    print(mod)\n",
    "    pipelines[mod].fit(X_train, y_train)\n",
    "    y_pred = pipelines[mod].predict(X_test)\n",
    "    eval_results = cu.eval(y_pred, y_test)\n",
    "    results_df = results_df.append({\n",
    "        'Model': mod,\n",
    "        'cm': eval_results['cm'].flatten(),\n",
    "        'acc': eval_results['acc'],\n",
    "        'balanced_acc': eval_results['balanced_acc'],\n",
    "        'precision': eval_results['precision'],\n",
    "        'recall': eval_results['recall'],\n",
    "        'f-score': eval_results['f-score'],\n",
    "        'support': eval_results['support']\n",
    "    }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"results/breast_cancer_trainset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>cm</th>\n",
       "      <th>acc</th>\n",
       "      <th>balanced_acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP-sigmoid-0.01-(5,)</td>\n",
       "      <td>[0.9375, 0.0625, 0.05555555555555555, 0.944444...</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.940972</td>\n",
       "      <td>0.931239</td>\n",
       "      <td>0.940972</td>\n",
       "      <td>0.935650</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP-relu-0.01-(5,)</td>\n",
       "      <td>[1.0, 0.0, 0.05555555555555555, 0.944444444444...</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP-sigmoid-0.001-(5,)</td>\n",
       "      <td>[0.9375, 0.0625, 0.1111111111111111, 0.8888888...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.913194</td>\n",
       "      <td>0.913194</td>\n",
       "      <td>0.913194</td>\n",
       "      <td>0.913194</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP-sigmoid-0.01-(10, 5, 5)</td>\n",
       "      <td>[0.96875, 0.03125, 0.05555555555555555, 0.9444...</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.956597</td>\n",
       "      <td>0.956597</td>\n",
       "      <td>0.956597</td>\n",
       "      <td>0.956597</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model  \\\n",
       "0        MLP-sigmoid-0.01-(5,)   \n",
       "1           MLP-relu-0.01-(5,)   \n",
       "2       MLP-sigmoid-0.001-(5,)   \n",
       "3  MLP-sigmoid-0.01-(10, 5, 5)   \n",
       "\n",
       "                                                  cm   acc  balanced_acc  \\\n",
       "0  [0.9375, 0.0625, 0.05555555555555555, 0.944444...  0.94      0.940972   \n",
       "1  [1.0, 0.0, 0.05555555555555555, 0.944444444444...  0.98      0.972222   \n",
       "2  [0.9375, 0.0625, 0.1111111111111111, 0.8888888...  0.92      0.913194   \n",
       "3  [0.96875, 0.03125, 0.05555555555555555, 0.9444...  0.96      0.956597   \n",
       "\n",
       "   precision    recall   f-score support  \n",
       "0   0.931239  0.940972  0.935650    None  \n",
       "1   0.984848  0.972222  0.978022    None  \n",
       "2   0.913194  0.913194  0.913194    None  \n",
       "3   0.956597  0.956597  0.956597    None  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
