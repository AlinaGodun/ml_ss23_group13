{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import classification_util as cu\n",
    "from nn_framework import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>...</th>\n",
       "      <th>debt_settlement_flag</th>\n",
       "      <th>issue_d_month</th>\n",
       "      <th>issue_d_year</th>\n",
       "      <th>earliest_cr_line_month</th>\n",
       "      <th>earliest_cr_line_year</th>\n",
       "      <th>last_pymnt_d_month</th>\n",
       "      <th>last_pymnt_d_year</th>\n",
       "      <th>last_credit_pull_d_month</th>\n",
       "      <th>last_credit_pull_d_year</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24341</td>\n",
       "      <td>12500.0</td>\n",
       "      <td>12500.0</td>\n",
       "      <td>12500.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>7.21</td>\n",
       "      <td>387.17</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>81000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67534</td>\n",
       "      <td>33850.0</td>\n",
       "      <td>33850.0</td>\n",
       "      <td>33775.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>20.99</td>\n",
       "      <td>915.57</td>\n",
       "      <td>1 year</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>9</td>\n",
       "      <td>1984</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35080</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>20.00</td>\n",
       "      <td>264.94</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>RENT</td>\n",
       "      <td>36580.0</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4828</td>\n",
       "      <td>20250.0</td>\n",
       "      <td>20250.0</td>\n",
       "      <td>20250.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>14.31</td>\n",
       "      <td>695.15</td>\n",
       "      <td>9 years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>48700.0</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>1996</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59259</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>14.99</td>\n",
       "      <td>866.52</td>\n",
       "      <td>1 year</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>85000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>11</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>6644</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>16.02</td>\n",
       "      <td>486.58</td>\n",
       "      <td>5 years</td>\n",
       "      <td>OWN</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>8</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>1991</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>25910</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>9.49</td>\n",
       "      <td>240.22</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>46386.0</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>10</td>\n",
       "      <td>2014</td>\n",
       "      <td>4</td>\n",
       "      <td>2004</td>\n",
       "      <td>3</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>95698</td>\n",
       "      <td>20750.0</td>\n",
       "      <td>20750.0</td>\n",
       "      <td>20750.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>15.05</td>\n",
       "      <td>494.19</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>RENT</td>\n",
       "      <td>185000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>4</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>2004</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>27371</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>16.29</td>\n",
       "      <td>269.20</td>\n",
       "      <td>1 year</td>\n",
       "      <td>RENT</td>\n",
       "      <td>38500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>9</td>\n",
       "      <td>2014</td>\n",
       "      <td>8</td>\n",
       "      <td>2006</td>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>2016</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>8729</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>29900.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>12.29</td>\n",
       "      <td>1000.59</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>110000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1982</td>\n",
       "      <td>9</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>2018</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  loan_amnt  funded_amnt  funded_amnt_inv        term  int_rate  \\\n",
       "0     24341    12500.0      12500.0          12500.0   36 months      7.21   \n",
       "1     67534    33850.0      33850.0          33775.0   60 months     20.99   \n",
       "2     35080    10000.0      10000.0          10000.0   60 months     20.00   \n",
       "3      4828    20250.0      20250.0          20250.0   36 months     14.31   \n",
       "4     59259    25000.0      25000.0          25000.0   36 months     14.99   \n",
       "...     ...        ...          ...              ...         ...       ...   \n",
       "9995   6644    20000.0      20000.0          20000.0   60 months     16.02   \n",
       "9996  25910     7500.0       7500.0           7500.0   36 months      9.49   \n",
       "9997  95698    20750.0      20750.0          20750.0   60 months     15.05   \n",
       "9998  27371    11000.0      11000.0          11000.0   60 months     16.29   \n",
       "9999   8729    30000.0      30000.0          29900.0   36 months     12.29   \n",
       "\n",
       "      installment emp_length home_ownership  annual_inc  ...  \\\n",
       "0          387.17   < 1 year       MORTGAGE     81000.0  ...   \n",
       "1          915.57     1 year       MORTGAGE     80000.0  ...   \n",
       "2          264.94   < 1 year           RENT     36580.0  ...   \n",
       "3          695.15    9 years           RENT     48700.0  ...   \n",
       "4          866.52     1 year       MORTGAGE     85000.0  ...   \n",
       "...           ...        ...            ...         ...  ...   \n",
       "9995       486.58    5 years            OWN     54000.0  ...   \n",
       "9996       240.22  10+ years           RENT     46386.0  ...   \n",
       "9997       494.19   < 1 year           RENT    185000.0  ...   \n",
       "9998       269.20     1 year           RENT     38500.0  ...   \n",
       "9999      1000.59  10+ years       MORTGAGE    110000.0  ...   \n",
       "\n",
       "     debt_settlement_flag issue_d_month issue_d_year earliest_cr_line_month  \\\n",
       "0                       N             6         2018                      6   \n",
       "1                       N            10         2015                      9   \n",
       "2                       N             9         2017                     10   \n",
       "3                       N             0         2015                      6   \n",
       "4                       N            11         2016                      0   \n",
       "...                   ...           ...          ...                    ...   \n",
       "9995                    N             8         2017                     11   \n",
       "9996                    N            10         2014                      4   \n",
       "9997                    N             4         2017                      2   \n",
       "9998                    N             9         2014                      8   \n",
       "9999                    N             9         2015                      1   \n",
       "\n",
       "     earliest_cr_line_year  last_pymnt_d_month  last_pymnt_d_year  \\\n",
       "0                     2000                   2               2019   \n",
       "1                     1984                   2               2019   \n",
       "2                     2006                   1               2018   \n",
       "3                     1996                   6               2016   \n",
       "4                     2002                   2               2019   \n",
       "...                    ...                 ...                ...   \n",
       "9995                  1991                   2               2019   \n",
       "9996                  2004                   3               2017   \n",
       "9997                  2004                   2               2019   \n",
       "9998                  2006                   3               2016   \n",
       "9999                  1982                   9               2018   \n",
       "\n",
       "      last_credit_pull_d_month  last_credit_pull_d_year  grade  \n",
       "0                            2                     2019      A  \n",
       "1                            2                     2019      E  \n",
       "2                           11                     2018      D  \n",
       "3                            9                     2017      C  \n",
       "4                            2                     2019      C  \n",
       "...                        ...                      ...    ...  \n",
       "9995                         2                     2019      C  \n",
       "9996                         6                     2018      B  \n",
       "9997                         2                     2019      C  \n",
       "9998                        10                     2016      D  \n",
       "9999                         9                     2018      C  \n",
       "\n",
       "[10000 rows x 92 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = r'../Ex1/data/Loan/loan-10k.lrn.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(X):\n",
    "        log_tranform_col = ['int_rate', 'annual_inc', 'total_rev_hi_lim', 'tot_hi_cred_lim', 'total_bc_limit']\n",
    "        c_root_tranform_col = ['installment', 'total_acc', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', \n",
    "                            'total_il_high_credit_limit', 'total_bal_ex_mort', 'avg_cur_bal', 'bc_open_to_buy', \n",
    "                            'revol_bal', 'total_rec_int', 'last_pymnt_amnt', 'tot_coll_amt','tot_cur_bal']\n",
    "\n",
    "        for col in c_root_tranform_col:\n",
    "            X[col] = np.cbrt(X[col])\n",
    "        for col in log_tranform_col:\n",
    "            X[col] = np.log(X[col])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(X, y):\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X,y, random_state=40, stratify=y, test_size=0.2)\n",
    "    X_te_id = X_te.pop('ID')\n",
    "    X_tr_id = X_tr.pop('ID')\n",
    "    return X_tr, X_te, y_tr, y_te, X_te_id, X_tr_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NNFramework()\n",
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.pop('grade')\n",
    "# X = data.pop('addr_state')\n",
    "X = data\n",
    "X = transformer(X)\n",
    "cols = list(X.select_dtypes('object').columns)\n",
    "nn.fit_encoder(X, cols)\n",
    "X = nn.encode_dataset(X)\n",
    "X_tr, X_te, y_tr, y_te, X_te_id, X_tr_id = train_test(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1038\n",
    "scaling = True\n",
    "oversampling = True\n",
    "\n",
    "scaler = preprocessing.StandardScaler() if scaling else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = ['relu', 'sigmoid']\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 0.5]\n",
    "hidden_layer_sizes = [(5,), (32,), (16, 16), (10, 5, 5), (16, 8, 8), (64, 32, 32),]\n",
    "\n",
    "# activation_functions = ['relu']\n",
    "# learning_rates = [0.0001]\n",
    "# hidden_layer_sizes = [(5,), (32,), (16, 16),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = []\n",
    "\n",
    "for af in activation_functions:\n",
    "    for lr in learning_rates:\n",
    "        for hls in hidden_layer_sizes:\n",
    "            methods.append((f'MLP-{af}-{lr}-{hls}', MLP(n_iter=1000, activation_function=af, learning_rate=lr, hidden_layer_sizes=hls)))\n",
    "    \n",
    "pipelines = cu.define_pipelines(methods, scaler=scaler, oversampling=oversampling)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 215...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 212...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 216...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 222...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 215...\n",
      "MLP-relu-0.0001-(5,)\n",
      "f1 scores: [0.67932916 0.6675278  0.67802017 0.69476442 0.71516773]\n",
      "f1 mean: 0.687\n",
      "f1 std: 0.017\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 177...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 190...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 193...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 194...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 193...\n",
      "MLP-relu-0.0001-(32,)\n",
      "f1 scores: [0.65489391 0.60999124 0.6350975  0.64816184 0.639269  ]\n",
      "f1 mean: 0.637\n",
      "f1 std: 0.015\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 124...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 131...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 143...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 150...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 147...\n",
      "MLP-relu-0.0001-(16, 16)\n",
      "f1 scores: [0.68463015 0.65500884 0.65736756 0.67480269 0.69530639]\n",
      "f1 mean: 0.673\n",
      "f1 std: 0.016\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:189: RuntimeWarning: divide by zero encountered in log\n",
      "  loss -= (idx == y)*np.log(y_prob)\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:189: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss -= (idx == y)*np.log(y_prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 72...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 85...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 81...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 87...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:189: RuntimeWarning: divide by zero encountered in log\n",
      "  loss -= (idx == y)*np.log(y_prob)\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:189: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss -= (idx == y)*np.log(y_prob)\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.0001-(10, 5, 5)\n",
      "f1 scores: [0.71559736 0.66564361 0.09680415 0.69734959 0.7173087 ]\n",
      "f1 mean: 0.579\n",
      "f1 std: 0.242\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 81...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:189: RuntimeWarning: divide by zero encountered in log\n",
      "  loss -= (idx == y)*np.log(y_prob)\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:189: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss -= (idx == y)*np.log(y_prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 91...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 86...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 106...\n",
      "MLP-relu-0.0001-(16, 8, 8)\n",
      "f1 scores: [0.67977603 0.630787   0.67841515 0.08249916 0.6238354 ]\n",
      "f1 mean: 0.539\n",
      "f1 std: 0.229\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.0001-(64, 32, 32)\n",
      "f1 scores: [0.0443804  0.0450492  0.04968458 0.04176603 0.04783793]\n",
      "f1 mean: 0.046\n",
      "f1 std: 0.003\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 89...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 89...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 76...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 83...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 93...\n",
      "MLP-relu-0.001-(5,)\n",
      "f1 scores: [0.70848622 0.63891171 0.71056022 0.69992047 0.71341224]\n",
      "f1 mean: 0.694\n",
      "f1 std: 0.028\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 43...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 44...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 43...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 40...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 40...\n",
      "MLP-relu-0.001-(32,)\n",
      "f1 scores: [0.62397614 0.62833408 0.6341415  0.64015813 0.62128779]\n",
      "f1 mean: 0.630\n",
      "f1 std: 0.007\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.001-(16, 16)\n",
      "f1 scores: [0.07342789 0.0774332  0.07357714 0.07020236 0.07428873]\n",
      "f1 mean: 0.074\n",
      "f1 std: 0.002\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.001-(10, 5, 5)\n",
      "f1 scores: [0.08938295 0.08897065 0.09680415 0.08486995 0.08762781]\n",
      "f1 mean: 0.090\n",
      "f1 std: 0.004\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.001-(16, 8, 8)\n",
      "f1 scores: [0.08667226 0.08059418 0.0880868  0.08249916 0.09365226]\n",
      "f1 mean: 0.086\n",
      "f1 std: 0.005\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.001-(64, 32, 32)\n",
      "f1 scores: [0.0443804  0.0450492  0.04968458 0.04176603 0.04783793]\n",
      "f1 mean: 0.046\n",
      "f1 std: 0.003\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(5,)\n",
      "f1 scores: [0.05993934 0.05882635 0.06795819 0.05973724 0.06367169]\n",
      "f1 mean: 0.062\n",
      "f1 std: 0.003\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(32,)\n",
      "f1 scores: [0.05452441 0.05752543 0.05736912 0.05249798 0.06233572]\n",
      "f1 mean: 0.057\n",
      "f1 std: 0.003\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(16, 16)\n",
      "f1 scores: [0.07342789 0.0774332  0.07357714 0.07020236 0.07428873]\n",
      "f1 mean: 0.074\n",
      "f1 std: 0.002\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(10, 5, 5)\n",
      "f1 scores: [0.08938295 0.08897065 0.09680415 0.08486995 0.08762781]\n",
      "f1 mean: 0.090\n",
      "f1 std: 0.004\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(16, 8, 8)\n",
      "f1 scores: [0.08667226 0.08059418 0.0880868  0.08249916 0.09365226]\n",
      "f1 mean: 0.086\n",
      "f1 std: 0.005\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(64, 32, 32)\n",
      "f1 scores: [0.0443804  0.0450492  0.04968458 0.04176603 0.04783793]\n",
      "f1 mean: 0.046\n",
      "f1 std: 0.003\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(5,)\n",
      "f1 scores: [0.05993934 0.05882635 0.06795819 0.05973724 0.06367169]\n",
      "f1 mean: 0.062\n",
      "f1 std: 0.003\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(32,)\n",
      "f1 scores: [0.05452441 0.05752543 0.05736912 0.05249798 0.06233572]\n",
      "f1 mean: 0.057\n",
      "f1 std: 0.003\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(16, 16)\n",
      "f1 scores: [0.07342789 0.0774332  0.07357714 0.07020236 0.07428873]\n",
      "f1 mean: 0.074\n",
      "f1 std: 0.002\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(10, 5, 5)\n",
      "f1 scores: [0.08938295 0.08897065 0.09680415 0.08486995 0.08762781]\n",
      "f1 mean: 0.090\n",
      "f1 std: 0.004\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(16, 8, 8)\n",
      "f1 scores: [0.08667226 0.08059418 0.0880868  0.08249916 0.09365226]\n",
      "f1 mean: 0.086\n",
      "f1 std: 0.005\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(64, 32, 32)\n",
      "f1 scores: [0.0443804  0.0450492  0.04968458 0.04176603 0.04783793]\n",
      "f1 mean: 0.046\n",
      "f1 std: 0.003\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.5-(5,)\n",
      "f1 scores: [0.05993934 0.05882635 0.06795819 0.05973724 0.06367169]\n",
      "f1 mean: 0.062\n",
      "f1 std: 0.003\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.5-(32,)\n",
      "f1 scores: [0.05452441 0.05752543 0.05736912 0.05249798 0.06233572]\n",
      "f1 mean: 0.057\n",
      "f1 std: 0.003\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.5-(16, 16)\n",
      "f1 scores: [0.07342789 0.0774332  0.07357714 0.07020236 0.07428873]\n",
      "f1 mean: 0.074\n",
      "f1 std: 0.002\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.5-(10, 5, 5)\n",
      "f1 scores: [0.08938295 0.08897065 0.09680415 0.08486995 0.08762781]\n",
      "f1 mean: 0.090\n",
      "f1 std: 0.004\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.5-(16, 8, 8)\n",
      "f1 scores: [0.08667226 0.08059418 0.0880868  0.08249916 0.09365226]\n",
      "f1 mean: 0.086\n",
      "f1 std: 0.005\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.5-(64, 32, 32)\n",
      "f1 scores: [0.0443804  0.0450492  0.04968458 0.04176603 0.04783793]\n",
      "f1 mean: 0.046\n",
      "f1 std: 0.003\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 463...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 430...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 438...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 479...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 439...\n",
      "MLP-sigmoid-0.0001-(5,)\n",
      "f1 scores: [0.6758425  0.63234255 0.6411129  0.67996945 0.69152219]\n",
      "f1 mean: 0.664\n",
      "f1 std: 0.023\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 413...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 419...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 420...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 415...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 425...\n",
      "MLP-sigmoid-0.0001-(32,)\n",
      "f1 scores: [0.63251229 0.59283372 0.62081616 0.65452182 0.62212149]\n",
      "f1 mean: 0.625\n",
      "f1 std: 0.020\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 427...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 437...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 463...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 457...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 467...\n",
      "MLP-sigmoid-0.0001-(16, 16)\n",
      "f1 scores: [0.6953413  0.61784621 0.66252325 0.68139608 0.69314643]\n",
      "f1 mean: 0.670\n",
      "f1 std: 0.029\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 15...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "MLP-sigmoid-0.0001-(10, 5, 5)\n",
      "f1 scores: [0.10929831 0.11198727 0.10040999 0.10641427 0.10177078]\n",
      "f1 mean: 0.106\n",
      "f1 std: 0.004\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 14...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 14...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 14...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 14...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 14...\n",
      "MLP-sigmoid-0.0001-(16, 8, 8)\n",
      "f1 scores: [0.1450318  0.13823271 0.13887387 0.1398033  0.13639884]\n",
      "f1 mean: 0.140\n",
      "f1 std: 0.003\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 451...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 446...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 448...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 451...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 444...\n",
      "MLP-sigmoid-0.0001-(64, 32, 32)\n",
      "f1 scores: [0.76551867 0.72116894 0.71294855 0.76524441 0.74680907]\n",
      "f1 mean: 0.742\n",
      "f1 std: 0.022\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 131...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 132...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 127...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 145...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 162...\n",
      "MLP-sigmoid-0.001-(5,)\n",
      "f1 scores: [0.71155186 0.64470211 0.66531515 0.70841607 0.69294737]\n",
      "f1 mean: 0.685\n",
      "f1 std: 0.026\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 122...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 127...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 125...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 135...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 139...\n",
      "MLP-sigmoid-0.001-(32,)\n",
      "f1 scores: [0.63342386 0.59810702 0.6145341  0.61936676 0.62419659]\n",
      "f1 mean: 0.618\n",
      "f1 std: 0.012\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 85...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 95...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 97...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 100...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 100...\n",
      "MLP-sigmoid-0.001-(16, 16)\n",
      "f1 scores: [0.66094169 0.61347906 0.65199985 0.66998616 0.67456483]\n",
      "f1 mean: 0.654\n",
      "f1 std: 0.022\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 132...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 127...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 140...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 133...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 144...\n",
      "MLP-sigmoid-0.001-(10, 5, 5)\n",
      "f1 scores: [0.65977409 0.68449612 0.68806356 0.66735606 0.63536463]\n",
      "f1 mean: 0.667\n",
      "f1 std: 0.019\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 114...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 120...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 122...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 112...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 126...\n",
      "MLP-sigmoid-0.001-(16, 8, 8)\n",
      "f1 scores: [0.70458835 0.71927751 0.66576974 0.67800888 0.69459044]\n",
      "f1 mean: 0.692\n",
      "f1 std: 0.019\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 82...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 78...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 79...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 82...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 80...\n",
      "MLP-sigmoid-0.001-(64, 32, 32)\n",
      "f1 scores: [0.76297629 0.71179826 0.72782743 0.77090736 0.73499569]\n",
      "f1 mean: 0.742\n",
      "f1 std: 0.022\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 36...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 24...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 51...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 54...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 45...\n",
      "MLP-sigmoid-0.01-(5,)\n",
      "f1 scores: [0.63734697 0.63186067 0.6551684  0.67661874 0.68396842]\n",
      "f1 mean: 0.657\n",
      "f1 std: 0.021\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 30...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 28...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 29...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 39...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 40...\n",
      "MLP-sigmoid-0.01-(32,)\n",
      "f1 scores: [0.63761397 0.59005311 0.59927724 0.61014821 0.62521927]\n",
      "f1 mean: 0.612\n",
      "f1 std: 0.017\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 19...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 21...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 19...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 20...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 19...\n",
      "MLP-sigmoid-0.01-(16, 16)\n",
      "f1 scores: [0.66155601 0.5743643  0.62332971 0.60552001 0.63638453]\n",
      "f1 mean: 0.620\n",
      "f1 std: 0.029\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 23...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 28...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 24...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 27...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 38...\n",
      "MLP-sigmoid-0.01-(10, 5, 5)\n",
      "f1 scores: [0.63962413 0.59712788 0.62599649 0.63295558 0.62164423]\n",
      "f1 mean: 0.623\n",
      "f1 std: 0.015\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 22...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 23...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 29...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 24...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 30...\n",
      "MLP-sigmoid-0.01-(16, 8, 8)\n",
      "f1 scores: [0.67059216 0.70406556 0.62157663 0.60743696 0.61897482]\n",
      "f1 mean: 0.645\n",
      "f1 std: 0.037\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 18...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 19...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 19...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 27...\n",
      "MLP-sigmoid-0.01-(64, 32, 32)\n",
      "f1 scores: [0.73268196 0.70587785 0.7200938  0.71975018 0.69794138]\n",
      "f1 mean: 0.715\n",
      "f1 std: 0.012\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 24...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 27...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 37...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 46...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 65...\n",
      "MLP-sigmoid-0.1-(5,)\n",
      "f1 scores: [0.50454319 0.45822951 0.48353204 0.46592962 0.41570505]\n",
      "f1 mean: 0.466\n",
      "f1 std: 0.030\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 22...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 28...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 31...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 33...\n",
      "MLP-sigmoid-0.1-(32,)\n",
      "f1 scores: [0.545338   0.55597438 0.57145098 0.55838566 0.52658892]\n",
      "f1 mean: 0.552\n",
      "f1 std: 0.015\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 28...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 33...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 33...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 68...\n",
      "MLP-sigmoid-0.1-(16, 16)\n",
      "f1 scores: [0.5507273  0.50128315 0.53966782 0.51624026 0.45021722]\n",
      "f1 mean: 0.512\n",
      "f1 std: 0.035\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 25...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 23...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 30...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 31...\n",
      "MLP-sigmoid-0.1-(10, 5, 5)\n",
      "f1 scores: [0.50406852 0.46006957 0.52781552 0.47994449 0.48038262]\n",
      "f1 mean: 0.490\n",
      "f1 std: 0.023\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 24...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 24...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 26...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 29...\n",
      "MLP-sigmoid-0.1-(16, 8, 8)\n",
      "f1 scores: [0.55304281 0.43221492 0.48281438 0.45742724 0.4507387 ]\n",
      "f1 mean: 0.475\n",
      "f1 std: 0.042\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "MLP-sigmoid-0.1-(64, 32, 32)\n",
      "f1 scores: [0.0639725  0.06388642 0.06388642 0.06388642 0.06388642]\n",
      "f1 mean: 0.064\n",
      "f1 std: 0.000\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 24...\n",
      "MLP-sigmoid-0.5-(5,)\n",
      "f1 scores: [0.39149382 0.27509023 0.26380682 0.27956995 0.32661276]\n",
      "f1 mean: 0.307\n",
      "f1 std: 0.047\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 27...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 30...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 63...\n",
      "MLP-sigmoid-0.5-(32,)\n",
      "f1 scores: [0.43088837 0.40977829 0.37664486 0.37591438 0.38808865]\n",
      "f1 mean: 0.396\n",
      "f1 std: 0.021\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 27...\n",
      "MLP-sigmoid-0.5-(16, 16)\n",
      "f1 scores: [0.0639725  0.08075986 0.06111319 0.11670335 0.1720944 ]\n",
      "f1 mean: 0.099\n",
      "f1 std: 0.042\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "MLP-sigmoid-0.5-(10, 5, 5)\n",
      "f1 scores: [0.0639725  0.06388642 0.06388642 0.06388642 0.06388642]\n",
      "f1 mean: 0.064\n",
      "f1 std: 0.000\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "MLP-sigmoid-0.5-(16, 8, 8)\n",
      "f1 scores: [0.0639725  0.06388642 0.06388642 0.06388642 0.06388642]\n",
      "f1 mean: 0.064\n",
      "f1 std: 0.000\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 13...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 11...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 133...\n",
      "MLP-sigmoid-0.5-(64, 32, 32)\n",
      "f1 scores: [0.0639725  0.06388642 0.06388642 0.06388642 0.06388642]\n",
      "f1 mean: 0.064\n",
      "f1 std: 0.000\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cv_num = 5\n",
    "model_params = {}\n",
    "models = {}\n",
    "models_lists = {}\n",
    "\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    cv_results = cross_validate(pipeline, X, y, cv=cv_num, scoring='f1_macro', return_estimator=True, n_jobs=10)\n",
    "\n",
    "    models[model_name] = cv_results['estimator']\n",
    "    model_params[model_name] = {}\n",
    "    models_lists[model_name] = {}\n",
    "\n",
    "    num_cols = ['test_score', 'fit_time', 'score_time']\n",
    "\n",
    "    for num_col in num_cols:\n",
    "        models_lists[model_name][num_col] = cv_results[num_col]\n",
    "        model_params[model_name][f'{num_col}_mean'] = cv_results[num_col].mean()\n",
    "        model_params[model_name][f'{num_col}_std'] = cv_results[num_col].std()\n",
    "    \n",
    "    model_params[model_name]['parameter_num'] = cv_results['estimator'][0][model_name].number_of_params_\n",
    "    model_params[model_name]['hidden_layer_sizes'] = cv_results['estimator'][0][model_name].hidden_layer_sizes\n",
    "    model_params[model_name]['activation_function'] = cv_results['estimator'][0][model_name].activation_function\n",
    "    model_params[model_name]['learning_rate'] = cv_results['estimator'][0][model_name].learning_rate\n",
    "    models_lists[model_name]['converged'] = [e[model_name].converged_ for e in cv_results['estimator']]\n",
    "    models_lists[model_name]['validation_losses'] = [e[model_name].validation_losses_ for e in cv_results['estimator']]\n",
    "    models_lists[model_name]['training_losses'] = [e[model_name].training_losses_ for e in cv_results['estimator']]\n",
    "    model_params[model_name]['num_iter'] = np.array(list([len(e[model_name].training_losses_) for e in cv_results['estimator']])).mean()\n",
    "\n",
    "    print(model_name)\n",
    "    print(\n",
    "        f\"f1 scores: {models_lists[model_name]['test_score']}\\n\" +\n",
    "        f\"f1 mean: {model_params[model_name]['test_score_mean']:.3f}\\n\" +\n",
    "        f\"f1 std: {model_params[model_name]['test_score_std']:.3f}\\n\"\n",
    "    )\n",
    "    print('----------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_param = pd.DataFrame(model_params).transpose()\n",
    "df_param = df_param.reset_index(drop=False)\n",
    "df_param = df_param.rename(columns={'index': 'model'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>test_score_mean</th>\n",
       "      <th>test_score_std</th>\n",
       "      <th>fit_time_mean</th>\n",
       "      <th>fit_time_std</th>\n",
       "      <th>score_time_mean</th>\n",
       "      <th>score_time_std</th>\n",
       "      <th>parameter_num</th>\n",
       "      <th>hidden_layer_sizes</th>\n",
       "      <th>activation_function</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>num_iter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP-relu-0.0001-(5,)</td>\n",
       "      <td>0.686962</td>\n",
       "      <td>0.016567</td>\n",
       "      <td>195.440456</td>\n",
       "      <td>10.348554</td>\n",
       "      <td>0.012875</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>912</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP-relu-0.0001-(32,)</td>\n",
       "      <td>0.637483</td>\n",
       "      <td>0.015374</td>\n",
       "      <td>227.997793</td>\n",
       "      <td>8.737823</td>\n",
       "      <td>0.018614</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>5799</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>190.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP-relu-0.0001-(16, 16)</td>\n",
       "      <td>0.673423</td>\n",
       "      <td>0.015513</td>\n",
       "      <td>220.494422</td>\n",
       "      <td>16.713127</td>\n",
       "      <td>0.016936</td>\n",
       "      <td>0.005812</td>\n",
       "      <td>3175</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP-relu-0.0001-(10, 5, 5)</td>\n",
       "      <td>0.578541</td>\n",
       "      <td>0.241584</td>\n",
       "      <td>177.741284</td>\n",
       "      <td>10.017526</td>\n",
       "      <td>0.022197</td>\n",
       "      <td>0.010148</td>\n",
       "      <td>1867</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>85.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP-relu-0.0001-(16, 8, 8)</td>\n",
       "      <td>0.539063</td>\n",
       "      <td>0.229464</td>\n",
       "      <td>137.542779</td>\n",
       "      <td>9.866459</td>\n",
       "      <td>0.012556</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>3055</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>90.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP-relu-0.0001-(64, 32, 32)</td>\n",
       "      <td>0.045744</td>\n",
       "      <td>0.00276</td>\n",
       "      <td>0.970717</td>\n",
       "      <td>0.513438</td>\n",
       "      <td>0.016173</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>14503</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLP-relu-0.001-(5,)</td>\n",
       "      <td>0.694258</td>\n",
       "      <td>0.028037</td>\n",
       "      <td>74.046086</td>\n",
       "      <td>4.702816</td>\n",
       "      <td>0.012439</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>912</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLP-relu-0.001-(32,)</td>\n",
       "      <td>0.62958</td>\n",
       "      <td>0.006847</td>\n",
       "      <td>47.40502</td>\n",
       "      <td>3.317254</td>\n",
       "      <td>0.015678</td>\n",
       "      <td>0.003852</td>\n",
       "      <td>5799</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MLP-relu-0.001-(16, 16)</td>\n",
       "      <td>0.073786</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>2.5712</td>\n",
       "      <td>0.444815</td>\n",
       "      <td>0.015335</td>\n",
       "      <td>0.003307</td>\n",
       "      <td>3175</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLP-relu-0.001-(10, 5, 5)</td>\n",
       "      <td>0.089531</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>1.779617</td>\n",
       "      <td>0.256817</td>\n",
       "      <td>0.014233</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>1867</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MLP-relu-0.001-(16, 8, 8)</td>\n",
       "      <td>0.086301</td>\n",
       "      <td>0.004569</td>\n",
       "      <td>0.893262</td>\n",
       "      <td>0.090265</td>\n",
       "      <td>0.01501</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>3055</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MLP-relu-0.001-(64, 32, 32)</td>\n",
       "      <td>0.045744</td>\n",
       "      <td>0.00276</td>\n",
       "      <td>0.22294</td>\n",
       "      <td>0.017618</td>\n",
       "      <td>0.024681</td>\n",
       "      <td>0.004612</td>\n",
       "      <td>14503</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MLP-relu-0.01-(5,)</td>\n",
       "      <td>0.062027</td>\n",
       "      <td>0.003398</td>\n",
       "      <td>0.819374</td>\n",
       "      <td>0.112048</td>\n",
       "      <td>0.016045</td>\n",
       "      <td>0.003327</td>\n",
       "      <td>912</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MLP-relu-0.01-(32,)</td>\n",
       "      <td>0.056851</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>0.622187</td>\n",
       "      <td>0.05841</td>\n",
       "      <td>0.018271</td>\n",
       "      <td>0.003483</td>\n",
       "      <td>5799</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MLP-relu-0.01-(16, 16)</td>\n",
       "      <td>0.073786</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.2426</td>\n",
       "      <td>0.018956</td>\n",
       "      <td>0.022185</td>\n",
       "      <td>0.003096</td>\n",
       "      <td>3175</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MLP-relu-0.01-(10, 5, 5)</td>\n",
       "      <td>0.089531</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>0.255646</td>\n",
       "      <td>0.036662</td>\n",
       "      <td>0.024954</td>\n",
       "      <td>0.00746</td>\n",
       "      <td>1867</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MLP-relu-0.01-(16, 8, 8)</td>\n",
       "      <td>0.086301</td>\n",
       "      <td>0.004569</td>\n",
       "      <td>0.203969</td>\n",
       "      <td>0.018059</td>\n",
       "      <td>0.021377</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>3055</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MLP-relu-0.01-(64, 32, 32)</td>\n",
       "      <td>0.045744</td>\n",
       "      <td>0.00276</td>\n",
       "      <td>0.214954</td>\n",
       "      <td>0.010826</td>\n",
       "      <td>0.025513</td>\n",
       "      <td>0.007809</td>\n",
       "      <td>14503</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MLP-relu-0.1-(5,)</td>\n",
       "      <td>0.062027</td>\n",
       "      <td>0.003398</td>\n",
       "      <td>0.196002</td>\n",
       "      <td>0.014887</td>\n",
       "      <td>0.020382</td>\n",
       "      <td>0.00488</td>\n",
       "      <td>912</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MLP-relu-0.1-(32,)</td>\n",
       "      <td>0.056851</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>0.200418</td>\n",
       "      <td>0.011752</td>\n",
       "      <td>0.025898</td>\n",
       "      <td>0.005824</td>\n",
       "      <td>5799</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MLP-relu-0.1-(16, 16)</td>\n",
       "      <td>0.073786</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.200258</td>\n",
       "      <td>0.011009</td>\n",
       "      <td>0.025368</td>\n",
       "      <td>0.005182</td>\n",
       "      <td>3175</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MLP-relu-0.1-(10, 5, 5)</td>\n",
       "      <td>0.089531</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>0.217665</td>\n",
       "      <td>0.014051</td>\n",
       "      <td>0.02595</td>\n",
       "      <td>0.005574</td>\n",
       "      <td>1867</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>MLP-relu-0.1-(16, 8, 8)</td>\n",
       "      <td>0.086301</td>\n",
       "      <td>0.004569</td>\n",
       "      <td>0.20253</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.024914</td>\n",
       "      <td>0.004484</td>\n",
       "      <td>3055</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>MLP-relu-0.1-(64, 32, 32)</td>\n",
       "      <td>0.045744</td>\n",
       "      <td>0.00276</td>\n",
       "      <td>0.217253</td>\n",
       "      <td>0.02578</td>\n",
       "      <td>0.032837</td>\n",
       "      <td>0.005794</td>\n",
       "      <td>14503</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MLP-relu-0.5-(5,)</td>\n",
       "      <td>0.062027</td>\n",
       "      <td>0.003398</td>\n",
       "      <td>0.202215</td>\n",
       "      <td>0.017748</td>\n",
       "      <td>0.026379</td>\n",
       "      <td>0.005975</td>\n",
       "      <td>912</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>MLP-relu-0.5-(32,)</td>\n",
       "      <td>0.056851</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>0.204371</td>\n",
       "      <td>0.017975</td>\n",
       "      <td>0.023531</td>\n",
       "      <td>0.006234</td>\n",
       "      <td>5799</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>MLP-relu-0.5-(16, 16)</td>\n",
       "      <td>0.073786</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.198439</td>\n",
       "      <td>0.014372</td>\n",
       "      <td>0.025747</td>\n",
       "      <td>0.009169</td>\n",
       "      <td>3175</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>MLP-relu-0.5-(10, 5, 5)</td>\n",
       "      <td>0.089531</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>0.21005</td>\n",
       "      <td>0.017537</td>\n",
       "      <td>0.022478</td>\n",
       "      <td>0.006589</td>\n",
       "      <td>1867</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>MLP-relu-0.5-(16, 8, 8)</td>\n",
       "      <td>0.086301</td>\n",
       "      <td>0.004569</td>\n",
       "      <td>0.204753</td>\n",
       "      <td>0.007796</td>\n",
       "      <td>0.02307</td>\n",
       "      <td>0.006416</td>\n",
       "      <td>3055</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>MLP-relu-0.5-(64, 32, 32)</td>\n",
       "      <td>0.045744</td>\n",
       "      <td>0.00276</td>\n",
       "      <td>0.209379</td>\n",
       "      <td>0.020704</td>\n",
       "      <td>0.029601</td>\n",
       "      <td>0.00825</td>\n",
       "      <td>14503</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>MLP-sigmoid-0.0001-(5,)</td>\n",
       "      <td>0.664158</td>\n",
       "      <td>0.023146</td>\n",
       "      <td>440.73597</td>\n",
       "      <td>12.864529</td>\n",
       "      <td>0.016419</td>\n",
       "      <td>0.002417</td>\n",
       "      <td>912</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>450.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>MLP-sigmoid-0.0001-(32,)</td>\n",
       "      <td>0.624561</td>\n",
       "      <td>0.019937</td>\n",
       "      <td>584.562932</td>\n",
       "      <td>28.495352</td>\n",
       "      <td>0.020113</td>\n",
       "      <td>0.005377</td>\n",
       "      <td>5799</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>419.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>MLP-sigmoid-0.0001-(16, 16)</td>\n",
       "      <td>0.670051</td>\n",
       "      <td>0.028579</td>\n",
       "      <td>657.85911</td>\n",
       "      <td>37.299178</td>\n",
       "      <td>0.018646</td>\n",
       "      <td>0.004392</td>\n",
       "      <td>3175</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>451.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>MLP-sigmoid-0.0001-(10, 5, 5)</td>\n",
       "      <td>0.105976</td>\n",
       "      <td>0.004382</td>\n",
       "      <td>29.592399</td>\n",
       "      <td>3.600011</td>\n",
       "      <td>0.014775</td>\n",
       "      <td>0.001648</td>\n",
       "      <td>1867</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>MLP-sigmoid-0.0001-(16, 8, 8)</td>\n",
       "      <td>0.139668</td>\n",
       "      <td>0.002904</td>\n",
       "      <td>28.331946</td>\n",
       "      <td>1.617752</td>\n",
       "      <td>0.015339</td>\n",
       "      <td>0.004689</td>\n",
       "      <td>3055</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>MLP-sigmoid-0.0001-(64, 32, 32)</td>\n",
       "      <td>0.742338</td>\n",
       "      <td>0.021881</td>\n",
       "      <td>1236.012081</td>\n",
       "      <td>20.893088</td>\n",
       "      <td>0.019591</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>14503</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>449.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>MLP-sigmoid-0.001-(5,)</td>\n",
       "      <td>0.684587</td>\n",
       "      <td>0.025795</td>\n",
       "      <td>144.577555</td>\n",
       "      <td>10.657759</td>\n",
       "      <td>0.015434</td>\n",
       "      <td>0.003355</td>\n",
       "      <td>912</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "      <td>140.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>MLP-sigmoid-0.001-(32,)</td>\n",
       "      <td>0.617926</td>\n",
       "      <td>0.011712</td>\n",
       "      <td>165.608033</td>\n",
       "      <td>9.164667</td>\n",
       "      <td>0.014667</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>5799</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "      <td>130.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>MLP-sigmoid-0.001-(16, 16)</td>\n",
       "      <td>0.654194</td>\n",
       "      <td>0.021783</td>\n",
       "      <td>151.893695</td>\n",
       "      <td>15.602767</td>\n",
       "      <td>0.017306</td>\n",
       "      <td>0.003098</td>\n",
       "      <td>3175</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "      <td>96.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>MLP-sigmoid-0.001-(10, 5, 5)</td>\n",
       "      <td>0.667011</td>\n",
       "      <td>0.018989</td>\n",
       "      <td>241.438698</td>\n",
       "      <td>14.732556</td>\n",
       "      <td>0.017306</td>\n",
       "      <td>0.004458</td>\n",
       "      <td>1867</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "      <td>136.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>MLP-sigmoid-0.001-(16, 8, 8)</td>\n",
       "      <td>0.692447</td>\n",
       "      <td>0.018932</td>\n",
       "      <td>242.074241</td>\n",
       "      <td>10.673677</td>\n",
       "      <td>0.017482</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>3055</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "      <td>119.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>MLP-sigmoid-0.001-(64, 32, 32)</td>\n",
       "      <td>0.741701</td>\n",
       "      <td>0.022078</td>\n",
       "      <td>229.434758</td>\n",
       "      <td>7.056314</td>\n",
       "      <td>0.023255</td>\n",
       "      <td>0.006144</td>\n",
       "      <td>14503</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "      <td>81.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>MLP-sigmoid-0.01-(5,)</td>\n",
       "      <td>0.656993</td>\n",
       "      <td>0.020658</td>\n",
       "      <td>41.154121</td>\n",
       "      <td>7.379066</td>\n",
       "      <td>0.014924</td>\n",
       "      <td>0.002388</td>\n",
       "      <td>912</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>MLP-sigmoid-0.01-(32,)</td>\n",
       "      <td>0.612462</td>\n",
       "      <td>0.017186</td>\n",
       "      <td>46.870801</td>\n",
       "      <td>5.088577</td>\n",
       "      <td>0.015754</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>5799</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "      <td>34.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>MLP-sigmoid-0.01-(16, 16)</td>\n",
       "      <td>0.620231</td>\n",
       "      <td>0.029321</td>\n",
       "      <td>32.883365</td>\n",
       "      <td>1.995959</td>\n",
       "      <td>0.019834</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>3175</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>MLP-sigmoid-0.01-(10, 5, 5)</td>\n",
       "      <td>0.62347</td>\n",
       "      <td>0.014523</td>\n",
       "      <td>51.430097</td>\n",
       "      <td>6.486761</td>\n",
       "      <td>0.019545</td>\n",
       "      <td>0.005087</td>\n",
       "      <td>1867</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>MLP-sigmoid-0.01-(16, 8, 8)</td>\n",
       "      <td>0.644529</td>\n",
       "      <td>0.036823</td>\n",
       "      <td>48.245863</td>\n",
       "      <td>7.77949</td>\n",
       "      <td>0.016937</td>\n",
       "      <td>0.002109</td>\n",
       "      <td>3055</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "      <td>26.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>MLP-sigmoid-0.01-(64, 32, 32)</td>\n",
       "      <td>0.715269</td>\n",
       "      <td>0.012124</td>\n",
       "      <td>57.154692</td>\n",
       "      <td>8.047412</td>\n",
       "      <td>0.019277</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>14503</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>MLP-sigmoid-0.1-(5,)</td>\n",
       "      <td>0.465588</td>\n",
       "      <td>0.029604</td>\n",
       "      <td>42.865201</td>\n",
       "      <td>11.277539</td>\n",
       "      <td>0.017712</td>\n",
       "      <td>0.00439</td>\n",
       "      <td>912</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1</td>\n",
       "      <td>40.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>MLP-sigmoid-0.1-(32,)</td>\n",
       "      <td>0.551548</td>\n",
       "      <td>0.014993</td>\n",
       "      <td>35.121342</td>\n",
       "      <td>6.353176</td>\n",
       "      <td>0.017669</td>\n",
       "      <td>0.004548</td>\n",
       "      <td>5799</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>MLP-sigmoid-0.1-(16, 16)</td>\n",
       "      <td>0.511627</td>\n",
       "      <td>0.035255</td>\n",
       "      <td>53.483966</td>\n",
       "      <td>24.132931</td>\n",
       "      <td>0.02048</td>\n",
       "      <td>0.005516</td>\n",
       "      <td>3175</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1</td>\n",
       "      <td>35.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>MLP-sigmoid-0.1-(10, 5, 5)</td>\n",
       "      <td>0.490456</td>\n",
       "      <td>0.023308</td>\n",
       "      <td>51.369136</td>\n",
       "      <td>6.373951</td>\n",
       "      <td>0.015095</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>1867</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>MLP-sigmoid-0.1-(16, 8, 8)</td>\n",
       "      <td>0.475248</td>\n",
       "      <td>0.042141</td>\n",
       "      <td>42.663736</td>\n",
       "      <td>10.462118</td>\n",
       "      <td>0.015105</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>3055</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>MLP-sigmoid-0.1-(64, 32, 32)</td>\n",
       "      <td>0.063904</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>29.39469</td>\n",
       "      <td>1.988913</td>\n",
       "      <td>0.017921</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>14503</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>MLP-sigmoid-0.5-(5,)</td>\n",
       "      <td>0.307315</td>\n",
       "      <td>0.047243</td>\n",
       "      <td>20.809324</td>\n",
       "      <td>3.228433</td>\n",
       "      <td>0.018158</td>\n",
       "      <td>0.005211</td>\n",
       "      <td>912</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.5</td>\n",
       "      <td>19.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>MLP-sigmoid-0.5-(32,)</td>\n",
       "      <td>0.396263</td>\n",
       "      <td>0.021201</td>\n",
       "      <td>47.649472</td>\n",
       "      <td>16.728562</td>\n",
       "      <td>0.019209</td>\n",
       "      <td>0.00842</td>\n",
       "      <td>5799</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.5</td>\n",
       "      <td>34.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>MLP-sigmoid-0.5-(16, 16)</td>\n",
       "      <td>0.098929</td>\n",
       "      <td>0.041596</td>\n",
       "      <td>34.703192</td>\n",
       "      <td>8.168717</td>\n",
       "      <td>0.020181</td>\n",
       "      <td>0.007853</td>\n",
       "      <td>3175</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.5</td>\n",
       "      <td>18.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>MLP-sigmoid-0.5-(10, 5, 5)</td>\n",
       "      <td>0.063904</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>19.741083</td>\n",
       "      <td>2.833805</td>\n",
       "      <td>0.014228</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>1867</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.5</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>MLP-sigmoid-0.5-(16, 8, 8)</td>\n",
       "      <td>0.063904</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>21.181497</td>\n",
       "      <td>1.437725</td>\n",
       "      <td>0.018468</td>\n",
       "      <td>0.005024</td>\n",
       "      <td>3055</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.5</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>MLP-sigmoid-0.5-(64, 32, 32)</td>\n",
       "      <td>0.063904</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>80.067443</td>\n",
       "      <td>94.98366</td>\n",
       "      <td>0.026701</td>\n",
       "      <td>0.007478</td>\n",
       "      <td>14503</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.5</td>\n",
       "      <td>36.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              model test_score_mean test_score_std  \\\n",
       "0              MLP-relu-0.0001-(5,)        0.686962       0.016567   \n",
       "1             MLP-relu-0.0001-(32,)        0.637483       0.015374   \n",
       "2          MLP-relu-0.0001-(16, 16)        0.673423       0.015513   \n",
       "3        MLP-relu-0.0001-(10, 5, 5)        0.578541       0.241584   \n",
       "4        MLP-relu-0.0001-(16, 8, 8)        0.539063       0.229464   \n",
       "5      MLP-relu-0.0001-(64, 32, 32)        0.045744        0.00276   \n",
       "6               MLP-relu-0.001-(5,)        0.694258       0.028037   \n",
       "7              MLP-relu-0.001-(32,)         0.62958       0.006847   \n",
       "8           MLP-relu-0.001-(16, 16)        0.073786       0.002305   \n",
       "9         MLP-relu-0.001-(10, 5, 5)        0.089531       0.003964   \n",
       "10        MLP-relu-0.001-(16, 8, 8)        0.086301       0.004569   \n",
       "11      MLP-relu-0.001-(64, 32, 32)        0.045744        0.00276   \n",
       "12               MLP-relu-0.01-(5,)        0.062027       0.003398   \n",
       "13              MLP-relu-0.01-(32,)        0.056851       0.003322   \n",
       "14           MLP-relu-0.01-(16, 16)        0.073786       0.002305   \n",
       "15         MLP-relu-0.01-(10, 5, 5)        0.089531       0.003964   \n",
       "16         MLP-relu-0.01-(16, 8, 8)        0.086301       0.004569   \n",
       "17       MLP-relu-0.01-(64, 32, 32)        0.045744        0.00276   \n",
       "18                MLP-relu-0.1-(5,)        0.062027       0.003398   \n",
       "19               MLP-relu-0.1-(32,)        0.056851       0.003322   \n",
       "20            MLP-relu-0.1-(16, 16)        0.073786       0.002305   \n",
       "21          MLP-relu-0.1-(10, 5, 5)        0.089531       0.003964   \n",
       "22          MLP-relu-0.1-(16, 8, 8)        0.086301       0.004569   \n",
       "23        MLP-relu-0.1-(64, 32, 32)        0.045744        0.00276   \n",
       "24                MLP-relu-0.5-(5,)        0.062027       0.003398   \n",
       "25               MLP-relu-0.5-(32,)        0.056851       0.003322   \n",
       "26            MLP-relu-0.5-(16, 16)        0.073786       0.002305   \n",
       "27          MLP-relu-0.5-(10, 5, 5)        0.089531       0.003964   \n",
       "28          MLP-relu-0.5-(16, 8, 8)        0.086301       0.004569   \n",
       "29        MLP-relu-0.5-(64, 32, 32)        0.045744        0.00276   \n",
       "30          MLP-sigmoid-0.0001-(5,)        0.664158       0.023146   \n",
       "31         MLP-sigmoid-0.0001-(32,)        0.624561       0.019937   \n",
       "32      MLP-sigmoid-0.0001-(16, 16)        0.670051       0.028579   \n",
       "33    MLP-sigmoid-0.0001-(10, 5, 5)        0.105976       0.004382   \n",
       "34    MLP-sigmoid-0.0001-(16, 8, 8)        0.139668       0.002904   \n",
       "35  MLP-sigmoid-0.0001-(64, 32, 32)        0.742338       0.021881   \n",
       "36           MLP-sigmoid-0.001-(5,)        0.684587       0.025795   \n",
       "37          MLP-sigmoid-0.001-(32,)        0.617926       0.011712   \n",
       "38       MLP-sigmoid-0.001-(16, 16)        0.654194       0.021783   \n",
       "39     MLP-sigmoid-0.001-(10, 5, 5)        0.667011       0.018989   \n",
       "40     MLP-sigmoid-0.001-(16, 8, 8)        0.692447       0.018932   \n",
       "41   MLP-sigmoid-0.001-(64, 32, 32)        0.741701       0.022078   \n",
       "42            MLP-sigmoid-0.01-(5,)        0.656993       0.020658   \n",
       "43           MLP-sigmoid-0.01-(32,)        0.612462       0.017186   \n",
       "44        MLP-sigmoid-0.01-(16, 16)        0.620231       0.029321   \n",
       "45      MLP-sigmoid-0.01-(10, 5, 5)         0.62347       0.014523   \n",
       "46      MLP-sigmoid-0.01-(16, 8, 8)        0.644529       0.036823   \n",
       "47    MLP-sigmoid-0.01-(64, 32, 32)        0.715269       0.012124   \n",
       "48             MLP-sigmoid-0.1-(5,)        0.465588       0.029604   \n",
       "49            MLP-sigmoid-0.1-(32,)        0.551548       0.014993   \n",
       "50         MLP-sigmoid-0.1-(16, 16)        0.511627       0.035255   \n",
       "51       MLP-sigmoid-0.1-(10, 5, 5)        0.490456       0.023308   \n",
       "52       MLP-sigmoid-0.1-(16, 8, 8)        0.475248       0.042141   \n",
       "53     MLP-sigmoid-0.1-(64, 32, 32)        0.063904       0.000034   \n",
       "54             MLP-sigmoid-0.5-(5,)        0.307315       0.047243   \n",
       "55            MLP-sigmoid-0.5-(32,)        0.396263       0.021201   \n",
       "56         MLP-sigmoid-0.5-(16, 16)        0.098929       0.041596   \n",
       "57       MLP-sigmoid-0.5-(10, 5, 5)        0.063904       0.000034   \n",
       "58       MLP-sigmoid-0.5-(16, 8, 8)        0.063904       0.000034   \n",
       "59     MLP-sigmoid-0.5-(64, 32, 32)        0.063904       0.000034   \n",
       "\n",
       "   fit_time_mean fit_time_std score_time_mean score_time_std parameter_num  \\\n",
       "0     195.440456    10.348554        0.012875       0.000351           912   \n",
       "1     227.997793     8.737823        0.018614       0.002242          5799   \n",
       "2     220.494422    16.713127        0.016936       0.005812          3175   \n",
       "3     177.741284    10.017526        0.022197       0.010148          1867   \n",
       "4     137.542779     9.866459        0.012556       0.000707          3055   \n",
       "5       0.970717     0.513438        0.016173       0.001392         14503   \n",
       "6      74.046086     4.702816        0.012439       0.000797           912   \n",
       "7       47.40502     3.317254        0.015678       0.003852          5799   \n",
       "8         2.5712     0.444815        0.015335       0.003307          3175   \n",
       "9       1.779617     0.256817        0.014233       0.001285          1867   \n",
       "10      0.893262     0.090265         0.01501       0.000975          3055   \n",
       "11       0.22294     0.017618        0.024681       0.004612         14503   \n",
       "12      0.819374     0.112048        0.016045       0.003327           912   \n",
       "13      0.622187      0.05841        0.018271       0.003483          5799   \n",
       "14        0.2426     0.018956        0.022185       0.003096          3175   \n",
       "15      0.255646     0.036662        0.024954        0.00746          1867   \n",
       "16      0.203969     0.018059        0.021377       0.002086          3055   \n",
       "17      0.214954     0.010826        0.025513       0.007809         14503   \n",
       "18      0.196002     0.014887        0.020382        0.00488           912   \n",
       "19      0.200418     0.011752        0.025898       0.005824          5799   \n",
       "20      0.200258     0.011009        0.025368       0.005182          3175   \n",
       "21      0.217665     0.014051         0.02595       0.005574          1867   \n",
       "22       0.20253       0.0129        0.024914       0.004484          3055   \n",
       "23      0.217253      0.02578        0.032837       0.005794         14503   \n",
       "24      0.202215     0.017748        0.026379       0.005975           912   \n",
       "25      0.204371     0.017975        0.023531       0.006234          5799   \n",
       "26      0.198439     0.014372        0.025747       0.009169          3175   \n",
       "27       0.21005     0.017537        0.022478       0.006589          1867   \n",
       "28      0.204753     0.007796         0.02307       0.006416          3055   \n",
       "29      0.209379     0.020704        0.029601        0.00825         14503   \n",
       "30     440.73597    12.864529        0.016419       0.002417           912   \n",
       "31    584.562932    28.495352        0.020113       0.005377          5799   \n",
       "32     657.85911    37.299178        0.018646       0.004392          3175   \n",
       "33     29.592399     3.600011        0.014775       0.001648          1867   \n",
       "34     28.331946     1.617752        0.015339       0.004689          3055   \n",
       "35   1236.012081    20.893088        0.019591       0.003391         14503   \n",
       "36    144.577555    10.657759        0.015434       0.003355           912   \n",
       "37    165.608033     9.164667        0.014667       0.001685          5799   \n",
       "38    151.893695    15.602767        0.017306       0.003098          3175   \n",
       "39    241.438698    14.732556        0.017306       0.004458          1867   \n",
       "40    242.074241    10.673677        0.017482       0.002108          3055   \n",
       "41    229.434758     7.056314        0.023255       0.006144         14503   \n",
       "42     41.154121     7.379066        0.014924       0.002388           912   \n",
       "43     46.870801     5.088577        0.015754       0.001379          5799   \n",
       "44     32.883365     1.995959        0.019834       0.003064          3175   \n",
       "45     51.430097     6.486761        0.019545       0.005087          1867   \n",
       "46     48.245863      7.77949        0.016937       0.002109          3055   \n",
       "47     57.154692     8.047412        0.019277       0.001969         14503   \n",
       "48     42.865201    11.277539        0.017712        0.00439           912   \n",
       "49     35.121342     6.353176        0.017669       0.004548          5799   \n",
       "50     53.483966    24.132931         0.02048       0.005516          3175   \n",
       "51     51.369136     6.373951        0.015095       0.001277          1867   \n",
       "52     42.663736    10.462118        0.015105       0.001057          3055   \n",
       "53      29.39469     1.988913        0.017921       0.001451         14503   \n",
       "54     20.809324     3.228433        0.018158       0.005211           912   \n",
       "55     47.649472    16.728562        0.019209        0.00842          5799   \n",
       "56     34.703192     8.168717        0.020181       0.007853          3175   \n",
       "57     19.741083     2.833805        0.014228       0.001078          1867   \n",
       "58     21.181497     1.437725        0.018468       0.005024          3055   \n",
       "59     80.067443     94.98366        0.026701       0.007478         14503   \n",
       "\n",
       "   hidden_layer_sizes activation_function learning_rate num_iter  \n",
       "0                (5,)                relu        0.0001    217.0  \n",
       "1               (32,)                relu        0.0001    190.4  \n",
       "2            (16, 16)                relu        0.0001    140.0  \n",
       "3          (10, 5, 5)                relu        0.0001     85.4  \n",
       "4          (16, 8, 8)                relu        0.0001     90.2  \n",
       "5        (64, 32, 32)                relu        0.0001      0.0  \n",
       "6                (5,)                relu         0.001     87.0  \n",
       "7               (32,)                relu         0.001     43.0  \n",
       "8            (16, 16)                relu         0.001      1.2  \n",
       "9          (10, 5, 5)                relu         0.001      0.8  \n",
       "10         (16, 8, 8)                relu         0.001      0.0  \n",
       "11       (64, 32, 32)                relu         0.001      0.0  \n",
       "12               (5,)                relu          0.01      0.0  \n",
       "13              (32,)                relu          0.01      0.0  \n",
       "14           (16, 16)                relu          0.01      0.0  \n",
       "15         (10, 5, 5)                relu          0.01      0.0  \n",
       "16         (16, 8, 8)                relu          0.01      0.0  \n",
       "17       (64, 32, 32)                relu          0.01      0.0  \n",
       "18               (5,)                relu           0.1      0.0  \n",
       "19              (32,)                relu           0.1      0.0  \n",
       "20           (16, 16)                relu           0.1      0.0  \n",
       "21         (10, 5, 5)                relu           0.1      0.0  \n",
       "22         (16, 8, 8)                relu           0.1      0.0  \n",
       "23       (64, 32, 32)                relu           0.1      0.0  \n",
       "24               (5,)                relu           0.5      0.0  \n",
       "25              (32,)                relu           0.5      0.0  \n",
       "26           (16, 16)                relu           0.5      0.0  \n",
       "27         (10, 5, 5)                relu           0.5      0.0  \n",
       "28         (16, 8, 8)                relu           0.5      0.0  \n",
       "29       (64, 32, 32)                relu           0.5      0.0  \n",
       "30               (5,)             sigmoid        0.0001    450.8  \n",
       "31              (32,)             sigmoid        0.0001    419.4  \n",
       "32           (16, 16)             sigmoid        0.0001    451.2  \n",
       "33         (10, 5, 5)             sigmoid        0.0001     16.8  \n",
       "34         (16, 8, 8)             sigmoid        0.0001     15.0  \n",
       "35       (64, 32, 32)             sigmoid        0.0001    449.0  \n",
       "36               (5,)             sigmoid         0.001    140.4  \n",
       "37              (32,)             sigmoid         0.001    130.6  \n",
       "38           (16, 16)             sigmoid         0.001     96.4  \n",
       "39         (10, 5, 5)             sigmoid         0.001    136.2  \n",
       "40         (16, 8, 8)             sigmoid         0.001    119.8  \n",
       "41       (64, 32, 32)             sigmoid         0.001     81.2  \n",
       "42               (5,)             sigmoid          0.01     43.0  \n",
       "43              (32,)             sigmoid          0.01     34.2  \n",
       "44           (16, 16)             sigmoid          0.01     20.6  \n",
       "45         (10, 5, 5)             sigmoid          0.01     29.0  \n",
       "46         (16, 8, 8)             sigmoid          0.01     26.6  \n",
       "47       (64, 32, 32)             sigmoid          0.01     20.8  \n",
       "48               (5,)             sigmoid           0.1     40.8  \n",
       "49              (32,)             sigmoid           0.1     27.0  \n",
       "50           (16, 16)             sigmoid           0.1     35.8  \n",
       "51         (10, 5, 5)             sigmoid           0.1     26.8  \n",
       "52         (16, 8, 8)             sigmoid           0.1     24.0  \n",
       "53       (64, 32, 32)             sigmoid           0.1     11.0  \n",
       "54               (5,)             sigmoid           0.5     19.4  \n",
       "55              (32,)             sigmoid           0.5     34.2  \n",
       "56           (16, 16)             sigmoid           0.5     18.8  \n",
       "57         (10, 5, 5)             sigmoid           0.5     11.0  \n",
       "58         (16, 8, 8)             sigmoid           0.5     11.0  \n",
       "59       (64, 32, 32)             sigmoid           0.5     36.4  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_param.to_csv(r'results/loan_params.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_param = pd.read_csv(r'results/loan_params.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>test_score_mean</th>\n",
       "      <th>test_score_std</th>\n",
       "      <th>fit_time_mean</th>\n",
       "      <th>fit_time_std</th>\n",
       "      <th>score_time_mean</th>\n",
       "      <th>score_time_std</th>\n",
       "      <th>parameter_num</th>\n",
       "      <th>hidden_layer_sizes</th>\n",
       "      <th>activation_function</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>num_iter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP-relu-0.0001-(5,)</td>\n",
       "      <td>0.686962</td>\n",
       "      <td>0.016567</td>\n",
       "      <td>195.440456</td>\n",
       "      <td>10.348554</td>\n",
       "      <td>0.012875</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>912</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP-relu-0.0001-(32,)</td>\n",
       "      <td>0.637483</td>\n",
       "      <td>0.015374</td>\n",
       "      <td>227.997793</td>\n",
       "      <td>8.737823</td>\n",
       "      <td>0.018614</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>5799</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>190.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP-relu-0.0001-(16, 16)</td>\n",
       "      <td>0.673423</td>\n",
       "      <td>0.015513</td>\n",
       "      <td>220.494422</td>\n",
       "      <td>16.713127</td>\n",
       "      <td>0.016936</td>\n",
       "      <td>0.005812</td>\n",
       "      <td>3175</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP-relu-0.0001-(10, 5, 5)</td>\n",
       "      <td>0.578541</td>\n",
       "      <td>0.241584</td>\n",
       "      <td>177.741284</td>\n",
       "      <td>10.017526</td>\n",
       "      <td>0.022197</td>\n",
       "      <td>0.010148</td>\n",
       "      <td>1867</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>85.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP-relu-0.0001-(16, 8, 8)</td>\n",
       "      <td>0.539063</td>\n",
       "      <td>0.229464</td>\n",
       "      <td>137.542779</td>\n",
       "      <td>9.866459</td>\n",
       "      <td>0.012556</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>3055</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>90.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP-relu-0.0001-(64, 32, 32)</td>\n",
       "      <td>0.045744</td>\n",
       "      <td>0.002760</td>\n",
       "      <td>0.970717</td>\n",
       "      <td>0.513438</td>\n",
       "      <td>0.016173</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>14503</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLP-relu-0.001-(5,)</td>\n",
       "      <td>0.694258</td>\n",
       "      <td>0.028037</td>\n",
       "      <td>74.046086</td>\n",
       "      <td>4.702816</td>\n",
       "      <td>0.012439</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>912</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLP-relu-0.001-(32,)</td>\n",
       "      <td>0.629580</td>\n",
       "      <td>0.006847</td>\n",
       "      <td>47.405020</td>\n",
       "      <td>3.317254</td>\n",
       "      <td>0.015678</td>\n",
       "      <td>0.003852</td>\n",
       "      <td>5799</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MLP-relu-0.001-(16, 16)</td>\n",
       "      <td>0.073786</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>2.571200</td>\n",
       "      <td>0.444815</td>\n",
       "      <td>0.015335</td>\n",
       "      <td>0.003307</td>\n",
       "      <td>3175</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLP-relu-0.001-(10, 5, 5)</td>\n",
       "      <td>0.089531</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>1.779617</td>\n",
       "      <td>0.256817</td>\n",
       "      <td>0.014233</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>1867</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MLP-relu-0.001-(16, 8, 8)</td>\n",
       "      <td>0.086301</td>\n",
       "      <td>0.004569</td>\n",
       "      <td>0.893262</td>\n",
       "      <td>0.090265</td>\n",
       "      <td>0.015010</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>3055</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MLP-relu-0.001-(64, 32, 32)</td>\n",
       "      <td>0.045744</td>\n",
       "      <td>0.002760</td>\n",
       "      <td>0.222940</td>\n",
       "      <td>0.017618</td>\n",
       "      <td>0.024681</td>\n",
       "      <td>0.004612</td>\n",
       "      <td>14503</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MLP-relu-0.01-(5,)</td>\n",
       "      <td>0.062027</td>\n",
       "      <td>0.003398</td>\n",
       "      <td>0.819374</td>\n",
       "      <td>0.112048</td>\n",
       "      <td>0.016045</td>\n",
       "      <td>0.003327</td>\n",
       "      <td>912</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MLP-relu-0.01-(32,)</td>\n",
       "      <td>0.056851</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>0.622187</td>\n",
       "      <td>0.058410</td>\n",
       "      <td>0.018271</td>\n",
       "      <td>0.003483</td>\n",
       "      <td>5799</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MLP-relu-0.01-(16, 16)</td>\n",
       "      <td>0.073786</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.242600</td>\n",
       "      <td>0.018956</td>\n",
       "      <td>0.022185</td>\n",
       "      <td>0.003096</td>\n",
       "      <td>3175</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MLP-relu-0.01-(10, 5, 5)</td>\n",
       "      <td>0.089531</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>0.255646</td>\n",
       "      <td>0.036662</td>\n",
       "      <td>0.024954</td>\n",
       "      <td>0.007460</td>\n",
       "      <td>1867</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MLP-relu-0.01-(16, 8, 8)</td>\n",
       "      <td>0.086301</td>\n",
       "      <td>0.004569</td>\n",
       "      <td>0.203969</td>\n",
       "      <td>0.018059</td>\n",
       "      <td>0.021377</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>3055</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MLP-relu-0.01-(64, 32, 32)</td>\n",
       "      <td>0.045744</td>\n",
       "      <td>0.002760</td>\n",
       "      <td>0.214954</td>\n",
       "      <td>0.010826</td>\n",
       "      <td>0.025513</td>\n",
       "      <td>0.007809</td>\n",
       "      <td>14503</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MLP-relu-0.1-(5,)</td>\n",
       "      <td>0.062027</td>\n",
       "      <td>0.003398</td>\n",
       "      <td>0.196002</td>\n",
       "      <td>0.014887</td>\n",
       "      <td>0.020382</td>\n",
       "      <td>0.004880</td>\n",
       "      <td>912</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MLP-relu-0.1-(32,)</td>\n",
       "      <td>0.056851</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>0.200418</td>\n",
       "      <td>0.011752</td>\n",
       "      <td>0.025898</td>\n",
       "      <td>0.005824</td>\n",
       "      <td>5799</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MLP-relu-0.1-(16, 16)</td>\n",
       "      <td>0.073786</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.200258</td>\n",
       "      <td>0.011009</td>\n",
       "      <td>0.025368</td>\n",
       "      <td>0.005182</td>\n",
       "      <td>3175</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MLP-relu-0.1-(10, 5, 5)</td>\n",
       "      <td>0.089531</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>0.217665</td>\n",
       "      <td>0.014051</td>\n",
       "      <td>0.025950</td>\n",
       "      <td>0.005574</td>\n",
       "      <td>1867</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>MLP-relu-0.1-(16, 8, 8)</td>\n",
       "      <td>0.086301</td>\n",
       "      <td>0.004569</td>\n",
       "      <td>0.202530</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.024914</td>\n",
       "      <td>0.004484</td>\n",
       "      <td>3055</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>MLP-relu-0.1-(64, 32, 32)</td>\n",
       "      <td>0.045744</td>\n",
       "      <td>0.002760</td>\n",
       "      <td>0.217253</td>\n",
       "      <td>0.025780</td>\n",
       "      <td>0.032837</td>\n",
       "      <td>0.005794</td>\n",
       "      <td>14503</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MLP-relu-0.5-(5,)</td>\n",
       "      <td>0.062027</td>\n",
       "      <td>0.003398</td>\n",
       "      <td>0.202215</td>\n",
       "      <td>0.017748</td>\n",
       "      <td>0.026379</td>\n",
       "      <td>0.005975</td>\n",
       "      <td>912</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>MLP-relu-0.5-(32,)</td>\n",
       "      <td>0.056851</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>0.204371</td>\n",
       "      <td>0.017975</td>\n",
       "      <td>0.023531</td>\n",
       "      <td>0.006234</td>\n",
       "      <td>5799</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>MLP-relu-0.5-(16, 16)</td>\n",
       "      <td>0.073786</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.198439</td>\n",
       "      <td>0.014372</td>\n",
       "      <td>0.025747</td>\n",
       "      <td>0.009169</td>\n",
       "      <td>3175</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>MLP-relu-0.5-(10, 5, 5)</td>\n",
       "      <td>0.089531</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>0.210050</td>\n",
       "      <td>0.017537</td>\n",
       "      <td>0.022478</td>\n",
       "      <td>0.006589</td>\n",
       "      <td>1867</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>MLP-relu-0.5-(16, 8, 8)</td>\n",
       "      <td>0.086301</td>\n",
       "      <td>0.004569</td>\n",
       "      <td>0.204753</td>\n",
       "      <td>0.007796</td>\n",
       "      <td>0.023070</td>\n",
       "      <td>0.006416</td>\n",
       "      <td>3055</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>MLP-relu-0.5-(64, 32, 32)</td>\n",
       "      <td>0.045744</td>\n",
       "      <td>0.002760</td>\n",
       "      <td>0.209379</td>\n",
       "      <td>0.020704</td>\n",
       "      <td>0.029601</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>14503</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>MLP-sigmoid-0.0001-(5,)</td>\n",
       "      <td>0.664158</td>\n",
       "      <td>0.023146</td>\n",
       "      <td>440.735970</td>\n",
       "      <td>12.864529</td>\n",
       "      <td>0.016419</td>\n",
       "      <td>0.002417</td>\n",
       "      <td>912</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>450.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>MLP-sigmoid-0.0001-(32,)</td>\n",
       "      <td>0.624561</td>\n",
       "      <td>0.019937</td>\n",
       "      <td>584.562932</td>\n",
       "      <td>28.495352</td>\n",
       "      <td>0.020113</td>\n",
       "      <td>0.005377</td>\n",
       "      <td>5799</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>419.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>MLP-sigmoid-0.0001-(16, 16)</td>\n",
       "      <td>0.670051</td>\n",
       "      <td>0.028579</td>\n",
       "      <td>657.859110</td>\n",
       "      <td>37.299178</td>\n",
       "      <td>0.018646</td>\n",
       "      <td>0.004392</td>\n",
       "      <td>3175</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>451.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>MLP-sigmoid-0.0001-(10, 5, 5)</td>\n",
       "      <td>0.105976</td>\n",
       "      <td>0.004382</td>\n",
       "      <td>29.592399</td>\n",
       "      <td>3.600011</td>\n",
       "      <td>0.014775</td>\n",
       "      <td>0.001648</td>\n",
       "      <td>1867</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>MLP-sigmoid-0.0001-(16, 8, 8)</td>\n",
       "      <td>0.139668</td>\n",
       "      <td>0.002904</td>\n",
       "      <td>28.331946</td>\n",
       "      <td>1.617752</td>\n",
       "      <td>0.015339</td>\n",
       "      <td>0.004689</td>\n",
       "      <td>3055</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>MLP-sigmoid-0.0001-(64, 32, 32)</td>\n",
       "      <td>0.742338</td>\n",
       "      <td>0.021881</td>\n",
       "      <td>1236.012081</td>\n",
       "      <td>20.893088</td>\n",
       "      <td>0.019591</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>14503</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>449.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>MLP-sigmoid-0.001-(5,)</td>\n",
       "      <td>0.684587</td>\n",
       "      <td>0.025795</td>\n",
       "      <td>144.577555</td>\n",
       "      <td>10.657759</td>\n",
       "      <td>0.015434</td>\n",
       "      <td>0.003355</td>\n",
       "      <td>912</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>140.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>MLP-sigmoid-0.001-(32,)</td>\n",
       "      <td>0.617926</td>\n",
       "      <td>0.011712</td>\n",
       "      <td>165.608033</td>\n",
       "      <td>9.164667</td>\n",
       "      <td>0.014667</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>5799</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>130.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>MLP-sigmoid-0.001-(16, 16)</td>\n",
       "      <td>0.654194</td>\n",
       "      <td>0.021783</td>\n",
       "      <td>151.893695</td>\n",
       "      <td>15.602767</td>\n",
       "      <td>0.017306</td>\n",
       "      <td>0.003098</td>\n",
       "      <td>3175</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>96.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>MLP-sigmoid-0.001-(10, 5, 5)</td>\n",
       "      <td>0.667011</td>\n",
       "      <td>0.018989</td>\n",
       "      <td>241.438698</td>\n",
       "      <td>14.732556</td>\n",
       "      <td>0.017306</td>\n",
       "      <td>0.004458</td>\n",
       "      <td>1867</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>136.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>MLP-sigmoid-0.001-(16, 8, 8)</td>\n",
       "      <td>0.692447</td>\n",
       "      <td>0.018932</td>\n",
       "      <td>242.074241</td>\n",
       "      <td>10.673677</td>\n",
       "      <td>0.017482</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>3055</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>119.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>MLP-sigmoid-0.001-(64, 32, 32)</td>\n",
       "      <td>0.741701</td>\n",
       "      <td>0.022078</td>\n",
       "      <td>229.434758</td>\n",
       "      <td>7.056314</td>\n",
       "      <td>0.023255</td>\n",
       "      <td>0.006144</td>\n",
       "      <td>14503</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>81.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>MLP-sigmoid-0.01-(5,)</td>\n",
       "      <td>0.656993</td>\n",
       "      <td>0.020658</td>\n",
       "      <td>41.154121</td>\n",
       "      <td>7.379066</td>\n",
       "      <td>0.014924</td>\n",
       "      <td>0.002388</td>\n",
       "      <td>912</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>MLP-sigmoid-0.01-(32,)</td>\n",
       "      <td>0.612462</td>\n",
       "      <td>0.017186</td>\n",
       "      <td>46.870801</td>\n",
       "      <td>5.088577</td>\n",
       "      <td>0.015754</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>5799</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>34.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>MLP-sigmoid-0.01-(16, 16)</td>\n",
       "      <td>0.620231</td>\n",
       "      <td>0.029321</td>\n",
       "      <td>32.883365</td>\n",
       "      <td>1.995959</td>\n",
       "      <td>0.019834</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>3175</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>MLP-sigmoid-0.01-(10, 5, 5)</td>\n",
       "      <td>0.623470</td>\n",
       "      <td>0.014523</td>\n",
       "      <td>51.430097</td>\n",
       "      <td>6.486761</td>\n",
       "      <td>0.019545</td>\n",
       "      <td>0.005087</td>\n",
       "      <td>1867</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>MLP-sigmoid-0.01-(16, 8, 8)</td>\n",
       "      <td>0.644529</td>\n",
       "      <td>0.036823</td>\n",
       "      <td>48.245863</td>\n",
       "      <td>7.779490</td>\n",
       "      <td>0.016937</td>\n",
       "      <td>0.002109</td>\n",
       "      <td>3055</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>26.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>MLP-sigmoid-0.01-(64, 32, 32)</td>\n",
       "      <td>0.715269</td>\n",
       "      <td>0.012124</td>\n",
       "      <td>57.154692</td>\n",
       "      <td>8.047412</td>\n",
       "      <td>0.019277</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>14503</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>20.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>MLP-sigmoid-0.1-(5,)</td>\n",
       "      <td>0.465588</td>\n",
       "      <td>0.029604</td>\n",
       "      <td>42.865201</td>\n",
       "      <td>11.277539</td>\n",
       "      <td>0.017712</td>\n",
       "      <td>0.004390</td>\n",
       "      <td>912</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>40.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>MLP-sigmoid-0.1-(32,)</td>\n",
       "      <td>0.551548</td>\n",
       "      <td>0.014993</td>\n",
       "      <td>35.121342</td>\n",
       "      <td>6.353176</td>\n",
       "      <td>0.017669</td>\n",
       "      <td>0.004548</td>\n",
       "      <td>5799</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>MLP-sigmoid-0.1-(16, 16)</td>\n",
       "      <td>0.511627</td>\n",
       "      <td>0.035255</td>\n",
       "      <td>53.483966</td>\n",
       "      <td>24.132931</td>\n",
       "      <td>0.020480</td>\n",
       "      <td>0.005516</td>\n",
       "      <td>3175</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>35.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>MLP-sigmoid-0.1-(10, 5, 5)</td>\n",
       "      <td>0.490456</td>\n",
       "      <td>0.023308</td>\n",
       "      <td>51.369136</td>\n",
       "      <td>6.373951</td>\n",
       "      <td>0.015095</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>1867</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>MLP-sigmoid-0.1-(16, 8, 8)</td>\n",
       "      <td>0.475248</td>\n",
       "      <td>0.042141</td>\n",
       "      <td>42.663736</td>\n",
       "      <td>10.462118</td>\n",
       "      <td>0.015105</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>3055</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>MLP-sigmoid-0.1-(64, 32, 32)</td>\n",
       "      <td>0.063904</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>29.394690</td>\n",
       "      <td>1.988913</td>\n",
       "      <td>0.017921</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>14503</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>MLP-sigmoid-0.5-(5,)</td>\n",
       "      <td>0.307315</td>\n",
       "      <td>0.047243</td>\n",
       "      <td>20.809324</td>\n",
       "      <td>3.228433</td>\n",
       "      <td>0.018158</td>\n",
       "      <td>0.005211</td>\n",
       "      <td>912</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>19.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>MLP-sigmoid-0.5-(32,)</td>\n",
       "      <td>0.396263</td>\n",
       "      <td>0.021201</td>\n",
       "      <td>47.649472</td>\n",
       "      <td>16.728562</td>\n",
       "      <td>0.019209</td>\n",
       "      <td>0.008420</td>\n",
       "      <td>5799</td>\n",
       "      <td>(32,)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>34.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>MLP-sigmoid-0.5-(16, 16)</td>\n",
       "      <td>0.098929</td>\n",
       "      <td>0.041596</td>\n",
       "      <td>34.703192</td>\n",
       "      <td>8.168717</td>\n",
       "      <td>0.020181</td>\n",
       "      <td>0.007853</td>\n",
       "      <td>3175</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>18.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>MLP-sigmoid-0.5-(10, 5, 5)</td>\n",
       "      <td>0.063904</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>19.741083</td>\n",
       "      <td>2.833805</td>\n",
       "      <td>0.014228</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>1867</td>\n",
       "      <td>(10, 5, 5)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>MLP-sigmoid-0.5-(16, 8, 8)</td>\n",
       "      <td>0.063904</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>21.181497</td>\n",
       "      <td>1.437725</td>\n",
       "      <td>0.018468</td>\n",
       "      <td>0.005024</td>\n",
       "      <td>3055</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>MLP-sigmoid-0.5-(64, 32, 32)</td>\n",
       "      <td>0.063904</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>80.067443</td>\n",
       "      <td>94.983660</td>\n",
       "      <td>0.026701</td>\n",
       "      <td>0.007478</td>\n",
       "      <td>14503</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>36.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              model  test_score_mean  test_score_std  \\\n",
       "0              MLP-relu-0.0001-(5,)         0.686962        0.016567   \n",
       "1             MLP-relu-0.0001-(32,)         0.637483        0.015374   \n",
       "2          MLP-relu-0.0001-(16, 16)         0.673423        0.015513   \n",
       "3        MLP-relu-0.0001-(10, 5, 5)         0.578541        0.241584   \n",
       "4        MLP-relu-0.0001-(16, 8, 8)         0.539063        0.229464   \n",
       "5      MLP-relu-0.0001-(64, 32, 32)         0.045744        0.002760   \n",
       "6               MLP-relu-0.001-(5,)         0.694258        0.028037   \n",
       "7              MLP-relu-0.001-(32,)         0.629580        0.006847   \n",
       "8           MLP-relu-0.001-(16, 16)         0.073786        0.002305   \n",
       "9         MLP-relu-0.001-(10, 5, 5)         0.089531        0.003964   \n",
       "10        MLP-relu-0.001-(16, 8, 8)         0.086301        0.004569   \n",
       "11      MLP-relu-0.001-(64, 32, 32)         0.045744        0.002760   \n",
       "12               MLP-relu-0.01-(5,)         0.062027        0.003398   \n",
       "13              MLP-relu-0.01-(32,)         0.056851        0.003322   \n",
       "14           MLP-relu-0.01-(16, 16)         0.073786        0.002305   \n",
       "15         MLP-relu-0.01-(10, 5, 5)         0.089531        0.003964   \n",
       "16         MLP-relu-0.01-(16, 8, 8)         0.086301        0.004569   \n",
       "17       MLP-relu-0.01-(64, 32, 32)         0.045744        0.002760   \n",
       "18                MLP-relu-0.1-(5,)         0.062027        0.003398   \n",
       "19               MLP-relu-0.1-(32,)         0.056851        0.003322   \n",
       "20            MLP-relu-0.1-(16, 16)         0.073786        0.002305   \n",
       "21          MLP-relu-0.1-(10, 5, 5)         0.089531        0.003964   \n",
       "22          MLP-relu-0.1-(16, 8, 8)         0.086301        0.004569   \n",
       "23        MLP-relu-0.1-(64, 32, 32)         0.045744        0.002760   \n",
       "24                MLP-relu-0.5-(5,)         0.062027        0.003398   \n",
       "25               MLP-relu-0.5-(32,)         0.056851        0.003322   \n",
       "26            MLP-relu-0.5-(16, 16)         0.073786        0.002305   \n",
       "27          MLP-relu-0.5-(10, 5, 5)         0.089531        0.003964   \n",
       "28          MLP-relu-0.5-(16, 8, 8)         0.086301        0.004569   \n",
       "29        MLP-relu-0.5-(64, 32, 32)         0.045744        0.002760   \n",
       "30          MLP-sigmoid-0.0001-(5,)         0.664158        0.023146   \n",
       "31         MLP-sigmoid-0.0001-(32,)         0.624561        0.019937   \n",
       "32      MLP-sigmoid-0.0001-(16, 16)         0.670051        0.028579   \n",
       "33    MLP-sigmoid-0.0001-(10, 5, 5)         0.105976        0.004382   \n",
       "34    MLP-sigmoid-0.0001-(16, 8, 8)         0.139668        0.002904   \n",
       "35  MLP-sigmoid-0.0001-(64, 32, 32)         0.742338        0.021881   \n",
       "36           MLP-sigmoid-0.001-(5,)         0.684587        0.025795   \n",
       "37          MLP-sigmoid-0.001-(32,)         0.617926        0.011712   \n",
       "38       MLP-sigmoid-0.001-(16, 16)         0.654194        0.021783   \n",
       "39     MLP-sigmoid-0.001-(10, 5, 5)         0.667011        0.018989   \n",
       "40     MLP-sigmoid-0.001-(16, 8, 8)         0.692447        0.018932   \n",
       "41   MLP-sigmoid-0.001-(64, 32, 32)         0.741701        0.022078   \n",
       "42            MLP-sigmoid-0.01-(5,)         0.656993        0.020658   \n",
       "43           MLP-sigmoid-0.01-(32,)         0.612462        0.017186   \n",
       "44        MLP-sigmoid-0.01-(16, 16)         0.620231        0.029321   \n",
       "45      MLP-sigmoid-0.01-(10, 5, 5)         0.623470        0.014523   \n",
       "46      MLP-sigmoid-0.01-(16, 8, 8)         0.644529        0.036823   \n",
       "47    MLP-sigmoid-0.01-(64, 32, 32)         0.715269        0.012124   \n",
       "48             MLP-sigmoid-0.1-(5,)         0.465588        0.029604   \n",
       "49            MLP-sigmoid-0.1-(32,)         0.551548        0.014993   \n",
       "50         MLP-sigmoid-0.1-(16, 16)         0.511627        0.035255   \n",
       "51       MLP-sigmoid-0.1-(10, 5, 5)         0.490456        0.023308   \n",
       "52       MLP-sigmoid-0.1-(16, 8, 8)         0.475248        0.042141   \n",
       "53     MLP-sigmoid-0.1-(64, 32, 32)         0.063904        0.000034   \n",
       "54             MLP-sigmoid-0.5-(5,)         0.307315        0.047243   \n",
       "55            MLP-sigmoid-0.5-(32,)         0.396263        0.021201   \n",
       "56         MLP-sigmoid-0.5-(16, 16)         0.098929        0.041596   \n",
       "57       MLP-sigmoid-0.5-(10, 5, 5)         0.063904        0.000034   \n",
       "58       MLP-sigmoid-0.5-(16, 8, 8)         0.063904        0.000034   \n",
       "59     MLP-sigmoid-0.5-(64, 32, 32)         0.063904        0.000034   \n",
       "\n",
       "    fit_time_mean  fit_time_std  score_time_mean  score_time_std  \\\n",
       "0      195.440456     10.348554         0.012875        0.000351   \n",
       "1      227.997793      8.737823         0.018614        0.002242   \n",
       "2      220.494422     16.713127         0.016936        0.005812   \n",
       "3      177.741284     10.017526         0.022197        0.010148   \n",
       "4      137.542779      9.866459         0.012556        0.000707   \n",
       "5        0.970717      0.513438         0.016173        0.001392   \n",
       "6       74.046086      4.702816         0.012439        0.000797   \n",
       "7       47.405020      3.317254         0.015678        0.003852   \n",
       "8        2.571200      0.444815         0.015335        0.003307   \n",
       "9        1.779617      0.256817         0.014233        0.001285   \n",
       "10       0.893262      0.090265         0.015010        0.000975   \n",
       "11       0.222940      0.017618         0.024681        0.004612   \n",
       "12       0.819374      0.112048         0.016045        0.003327   \n",
       "13       0.622187      0.058410         0.018271        0.003483   \n",
       "14       0.242600      0.018956         0.022185        0.003096   \n",
       "15       0.255646      0.036662         0.024954        0.007460   \n",
       "16       0.203969      0.018059         0.021377        0.002086   \n",
       "17       0.214954      0.010826         0.025513        0.007809   \n",
       "18       0.196002      0.014887         0.020382        0.004880   \n",
       "19       0.200418      0.011752         0.025898        0.005824   \n",
       "20       0.200258      0.011009         0.025368        0.005182   \n",
       "21       0.217665      0.014051         0.025950        0.005574   \n",
       "22       0.202530      0.012900         0.024914        0.004484   \n",
       "23       0.217253      0.025780         0.032837        0.005794   \n",
       "24       0.202215      0.017748         0.026379        0.005975   \n",
       "25       0.204371      0.017975         0.023531        0.006234   \n",
       "26       0.198439      0.014372         0.025747        0.009169   \n",
       "27       0.210050      0.017537         0.022478        0.006589   \n",
       "28       0.204753      0.007796         0.023070        0.006416   \n",
       "29       0.209379      0.020704         0.029601        0.008250   \n",
       "30     440.735970     12.864529         0.016419        0.002417   \n",
       "31     584.562932     28.495352         0.020113        0.005377   \n",
       "32     657.859110     37.299178         0.018646        0.004392   \n",
       "33      29.592399      3.600011         0.014775        0.001648   \n",
       "34      28.331946      1.617752         0.015339        0.004689   \n",
       "35    1236.012081     20.893088         0.019591        0.003391   \n",
       "36     144.577555     10.657759         0.015434        0.003355   \n",
       "37     165.608033      9.164667         0.014667        0.001685   \n",
       "38     151.893695     15.602767         0.017306        0.003098   \n",
       "39     241.438698     14.732556         0.017306        0.004458   \n",
       "40     242.074241     10.673677         0.017482        0.002108   \n",
       "41     229.434758      7.056314         0.023255        0.006144   \n",
       "42      41.154121      7.379066         0.014924        0.002388   \n",
       "43      46.870801      5.088577         0.015754        0.001379   \n",
       "44      32.883365      1.995959         0.019834        0.003064   \n",
       "45      51.430097      6.486761         0.019545        0.005087   \n",
       "46      48.245863      7.779490         0.016937        0.002109   \n",
       "47      57.154692      8.047412         0.019277        0.001969   \n",
       "48      42.865201     11.277539         0.017712        0.004390   \n",
       "49      35.121342      6.353176         0.017669        0.004548   \n",
       "50      53.483966     24.132931         0.020480        0.005516   \n",
       "51      51.369136      6.373951         0.015095        0.001277   \n",
       "52      42.663736     10.462118         0.015105        0.001057   \n",
       "53      29.394690      1.988913         0.017921        0.001451   \n",
       "54      20.809324      3.228433         0.018158        0.005211   \n",
       "55      47.649472     16.728562         0.019209        0.008420   \n",
       "56      34.703192      8.168717         0.020181        0.007853   \n",
       "57      19.741083      2.833805         0.014228        0.001078   \n",
       "58      21.181497      1.437725         0.018468        0.005024   \n",
       "59      80.067443     94.983660         0.026701        0.007478   \n",
       "\n",
       "    parameter_num hidden_layer_sizes activation_function  learning_rate  \\\n",
       "0             912               (5,)                relu         0.0001   \n",
       "1            5799              (32,)                relu         0.0001   \n",
       "2            3175           (16, 16)                relu         0.0001   \n",
       "3            1867         (10, 5, 5)                relu         0.0001   \n",
       "4            3055         (16, 8, 8)                relu         0.0001   \n",
       "5           14503       (64, 32, 32)                relu         0.0001   \n",
       "6             912               (5,)                relu         0.0010   \n",
       "7            5799              (32,)                relu         0.0010   \n",
       "8            3175           (16, 16)                relu         0.0010   \n",
       "9            1867         (10, 5, 5)                relu         0.0010   \n",
       "10           3055         (16, 8, 8)                relu         0.0010   \n",
       "11          14503       (64, 32, 32)                relu         0.0010   \n",
       "12            912               (5,)                relu         0.0100   \n",
       "13           5799              (32,)                relu         0.0100   \n",
       "14           3175           (16, 16)                relu         0.0100   \n",
       "15           1867         (10, 5, 5)                relu         0.0100   \n",
       "16           3055         (16, 8, 8)                relu         0.0100   \n",
       "17          14503       (64, 32, 32)                relu         0.0100   \n",
       "18            912               (5,)                relu         0.1000   \n",
       "19           5799              (32,)                relu         0.1000   \n",
       "20           3175           (16, 16)                relu         0.1000   \n",
       "21           1867         (10, 5, 5)                relu         0.1000   \n",
       "22           3055         (16, 8, 8)                relu         0.1000   \n",
       "23          14503       (64, 32, 32)                relu         0.1000   \n",
       "24            912               (5,)                relu         0.5000   \n",
       "25           5799              (32,)                relu         0.5000   \n",
       "26           3175           (16, 16)                relu         0.5000   \n",
       "27           1867         (10, 5, 5)                relu         0.5000   \n",
       "28           3055         (16, 8, 8)                relu         0.5000   \n",
       "29          14503       (64, 32, 32)                relu         0.5000   \n",
       "30            912               (5,)             sigmoid         0.0001   \n",
       "31           5799              (32,)             sigmoid         0.0001   \n",
       "32           3175           (16, 16)             sigmoid         0.0001   \n",
       "33           1867         (10, 5, 5)             sigmoid         0.0001   \n",
       "34           3055         (16, 8, 8)             sigmoid         0.0001   \n",
       "35          14503       (64, 32, 32)             sigmoid         0.0001   \n",
       "36            912               (5,)             sigmoid         0.0010   \n",
       "37           5799              (32,)             sigmoid         0.0010   \n",
       "38           3175           (16, 16)             sigmoid         0.0010   \n",
       "39           1867         (10, 5, 5)             sigmoid         0.0010   \n",
       "40           3055         (16, 8, 8)             sigmoid         0.0010   \n",
       "41          14503       (64, 32, 32)             sigmoid         0.0010   \n",
       "42            912               (5,)             sigmoid         0.0100   \n",
       "43           5799              (32,)             sigmoid         0.0100   \n",
       "44           3175           (16, 16)             sigmoid         0.0100   \n",
       "45           1867         (10, 5, 5)             sigmoid         0.0100   \n",
       "46           3055         (16, 8, 8)             sigmoid         0.0100   \n",
       "47          14503       (64, 32, 32)             sigmoid         0.0100   \n",
       "48            912               (5,)             sigmoid         0.1000   \n",
       "49           5799              (32,)             sigmoid         0.1000   \n",
       "50           3175           (16, 16)             sigmoid         0.1000   \n",
       "51           1867         (10, 5, 5)             sigmoid         0.1000   \n",
       "52           3055         (16, 8, 8)             sigmoid         0.1000   \n",
       "53          14503       (64, 32, 32)             sigmoid         0.1000   \n",
       "54            912               (5,)             sigmoid         0.5000   \n",
       "55           5799              (32,)             sigmoid         0.5000   \n",
       "56           3175           (16, 16)             sigmoid         0.5000   \n",
       "57           1867         (10, 5, 5)             sigmoid         0.5000   \n",
       "58           3055         (16, 8, 8)             sigmoid         0.5000   \n",
       "59          14503       (64, 32, 32)             sigmoid         0.5000   \n",
       "\n",
       "    num_iter  \n",
       "0      217.0  \n",
       "1      190.4  \n",
       "2      140.0  \n",
       "3       85.4  \n",
       "4       90.2  \n",
       "5        0.0  \n",
       "6       87.0  \n",
       "7       43.0  \n",
       "8        1.2  \n",
       "9        0.8  \n",
       "10       0.0  \n",
       "11       0.0  \n",
       "12       0.0  \n",
       "13       0.0  \n",
       "14       0.0  \n",
       "15       0.0  \n",
       "16       0.0  \n",
       "17       0.0  \n",
       "18       0.0  \n",
       "19       0.0  \n",
       "20       0.0  \n",
       "21       0.0  \n",
       "22       0.0  \n",
       "23       0.0  \n",
       "24       0.0  \n",
       "25       0.0  \n",
       "26       0.0  \n",
       "27       0.0  \n",
       "28       0.0  \n",
       "29       0.0  \n",
       "30     450.8  \n",
       "31     419.4  \n",
       "32     451.2  \n",
       "33      16.8  \n",
       "34      15.0  \n",
       "35     449.0  \n",
       "36     140.4  \n",
       "37     130.6  \n",
       "38      96.4  \n",
       "39     136.2  \n",
       "40     119.8  \n",
       "41      81.2  \n",
       "42      43.0  \n",
       "43      34.2  \n",
       "44      20.6  \n",
       "45      29.0  \n",
       "46      26.6  \n",
       "47      20.8  \n",
       "48      40.8  \n",
       "49      27.0  \n",
       "50      35.8  \n",
       "51      26.8  \n",
       "52      24.0  \n",
       "53      11.0  \n",
       "54      19.4  \n",
       "55      34.2  \n",
       "56      18.8  \n",
       "57      11.0  \n",
       "58      11.0  \n",
       "59      36.4  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>test_score_mean</th>\n",
       "      <th>test_score_std</th>\n",
       "      <th>fit_time_mean</th>\n",
       "      <th>fit_time_std</th>\n",
       "      <th>score_time_mean</th>\n",
       "      <th>score_time_std</th>\n",
       "      <th>parameter_num</th>\n",
       "      <th>hidden_layer_sizes</th>\n",
       "      <th>activation_function</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>num_iter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>MLP-sigmoid-0.0001-(64, 32, 32)</td>\n",
       "      <td>0.742338</td>\n",
       "      <td>0.021881</td>\n",
       "      <td>1236.012081</td>\n",
       "      <td>20.893088</td>\n",
       "      <td>0.019591</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>14503</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>449.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>MLP-sigmoid-0.001-(64, 32, 32)</td>\n",
       "      <td>0.741701</td>\n",
       "      <td>0.022078</td>\n",
       "      <td>229.434758</td>\n",
       "      <td>7.056314</td>\n",
       "      <td>0.023255</td>\n",
       "      <td>0.006144</td>\n",
       "      <td>14503</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>81.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>MLP-sigmoid-0.01-(64, 32, 32)</td>\n",
       "      <td>0.715269</td>\n",
       "      <td>0.012124</td>\n",
       "      <td>57.154692</td>\n",
       "      <td>8.047412</td>\n",
       "      <td>0.019277</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>14503</td>\n",
       "      <td>(64, 32, 32)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>20.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLP-relu-0.001-(5,)</td>\n",
       "      <td>0.694258</td>\n",
       "      <td>0.028037</td>\n",
       "      <td>74.046086</td>\n",
       "      <td>4.702816</td>\n",
       "      <td>0.012439</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>912</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>MLP-sigmoid-0.001-(16, 8, 8)</td>\n",
       "      <td>0.692447</td>\n",
       "      <td>0.018932</td>\n",
       "      <td>242.074241</td>\n",
       "      <td>10.673677</td>\n",
       "      <td>0.017482</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>3055</td>\n",
       "      <td>(16, 8, 8)</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>119.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              model  test_score_mean  test_score_std  \\\n",
       "35  MLP-sigmoid-0.0001-(64, 32, 32)         0.742338        0.021881   \n",
       "41   MLP-sigmoid-0.001-(64, 32, 32)         0.741701        0.022078   \n",
       "47    MLP-sigmoid-0.01-(64, 32, 32)         0.715269        0.012124   \n",
       "6               MLP-relu-0.001-(5,)         0.694258        0.028037   \n",
       "40     MLP-sigmoid-0.001-(16, 8, 8)         0.692447        0.018932   \n",
       "\n",
       "    fit_time_mean  fit_time_std  score_time_mean  score_time_std  \\\n",
       "35    1236.012081     20.893088         0.019591        0.003391   \n",
       "41     229.434758      7.056314         0.023255        0.006144   \n",
       "47      57.154692      8.047412         0.019277        0.001969   \n",
       "6       74.046086      4.702816         0.012439        0.000797   \n",
       "40     242.074241     10.673677         0.017482        0.002108   \n",
       "\n",
       "    parameter_num hidden_layer_sizes activation_function  learning_rate  \\\n",
       "35          14503       (64, 32, 32)             sigmoid         0.0001   \n",
       "41          14503       (64, 32, 32)             sigmoid         0.0010   \n",
       "47          14503       (64, 32, 32)             sigmoid         0.0100   \n",
       "6             912               (5,)                relu         0.0010   \n",
       "40           3055         (16, 8, 8)             sigmoid         0.0010   \n",
       "\n",
       "    num_iter  \n",
       "35     449.0  \n",
       "41      81.2  \n",
       "47      20.8  \n",
       "6       87.0  \n",
       "40     119.8  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_param.sort_values(['test_score_mean'], ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 172)\n",
      "MLP-sigmoid-0.0001-(64, 32, 32)\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 431...\n",
      "Evaluation metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9532967 , 0.0467033 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.078125  , 0.86458333, 0.05729167, 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.00167224, 0.04347826, 0.89464883, 0.06020067, 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.05172414, 0.84827586, 0.1       ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.2016129 , 0.68548387,\n",
       "        0.11290323, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.43243243,\n",
       "        0.45945946, 0.10810811],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.72727273, 0.27272727]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.8655\n",
      "balanced_acc: 0.7112107616121307\n",
      "\n",
      "Macro-averaged precision: 0.7201077717644317\n",
      "Macro-averaged recall: 0.7112107616121307\n",
      "Macro-averaged f-score: 0.7126537754001301\n",
      "Macro-averaged support: None\n"
     ]
    }
   ],
   "source": [
    "modelsss = df_param.sort_values(['test_score_mean'], ascending=False).head(5)['model']\n",
    "print(X_tr.shape)\n",
    "results_df2 = pd.DataFrame()\n",
    "for mod in modelsss[0:1]:\n",
    "    print(mod)\n",
    "    pipelines[mod].fit(X_tr, y_tr)\n",
    "    y_pred = pipelines[mod].predict(X_te)\n",
    "    eval_results = cu.eval(y_pred, y_te)\n",
    "    results_df2 = results_df2.append({\n",
    "        'Model': mod,\n",
    "        'cm': eval_results['cm'].flatten(),\n",
    "        'acc': eval_results['acc'],\n",
    "        'balanced_acc': eval_results['balanced_acc'],\n",
    "        'precision': eval_results['precision'],\n",
    "        'recall': eval_results['recall'],\n",
    "        'f-score': eval_results['f-score'],\n",
    "        'support': eval_results['support']\n",
    "    }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df2.to_csv(\"results/loan_trainset_big_model.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>cm</th>\n",
       "      <th>acc</th>\n",
       "      <th>balanced_acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP-sigmoid-0.0001-(64, 32, 32)</td>\n",
       "      <td>[0.9532967032967034, 0.046703296703296704, 0.0...</td>\n",
       "      <td>0.8655</td>\n",
       "      <td>0.711211</td>\n",
       "      <td>0.720108</td>\n",
       "      <td>0.711211</td>\n",
       "      <td>0.712654</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Model  \\\n",
       "0  MLP-sigmoid-0.0001-(64, 32, 32)   \n",
       "\n",
       "                                                  cm     acc  balanced_acc  \\\n",
       "0  [0.9532967032967034, 0.046703296703296704, 0.0...  0.8655      0.711211   \n",
       "\n",
       "   precision    recall   f-score support  \n",
       "0   0.720108  0.711211  0.712654    None  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab3b8a1ad6a92cd8d78d8440c121a2929fc9ca6cecf54118f3d81aa45c5df661"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
