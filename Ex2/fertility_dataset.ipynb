{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "\n",
    "from utils import data_preprocessing_util as dpu\n",
    "from utils import classification_util as cu\n",
    "\n",
    "from MLP import MLP\n",
    "from nn_framework import NNFramework\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fertility Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'../Ex1/data/Fertility/fertility_diagnosis.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "df = dpu.preprocess_fertility_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NNFramework()\n",
    "nn.fit_encoder(df=df, cols_to_encode=df.columns.difference(['age', 'hours_sitting']))\n",
    "df_encoded = nn.encode_dataset(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1038\n",
    "scaling = True\n",
    "oversampling = True\n",
    "\n",
    "scaler = preprocessing.StandardScaler() if scaling else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_encoded['diagnosis']\n",
    "X = df_encoded[df_encoded.columns.difference(['diagnosis'])]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = random_seed, shuffle=True, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = ['relu', 'sigmoid']\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 0.5]\n",
    "hidden_layer_sizes = [(5,), (32,), (16, 16), (10, 5, 5), (16, 8, 8), (64, 32, 32),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = []\n",
    "\n",
    "for af in activation_functions:\n",
    "    for lr in learning_rates:\n",
    "        for hls in hidden_layer_sizes:\n",
    "            methods.append((f'MLP-{af}-{lr}-{hls}', MLP(n_iter=5000, activation_function=af, learning_rate=lr, hidden_layer_sizes=hls)))\n",
    "    \n",
    "pipelines = cu.define_pipelines(methods, scaler=scaler, oversampling=oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 75...\n",
      "MLP-relu-0.0001-(5,)\n",
      "f1 scores: [0.375      0.2        0.30666667 0.46666667 0.56709957]\n",
      "f1 mean: 0.383\n",
      "f1 std: 0.127\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 30...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 45...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 30...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 519...\n",
      "MLP-relu-0.0001-(32,)\n",
      "f1 scores: [0.31034483 0.33333333 0.41333333 0.41333333 0.375     ]\n",
      "f1 mean: 0.369\n",
      "f1 std: 0.042\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 36...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 36...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 49...\n",
      "MLP-relu-0.0001-(16, 16)\n",
      "f1 scores: [0.23273657 0.29292929 0.41333333 0.14786967 0.48717949]\n",
      "f1 mean: 0.315\n",
      "f1 std: 0.122\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 23...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 22...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 52...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 54...\n",
      "MLP-relu-0.0001-(10, 5, 5)\n",
      "f1 scores: [0.375      0.28571429 0.37321937 0.6875     0.45945946]\n",
      "f1 mean: 0.436\n",
      "f1 std: 0.137\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 22...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 42...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 39...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 36...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 40...\n",
      "MLP-relu-0.0001-(16, 8, 8)\n",
      "f1 scores: [0.375      0.28571429 0.45054945 0.37321937 0.41176471]\n",
      "f1 mean: 0.379\n",
      "f1 std: 0.055\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "MLP-relu-0.0001-(64, 32, 32)\n",
      "f1 scores: [0.2        0.29292929 0.09090909 0.42857143 0.44444444]\n",
      "f1 mean: 0.291\n",
      "f1 std: 0.135\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 66...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 158...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 217...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 222...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 234...\n",
      "MLP-relu-0.001-(5,)\n",
      "f1 scores: [0.25925926 0.64157706 0.53125    0.46666667 0.42857143]\n",
      "f1 mean: 0.465\n",
      "f1 std: 0.126\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 226...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 213...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 234...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 264...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 273...\n",
      "MLP-relu-0.001-(32,)\n",
      "f1 scores: [0.375      0.56709957 0.60784314 0.42857143 0.60784314]\n",
      "f1 mean: 0.517\n",
      "f1 std: 0.097\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 148...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 171...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 228...\n",
      "MLP-relu-0.001-(16, 16)\n",
      "f1 scores: [0.39393939 0.56709957 0.56709957 0.28571429 0.60784314]\n",
      "f1 mean: 0.484\n",
      "f1 std: 0.124\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 11...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 11...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 15...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 199...\n",
      "MLP-relu-0.001-(10, 5, 5)\n",
      "f1 scores: [0.56709957 0.28571429 0.43573668 0.56112853 0.42857143]\n",
      "f1 mean: 0.456\n",
      "f1 std: 0.103\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 12...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 12...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 18...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 12...\n",
      "MLP-relu-0.001-(16, 8, 8)\n",
      "f1 scores: [0.43573668 0.33333333 0.52380952 0.34065934 0.375     ]\n",
      "f1 mean: 0.402\n",
      "f1 std: 0.071\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.001-(64, 32, 32)\n",
      "f1 scores: [0.09090909 0.09090909 0.47368421 0.42857143 0.13043478]\n",
      "f1 mean: 0.243\n",
      "f1 std: 0.171\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 33...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 34...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 47...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 80...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 81...\n",
      "MLP-relu-0.01-(5,)\n",
      "f1 scores: [0.33333333 0.64157706 0.53125    0.56709957 0.42857143]\n",
      "f1 mean: 0.500\n",
      "f1 std: 0.108\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 46...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 87...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 56...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 62...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 75...\n",
      "MLP-relu-0.01-(32,)\n",
      "f1 scores: [0.375      0.60784314 0.60784314 0.42857143 0.60784314]\n",
      "f1 mean: 0.525\n",
      "f1 std: 0.102\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 23...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 25...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 29...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 44...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 79...\n",
      "MLP-relu-0.01-(16, 16)\n",
      "f1 scores: [0.39393939 0.56709957 0.35483871 0.42857143 0.42857143]\n",
      "f1 mean: 0.435\n",
      "f1 std: 0.072\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 13...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 41...\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 66...\n",
      "MLP-relu-0.01-(10, 5, 5)\n",
      "f1 scores: [0.29292929 0.6        0.6        0.44444444 0.44444444]\n",
      "f1 mean: 0.476\n",
      "f1 std: 0.115\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 51...\n",
      "MLP-relu-0.01-(16, 8, 8)\n",
      "f1 scores: [0.14786967 0.14786967 0.14786967 0.2481203  0.44444444]\n",
      "f1 mean: 0.227\n",
      "f1 std: 0.115\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.01-(64, 32, 32)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.13043478 0.13043478]\n",
      "f1 mean: 0.336\n",
      "f1 std: 0.168\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:189: RuntimeWarning: divide by zero encountered in log\n",
      "  loss -= (idx == y)*np.log(y_prob)\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:189: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss -= (idx == y)*np.log(y_prob)\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 21...\n",
      "MLP-relu-0.1-(5,)\n",
      "f1 scores: [0.375      0.52380952 0.2        0.41176471 0.60784314]\n",
      "f1 mean: 0.424\n",
      "f1 std: 0.139\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(32,)\n",
      "f1 scores: [0.375      0.6        0.48717949 0.65714286 0.37321937]\n",
      "f1 mean: 0.499\n",
      "f1 std: 0.115\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(16, 16)\n",
      "f1 scores: [0.47368421 0.41176471 0.45945946 0.44444444 0.4047619 ]\n",
      "f1 mean: 0.439\n",
      "f1 std: 0.027\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:189: RuntimeWarning: divide by zero encountered in log\n",
      "  loss -= (idx == y)*np.log(y_prob)\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:189: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss -= (idx == y)*np.log(y_prob)\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(10, 5, 5)\n",
      "f1 scores: [0.29292929 0.29292929 0.2481203  0.43734015 0.13043478]\n",
      "f1 mean: 0.280\n",
      "f1 std: 0.098\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(16, 8, 8)\n",
      "f1 scores: [0.14786967 0.14786967 0.14786967 0.2481203  0.13043478]\n",
      "f1 mean: 0.164\n",
      "f1 std: 0.042\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.1-(64, 32, 32)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.13043478 0.13043478]\n",
      "f1 mean: 0.336\n",
      "f1 std: 0.168\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.5-(5,)\n",
      "f1 scores: [0.375      0.52380952 0.2        0.41176471 0.46666667]\n",
      "f1 mean: 0.395\n",
      "f1 std: 0.110\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.5-(32,)\n",
      "f1 scores: [0.375      0.6        0.48717949 0.65714286 0.37321937]\n",
      "f1 mean: 0.499\n",
      "f1 std: 0.115\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.5-(16, 16)\n",
      "f1 scores: [0.47368421 0.41176471 0.45945946 0.44444444 0.4047619 ]\n",
      "f1 mean: 0.439\n",
      "f1 std: 0.027\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n",
      "/home/lexi/ML/ml_ss23_group13/Ex2/MLP.py:108: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(X)/np.sum(np.exp(X), axis=1)[:, None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.5-(10, 5, 5)\n",
      "f1 scores: [0.29292929 0.29292929 0.2481203  0.43734015 0.13043478]\n",
      "f1 mean: 0.280\n",
      "f1 std: 0.098\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.5-(16, 8, 8)\n",
      "f1 scores: [0.14786967 0.14786967 0.14786967 0.2481203  0.13043478]\n",
      "f1 mean: 0.164\n",
      "f1 std: 0.042\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "WARNING: divergence detected, please set smaller learning rate.\n",
      "As a punishment we return a model with randomly initialized weights.\n",
      "MLP-relu-0.5-(64, 32, 32)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.13043478 0.13043478]\n",
      "f1 mean: 0.336\n",
      "f1 std: 0.168\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 130...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 132...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 142...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 126...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 168...\n",
      "MLP-sigmoid-0.0001-(5,)\n",
      "f1 scores: [0.19191919 0.04761905 0.29292929 0.41176471 0.39393939]\n",
      "f1 mean: 0.268\n",
      "f1 std: 0.135\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 29...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 45...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 45...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 113...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 187...\n",
      "MLP-sigmoid-0.0001-(32,)\n",
      "f1 scores: [0.31034483 0.19191919 0.375      0.30666667 0.35483871]\n",
      "f1 mean: 0.308\n",
      "f1 std: 0.064\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 57...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 59...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 61...\n",
      "MLP-sigmoid-0.0001-(16, 16)\n",
      "f1 scores: [0.41176471 0.65714286 1.         0.28571429 0.52      ]\n",
      "f1 mean: 0.575\n",
      "f1 std: 0.245\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 25...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 25...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 85...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 85...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 85...\n",
      "MLP-sigmoid-0.0001-(10, 5, 5)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.45945946 0.45945946]\n",
      "f1 mean: 0.238\n",
      "f1 std: 0.181\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 81...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 102...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 82...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 102...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 108...\n",
      "MLP-sigmoid-0.0001-(16, 8, 8)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.45945946 0.45945946]\n",
      "f1 mean: 0.238\n",
      "f1 std: 0.181\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 38...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 33...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 38...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 38...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 39...\n",
      "MLP-sigmoid-0.0001-(64, 32, 32)\n",
      "f1 scores: [0.09090909 0.09090909 0.09090909 0.45945946 0.72222222]\n",
      "f1 mean: 0.291\n",
      "f1 std: 0.259\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 44...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 220...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 298...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 387...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 523...\n",
      "MLP-sigmoid-0.001-(5,)\n",
      "f1 scores: [0.34065934 0.27083333 0.56709957 0.65714286 0.39393939]\n",
      "f1 mean: 0.446\n",
      "f1 std: 0.144\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 182...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 228...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 306...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 440...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 432...\n",
      "MLP-sigmoid-0.001-(32,)\n",
      "f1 scores: [0.43573668 0.74025974 0.49820789 0.60784314 0.41176471]\n",
      "f1 mean: 0.539\n",
      "f1 std: 0.121\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 21...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 25...\n",
      "MLP-sigmoid-0.001-(16, 16)\n",
      "f1 scores: [0.25925926 0.29292929 0.52380952 0.34065934 0.39393939]\n",
      "f1 mean: 0.362\n",
      "f1 std: 0.093\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 17...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 24...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 25...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 25...\n",
      "MLP-sigmoid-0.001-(10, 5, 5)\n",
      "f1 scores: [0.14786967 0.1        0.14786967 0.42857143 0.45945946]\n",
      "f1 mean: 0.257\n",
      "f1 std: 0.154\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 19...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 23...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 19...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 23...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 24...\n",
      "MLP-sigmoid-0.001-(16, 8, 8)\n",
      "f1 scores: [0.09090909 0.09090909 0.14786967 0.45945946 0.60784314]\n",
      "f1 mean: 0.279\n",
      "f1 std: 0.214\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 13...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 12...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 13...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 12...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 12...\n",
      "MLP-sigmoid-0.001-(64, 32, 32)\n",
      "f1 scores: [0.60784314 0.45945946 0.39393939 0.33333333 0.60784314]\n",
      "f1 mean: 0.480\n",
      "f1 std: 0.111\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 129...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 110...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 159...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 167...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 169...\n",
      "MLP-sigmoid-0.01-(5,)\n",
      "f1 scores: [0.31034483 0.65714286 0.60784314 0.53125    0.60784314]\n",
      "f1 mean: 0.543\n",
      "f1 std: 0.123\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 117...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 122...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 120...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 149...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 181...\n",
      "MLP-sigmoid-0.01-(32,)\n",
      "f1 scores: [0.4047619  0.72222222 0.44444444 0.49820789 0.42857143]\n",
      "f1 mean: 0.500\n",
      "f1 std: 0.115\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 107...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 115...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 161...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 143...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 190...\n",
      "MLP-sigmoid-0.01-(16, 16)\n",
      "f1 scores: [0.37321937 0.80392157 0.42857143 0.53125    0.42857143]\n",
      "f1 mean: 0.513\n",
      "f1 std: 0.154\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 11...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 11...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 11...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "MLP-sigmoid-0.01-(10, 5, 5)\n",
      "f1 scores: [0.45945946 0.45945946 0.45945946 0.45945946 0.45945946]\n",
      "f1 mean: 0.459\n",
      "f1 std: 0.000\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 11...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 11...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 11...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 11...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 11...\n",
      "MLP-sigmoid-0.01-(16, 8, 8)\n",
      "f1 scores: [0.42857143 0.45945946 0.42857143 0.45945946 0.45945946]\n",
      "f1 mean: 0.447\n",
      "f1 std: 0.015\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 162...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 161...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 165...\n",
      "MLP-sigmoid-0.01-(64, 32, 32)\n",
      "f1 scores: [0.37321937 0.88571429 0.44444444 0.45945946 0.45945946]\n",
      "f1 mean: 0.524\n",
      "f1 std: 0.183\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 20...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 23...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 42...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 52...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 58...\n",
      "MLP-sigmoid-0.1-(5,)\n",
      "f1 scores: [0.31034483 0.65714286 0.42857143 0.53125    0.56709957]\n",
      "f1 mean: 0.499\n",
      "f1 std: 0.119\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 24...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 17...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 19...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 33...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 60...\n",
      "MLP-sigmoid-0.1-(32,)\n",
      "f1 scores: [0.43573668 0.72222222 0.44444444 0.375      0.44444444]\n",
      "f1 mean: 0.484\n",
      "f1 std: 0.122\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 23...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 19...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 34...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 35...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 42...\n",
      "MLP-sigmoid-0.1-(16, 16)\n",
      "f1 scores: [0.37321937 0.65714286 0.39393939 0.39393939 0.60784314]\n",
      "f1 mean: 0.485\n",
      "f1 std: 0.121\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 57...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 37...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 49...\n",
      "MLP-sigmoid-0.1-(10, 5, 5)\n",
      "f1 scores: [0.34065934 0.47368421 0.42857143 0.45945946 0.60784314]\n",
      "f1 mean: 0.462\n",
      "f1 std: 0.086\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 29...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 37...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 34...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 38...\n",
      "MLP-sigmoid-0.1-(16, 8, 8)\n",
      "f1 scores: [0.37321937 0.47368421 0.44444444 0.375      0.44444444]\n",
      "f1 mean: 0.422\n",
      "f1 std: 0.041\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "MLP-sigmoid-0.1-(64, 32, 32)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.468\n",
      "f1 std: 0.007\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 17...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 16...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 23...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 30...\n",
      "MLP-sigmoid-0.5-(5,)\n",
      "f1 scores: [0.49820789 0.41176471 0.44444444 0.35483871 0.53125   ]\n",
      "f1 mean: 0.448\n",
      "f1 std: 0.062\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 18...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 17...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 18...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 19...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 54...\n",
      "MLP-sigmoid-0.5-(32,)\n",
      "f1 scores: [0.28571429 0.39393939 0.64157706 0.39393939 0.53125   ]\n",
      "f1 mean: 0.449\n",
      "f1 std: 0.124\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 22...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 21...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 32...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 35...\n",
      "MLP-sigmoid-0.5-(16, 16)\n",
      "f1 scores: [0.39393939 0.47368421 0.42857143 0.45945946 0.56709957]\n",
      "f1 mean: 0.465\n",
      "f1 std: 0.058\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "MLP-sigmoid-0.5-(10, 5, 5)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.468\n",
      "f1 std: 0.007\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "MLP-sigmoid-0.5-(16, 8, 8)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.468\n",
      "f1 std: 0.007\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 10...\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 126...\n",
      "MLP-sigmoid-0.5-(64, 32, 32)\n",
      "f1 scores: [0.47368421 0.47368421 0.47368421 0.45945946 0.45945946]\n",
      "f1 mean: 0.468\n",
      "f1 std: 0.007\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cv_num = 5\n",
    "model_params = {}\n",
    "models = {}\n",
    "models_lists = {}\n",
    "\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    cv_results = cross_validate(pipeline, X, y, cv=cv_num, scoring='f1_macro', return_estimator=True, n_jobs=10)\n",
    "\n",
    "    models[model_name] = cv_results['estimator']\n",
    "    model_params[model_name] = {}\n",
    "    models_lists[model_name] = {}\n",
    "\n",
    "    num_cols = ['test_score', 'fit_time', 'score_time']\n",
    "\n",
    "    for num_col in num_cols:\n",
    "        models_lists[model_name][num_col] = cv_results[num_col]\n",
    "        model_params[model_name][f'{num_col}_mean'] = cv_results[num_col].mean()\n",
    "        model_params[model_name][f'{num_col}_std'] = cv_results[num_col].std()\n",
    "    \n",
    "    model_params[model_name]['parameter_num'] = cv_results['estimator'][0][model_name].number_of_params_\n",
    "    model_params[model_name]['hidden_layer_sizes'] = cv_results['estimator'][0][model_name].hidden_layer_sizes\n",
    "    model_params[model_name]['activation_function'] = cv_results['estimator'][0][model_name].activation_function\n",
    "    model_params[model_name]['learning_rate'] = cv_results['estimator'][0][model_name].learning_rate\n",
    "    models_lists[model_name]['converged'] = [e[model_name].converged_ for e in cv_results['estimator']]\n",
    "    models_lists[model_name]['validation_losses'] = [e[model_name].validation_losses_ for e in cv_results['estimator']]\n",
    "    models_lists[model_name]['training_losses'] = [e[model_name].training_losses_ for e in cv_results['estimator']]\n",
    "    model_params[model_name]['num_iter'] = np.array(list([len(e[model_name].training_losses_) for e in cv_results['estimator']])).mean()\n",
    "\n",
    "    print(model_name)\n",
    "    print(\n",
    "        f\"f1 scores: {models_lists[model_name]['test_score']}\\n\" +\n",
    "        f\"f1 mean: {model_params[model_name]['test_score_mean']:.3f}\\n\" +\n",
    "        f\"f1 std: {model_params[model_name]['test_score_std']:.3f}\\n\"\n",
    "    )\n",
    "    print('----------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_param = pd.DataFrame(model_params).transpose()\n",
    "df_param = df_param.reset_index(drop=False)\n",
    "df_param = df_param.rename(columns={'index': 'model'})\n",
    "\n",
    "# df_param.to_csv(r'results/fertility_params.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss did not go down for 10 iterations. Stopping training at iteration 57...\n",
      "Evaluation metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.94444444, 0.05555556],\n",
       "       [1.        , 0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.85\n",
      "balanced_acc: 0.4722222222222222\n",
      "\n",
      "Macro-averaged precision: 0.4473684210526316\n",
      "Macro-averaged recall: 0.4722222222222222\n",
      "Macro-averaged f-score: 0.4594594594594595\n",
      "Macro-averaged support: None\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 181...\n",
      "Evaluation metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5, 0.5],\n",
       "       [1. , 0. ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.45\n",
      "balanced_acc: 0.25\n",
      "\n",
      "Macro-averaged precision: 0.4090909090909091\n",
      "Macro-averaged recall: 0.25\n",
      "Macro-averaged f-score: 0.31034482758620685\n",
      "Macro-averaged support: None\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 308...\n",
      "Evaluation metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5, 0.5],\n",
       "       [0.5, 0.5]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5\n",
      "balanced_acc: 0.5\n",
      "\n",
      "Macro-averaged precision: 0.5\n",
      "Macro-averaged recall: 0.5\n",
      "Macro-averaged f-score: 0.40476190476190477\n",
      "Macro-averaged support: None\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 47...\n",
      "Evaluation metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.72222222, 0.27777778],\n",
       "       [1.        , 0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.65\n",
      "balanced_acc: 0.3611111111111111\n",
      "\n",
      "Macro-averaged precision: 0.43333333333333335\n",
      "Macro-averaged recall: 0.3611111111111111\n",
      "Macro-averaged f-score: 0.39393939393939387\n",
      "Macro-averaged support: None\n",
      "Loss did not go down for 10 iterations. Stopping training at iteration 159...\n",
      "Evaluation metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5, 0.5],\n",
       "       [0.5, 0.5]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5\n",
      "balanced_acc: 0.5\n",
      "\n",
      "Macro-averaged precision: 0.5\n",
      "Macro-averaged recall: 0.5\n",
      "Macro-averaged f-score: 0.40476190476190477\n",
      "Macro-averaged support: None\n"
     ]
    }
   ],
   "source": [
    "modelsss = df_param.sort_values(['test_score_mean'], ascending=False).head(5)['model']\n",
    "for mod in modelsss:\n",
    "    pipelines[mod].fit(X_train, y_train)\n",
    "    y_pred = pipelines[mod].predict(X_test)\n",
    "    cu.eval(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.94444444, 0.05555556],\n",
       "       [1.        , 0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.85\n",
      "balanced_acc: 0.4722222222222222\n",
      "\n",
      "Macro-averaged precision: 0.4473684210526316\n",
      "Macro-averaged recall: 0.4722222222222222\n",
      "Macro-averaged f-score: 0.4594594594594595\n",
      "Macro-averaged support: None\n",
      "Evaluation metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5, 0.5],\n",
       "       [1. , 0. ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.45\n",
      "balanced_acc: 0.25\n",
      "\n",
      "Macro-averaged precision: 0.4090909090909091\n",
      "Macro-averaged recall: 0.25\n",
      "Macro-averaged f-score: 0.31034482758620685\n",
      "Macro-averaged support: None\n",
      "Evaluation metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5, 0.5],\n",
       "       [0.5, 0.5]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5\n",
      "balanced_acc: 0.5\n",
      "\n",
      "Macro-averaged precision: 0.5\n",
      "Macro-averaged recall: 0.5\n",
      "Macro-averaged f-score: 0.40476190476190477\n",
      "Macro-averaged support: None\n",
      "Evaluation metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.72222222, 0.27777778],\n",
       "       [1.        , 0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.65\n",
      "balanced_acc: 0.3611111111111111\n",
      "\n",
      "Macro-averaged precision: 0.43333333333333335\n",
      "Macro-averaged recall: 0.3611111111111111\n",
      "Macro-averaged f-score: 0.39393939393939387\n",
      "Macro-averaged support: None\n",
      "Evaluation metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5, 0.5],\n",
       "       [0.5, 0.5]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5\n",
      "balanced_acc: 0.5\n",
      "\n",
      "Macro-averaged precision: 0.5\n",
      "Macro-averaged recall: 0.5\n",
      "Macro-averaged f-score: 0.40476190476190477\n",
      "Macro-averaged support: None\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame()\n",
    "for mod in modelsss:\n",
    "    y_pred = pipelines[mod].predict(X_test)\n",
    "    eval_results = cu.eval(y_pred, y_test)\n",
    "\n",
    "    results_df = results_df.append({\n",
    "        'Model': mod,\n",
    "        'cm': eval_results['cm'].flatten(),\n",
    "        'acc': eval_results['acc'],\n",
    "        'balanced_acc': eval_results['balanced_acc'],\n",
    "        'precision': eval_results['precision'],\n",
    "        'recall': eval_results['recall'],\n",
    "        'f-score': eval_results['f-score'],\n",
    "        'support': eval_results['support']\n",
    "    }, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"results/fertility_trainset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31034483 0.65714286 0.60784314 0.53125    0.60784314]\n",
      "[0.41176471 0.65714286 1.         0.28571429 0.52      ]\n",
      "[0.375      0.60784314 0.60784314 0.42857143 0.60784314]\n"
     ]
    }
   ],
   "source": [
    "print(models_lists['MLP-sigmoid-0.01-(5,)']['test_score'])\n",
    "print(models_lists['MLP-sigmoid-0.0001-(16, 16)']['test_score'])\n",
    "print(models_lists['MLP-relu-0.01-(32,)']['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_losses(df, loss_col = 'validation_losses', from_str = False):\n",
    "    model_names = df.model.tolist()\n",
    "    validation_losses = df[loss_col].tolist()\n",
    "\n",
    "    models_vl = {}\n",
    "    for vl, model_name in zip(validation_losses, model_names):\n",
    "        models_vl[model_name] = {}\n",
    "        if from_str:\n",
    "            vl = vl.split('], [')\n",
    "        for i, vl_cv in enumerate(vl):\n",
    "            if from_str:\n",
    "                vl_cv = [float(s) for s in vl_cv.replace('[', '').replace(']', '').split(', ')]\n",
    "            models_vl[model_name][i] = vl_cv\n",
    "    \n",
    "    return models_vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'validation_losses'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'validation_losses'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vl_losses_dict \u001b[39m=\u001b[39m get_losses(df_param)\n\u001b[1;32m      2\u001b[0m tl_losses_dict \u001b[39m=\u001b[39m get_losses(df_param, loss_col\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtraining_losses\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[79], line 3\u001b[0m, in \u001b[0;36mget_losses\u001b[0;34m(df, loss_col, from_str)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_losses\u001b[39m(df, loss_col \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvalidation_losses\u001b[39m\u001b[39m'\u001b[39m, from_str \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m     model_names \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m----> 3\u001b[0m     validation_losses \u001b[39m=\u001b[39m df[loss_col]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m      5\u001b[0m     models_vl \u001b[39m=\u001b[39m {}\n\u001b[1;32m      6\u001b[0m     \u001b[39mfor\u001b[39;00m vl, model_name \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(validation_losses, model_names):\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'validation_losses'"
     ]
    }
   ],
   "source": [
    "vl_losses_dict = get_losses(df_param)\n",
    "tl_losses_dict = get_losses(df_param, loss_col='training_losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      3\u001b[0m plot_data \u001b[39m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mvls\u001b[39m\u001b[39m'\u001b[39m: vl_losses_dict[model_name][i],\n\u001b[1;32m      5\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtls\u001b[39m\u001b[39m'\u001b[39m: tl_losses_dict[model_name][i]\n\u001b[1;32m      6\u001b[0m }\n\u001b[0;32m----> 7\u001b[0m plot_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(plot_data)\n\u001b[1;32m      8\u001b[0m plot_data \u001b[39m=\u001b[39m plot_data\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m plot_data \u001b[39m=\u001b[39m plot_data\u001b[39m.\u001b[39mmelt(id_vars\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m, value_vars\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mvls\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtls\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/pandas/core/frame.py:664\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    658\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    659\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    660\u001b[0m     )\n\u001b[1;32m    662\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    663\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 664\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[1;32m    665\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    666\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/pandas/core/internals/construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    491\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m--> 493\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/pandas/core/internals/construction.py:118\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    116\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    119\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/pandas/core/internals/construction.py:656\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    655\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m indexes \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m raw_lengths:\n\u001b[0;32m--> 656\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIf using all scalar values, you must pass an index\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    658\u001b[0m \u001b[39melif\u001b[39;00m have_series:\n\u001b[1;32m    659\u001b[0m     index \u001b[39m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "model_name = 'MLP-sigmoid-0.1-(5,)'\n",
    "i = 0\n",
    "plot_data = {\n",
    "    'vls': vl_losses_dict[model_name][i],\n",
    "    'tls': tl_losses_dict[model_name][i]\n",
    "}\n",
    "plot_data = pd.DataFrame(plot_data)\n",
    "plot_data = plot_data.reset_index(drop=False)\n",
    "plot_data = plot_data.melt(id_vars='index', value_vars=['vls', 'tls'])\n",
    "plot_data.columns = ['iteration', 'loss_type', 'loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret value `iteration` for parameter `x`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m y_key \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvls\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m sns\u001b[39m.\u001b[39;49mlineplot(data\u001b[39m=\u001b[39;49mplot_data, x\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39miteration\u001b[39;49m\u001b[39m'\u001b[39;49m, y\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mloss\u001b[39;49m\u001b[39m'\u001b[39;49m, hue\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mloss_type\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/seaborn/relational.py:618\u001b[0m, in \u001b[0;36mlineplot\u001b[0;34m(data, x, y, hue, size, style, units, palette, hue_order, hue_norm, sizes, size_order, size_norm, dashes, markers, style_order, estimator, errorbar, n_boot, seed, orient, sort, err_style, err_kws, legend, ci, ax, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m errorbar \u001b[39m=\u001b[39m _deprecate_ci(errorbar, ci)\n\u001b[1;32m    617\u001b[0m variables \u001b[39m=\u001b[39m _LinePlotter\u001b[39m.\u001b[39mget_semantics(\u001b[39mlocals\u001b[39m())\n\u001b[0;32m--> 618\u001b[0m p \u001b[39m=\u001b[39m _LinePlotter(\n\u001b[1;32m    619\u001b[0m     data\u001b[39m=\u001b[39;49mdata, variables\u001b[39m=\u001b[39;49mvariables,\n\u001b[1;32m    620\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator, n_boot\u001b[39m=\u001b[39;49mn_boot, seed\u001b[39m=\u001b[39;49mseed, errorbar\u001b[39m=\u001b[39;49merrorbar,\n\u001b[1;32m    621\u001b[0m     sort\u001b[39m=\u001b[39;49msort, orient\u001b[39m=\u001b[39;49morient, err_style\u001b[39m=\u001b[39;49merr_style, err_kws\u001b[39m=\u001b[39;49merr_kws,\n\u001b[1;32m    622\u001b[0m     legend\u001b[39m=\u001b[39;49mlegend,\n\u001b[1;32m    623\u001b[0m )\n\u001b[1;32m    625\u001b[0m p\u001b[39m.\u001b[39mmap_hue(palette\u001b[39m=\u001b[39mpalette, order\u001b[39m=\u001b[39mhue_order, norm\u001b[39m=\u001b[39mhue_norm)\n\u001b[1;32m    626\u001b[0m p\u001b[39m.\u001b[39mmap_size(sizes\u001b[39m=\u001b[39msizes, order\u001b[39m=\u001b[39msize_order, norm\u001b[39m=\u001b[39msize_norm)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/seaborn/relational.py:365\u001b[0m, in \u001b[0;36m_LinePlotter.__init__\u001b[0;34m(self, data, variables, estimator, n_boot, seed, errorbar, sort, orient, err_style, err_kws, legend)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    352\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m,\n\u001b[1;32m    353\u001b[0m     data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, variables\u001b[39m=\u001b[39m{},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[39m# the kind of plot to draw, but for the time being we need to set\u001b[39;00m\n\u001b[1;32m    360\u001b[0m     \u001b[39m# this information so the SizeMapping can use it\u001b[39;00m\n\u001b[1;32m    361\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_default_size_range \u001b[39m=\u001b[39m (\n\u001b[1;32m    362\u001b[0m         np\u001b[39m.\u001b[39mr_[\u001b[39m.5\u001b[39m, \u001b[39m2\u001b[39m] \u001b[39m*\u001b[39m mpl\u001b[39m.\u001b[39mrcParams[\u001b[39m\"\u001b[39m\u001b[39mlines.linewidth\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    363\u001b[0m     )\n\u001b[0;32m--> 365\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(data\u001b[39m=\u001b[39;49mdata, variables\u001b[39m=\u001b[39;49mvariables)\n\u001b[1;32m    367\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator \u001b[39m=\u001b[39m estimator\n\u001b[1;32m    368\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrorbar \u001b[39m=\u001b[39m errorbar\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/seaborn/_oldcore.py:640\u001b[0m, in \u001b[0;36mVectorPlotter.__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[39m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[39m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[39m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[39m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_var_ordered \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mFalse\u001b[39;00m}  \u001b[39m# alt., used DefaultDict\u001b[39;00m\n\u001b[0;32m--> 640\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49massign_variables(data, variables)\n\u001b[1;32m    642\u001b[0m \u001b[39mfor\u001b[39;00m var, \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_semantic_mappings\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    643\u001b[0m \n\u001b[1;32m    644\u001b[0m     \u001b[39m# Create the mapping function\u001b[39;00m\n\u001b[1;32m    645\u001b[0m     map_func \u001b[39m=\u001b[39m partial(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmap, plotter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/seaborn/_oldcore.py:701\u001b[0m, in \u001b[0;36mVectorPlotter.assign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_format \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlong\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 701\u001b[0m     plot_data, variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_assign_variables_longform(\n\u001b[1;32m    702\u001b[0m         data, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mvariables,\n\u001b[1;32m    703\u001b[0m     )\n\u001b[1;32m    705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplot_data \u001b[39m=\u001b[39m plot_data\n\u001b[1;32m    706\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariables \u001b[39m=\u001b[39m variables\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/seaborn/_oldcore.py:938\u001b[0m, in \u001b[0;36mVectorPlotter._assign_variables_longform\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(val, (\u001b[39mstr\u001b[39m, \u001b[39mbytes\u001b[39m)):\n\u001b[1;32m    934\u001b[0m \n\u001b[1;32m    935\u001b[0m     \u001b[39m# This looks like a column name but we don't know what it means!\u001b[39;00m\n\u001b[1;32m    937\u001b[0m     err \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not interpret value `\u001b[39m\u001b[39m{\u001b[39;00mval\u001b[39m}\u001b[39;00m\u001b[39m` for parameter `\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 938\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err)\n\u001b[1;32m    940\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    941\u001b[0m \n\u001b[1;32m    942\u001b[0m     \u001b[39m# Otherwise, assume the value is itself data\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \n\u001b[1;32m    944\u001b[0m     \u001b[39m# Raise when data object is present and a vector can't matched\u001b[39;00m\n\u001b[1;32m    945\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pd\u001b[39m.\u001b[39mDataFrame) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(val, pd\u001b[39m.\u001b[39mSeries):\n",
      "\u001b[0;31mValueError\u001b[0m: Could not interpret value `iteration` for parameter `x`"
     ]
    }
   ],
   "source": [
    "y_key = 'vls'\n",
    "sns.lineplot(data=plot_data, x='iteration', y='loss', hue='loss_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_param = pd.read_csv(r'results/fertility_params.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "converged = df_param['converged'].values.tolist()\n",
    "converged_new = []\n",
    "for cs in converged:\n",
    "    cs = cs.replace('[', '').replace(']', '').split(', ')\n",
    "    c_bools = [c == 'True' for c in cs]\n",
    "    converged_new.append(c_bools)\n",
    "df_param['converged'] = converged_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_param['converged_final'] = df_param.converged.apply(lambda x: all(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['model', 'test_score', 'test_score_mean', 'test_score_std', 'fit_time',\n",
       "       'fit_time_mean', 'fit_time_std', 'score_time', 'score_time_mean',\n",
       "       'score_time_std', 'parameter_num', 'hidden_layer_sizes',\n",
       "       'activation_function', 'learning_rate', 'converged',\n",
       "       'validation_losses', 'training_losses', 'num_iter', 'converged_final'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_param.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`errorbar` must be a callable, string, or (string, number) tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/seaborn/_statistics.py:546\u001b[0m, in \u001b[0;36m_validate_errorbar_arg\u001b[0;34m(arg)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 546\u001b[0m     method, level \u001b[39m=\u001b[39m arg\n\u001b[1;32m    547\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m _, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(figsize\u001b[39m=\u001b[39m(\u001b[39m15\u001b[39m, \u001b[39m8\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m sns\u001b[39m.\u001b[39;49mbarplot(data\u001b[39m=\u001b[39;49mdf_param[df_param\u001b[39m.\u001b[39;49mconverged_final], y\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m'\u001b[39;49m, x\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfit_time_mean\u001b[39;49m\u001b[39m'\u001b[39;49m, hue\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mactivation_function\u001b[39;49m\u001b[39m'\u001b[39;49m, ax\u001b[39m=\u001b[39;49max, errorbar\u001b[39m=\u001b[39;49m())\n\u001b[1;32m      3\u001b[0m ax\u001b[39m.\u001b[39mset_title(\u001b[39m'\u001b[39m\u001b[39mMean fitting times over converged models\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/seaborn/categorical.py:2755\u001b[0m, in \u001b[0;36mbarplot\u001b[0;34m(data, x, y, hue, order, hue_order, estimator, errorbar, n_boot, units, seed, orient, color, palette, saturation, width, errcolor, errwidth, capsize, dodge, ci, ax, **kwargs)\u001b[0m\n\u001b[1;32m   2752\u001b[0m \u001b[39mif\u001b[39;00m estimator \u001b[39mis\u001b[39;00m \u001b[39mlen\u001b[39m:\n\u001b[1;32m   2753\u001b[0m     estimator \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msize\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 2755\u001b[0m plotter \u001b[39m=\u001b[39m _BarPlotter(x, y, hue, data, order, hue_order,\n\u001b[1;32m   2756\u001b[0m                       estimator, errorbar, n_boot, units, seed,\n\u001b[1;32m   2757\u001b[0m                       orient, color, palette, saturation,\n\u001b[1;32m   2758\u001b[0m                       width, errcolor, errwidth, capsize, dodge)\n\u001b[1;32m   2760\u001b[0m \u001b[39mif\u001b[39;00m ax \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2761\u001b[0m     ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mgca()\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/seaborn/categorical.py:1533\u001b[0m, in \u001b[0;36m_BarPlotter.__init__\u001b[0;34m(self, x, y, hue, data, order, hue_order, estimator, errorbar, n_boot, units, seed, orient, color, palette, saturation, width, errcolor, errwidth, capsize, dodge)\u001b[0m\n\u001b[1;32m   1530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestablish_variables(x, y, hue, data, orient,\n\u001b[1;32m   1531\u001b[0m                          order, hue_order, units)\n\u001b[1;32m   1532\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestablish_colors(color, palette, saturation)\n\u001b[0;32m-> 1533\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimate_statistic(estimator, errorbar, n_boot, seed)\n\u001b[1;32m   1535\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdodge \u001b[39m=\u001b[39m dodge\n\u001b[1;32m   1536\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39m=\u001b[39m width\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/seaborn/categorical.py:1449\u001b[0m, in \u001b[0;36m_CategoricalStatPlotter.estimate_statistic\u001b[0;34m(self, estimator, errorbar, n_boot, seed)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     confint \u001b[39m=\u001b[39m [[] \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplot_data]\n\u001b[1;32m   1447\u001b[0m var \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mv\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mh\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m}[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39morient]\n\u001b[0;32m-> 1449\u001b[0m agg \u001b[39m=\u001b[39m EstimateAggregator(estimator, errorbar, n_boot\u001b[39m=\u001b[39;49mn_boot, seed\u001b[39m=\u001b[39;49mseed)\n\u001b[1;32m   1451\u001b[0m \u001b[39mfor\u001b[39;00m i, group_data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplot_data):\n\u001b[1;32m   1452\u001b[0m \n\u001b[1;32m   1453\u001b[0m     \u001b[39m# Option 1: we have a single layer of grouping\u001b[39;00m\n\u001b[1;32m   1454\u001b[0m     \u001b[39m# --------------------------------------------\u001b[39;00m\n\u001b[1;32m   1455\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplot_hues \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/seaborn/_statistics.py:474\u001b[0m, in \u001b[0;36mEstimateAggregator.__init__\u001b[0;34m(self, estimator, errorbar, **boot_kws)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[39mData aggregator that produces an estimate and error bar interval.\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    470\u001b[0m \n\u001b[1;32m    471\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator \u001b[39m=\u001b[39m estimator\n\u001b[0;32m--> 474\u001b[0m method, level \u001b[39m=\u001b[39m _validate_errorbar_arg(errorbar)\n\u001b[1;32m    475\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_method \u001b[39m=\u001b[39m method\n\u001b[1;32m    476\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_level \u001b[39m=\u001b[39m level\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/seaborn/_statistics.py:548\u001b[0m, in \u001b[0;36m_validate_errorbar_arg\u001b[0;34m(arg)\u001b[0m\n\u001b[1;32m    546\u001b[0m         method, level \u001b[39m=\u001b[39m arg\n\u001b[1;32m    547\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 548\u001b[0m         \u001b[39mraise\u001b[39;00m err\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m(usage) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m    550\u001b[0m _check_argument(\u001b[39m\"\u001b[39m\u001b[39merrorbar\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlist\u001b[39m(DEFAULT_LEVELS), method)\n\u001b[1;32m    551\u001b[0m \u001b[39mif\u001b[39;00m level \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(level, Number):\n",
      "\u001b[0;31mValueError\u001b[0m: `errorbar` must be a callable, string, or (string, number) tuple"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, ax = plt.subplots(figsize=(15, 8))\n",
    "sns.barplot(data=df_param[df_param.converged_final], y='model', x='fit_time_mean', hue='activation_function', ax=ax, errorbar=())\n",
    "ax.set_title('Mean fitting times over converged models')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
